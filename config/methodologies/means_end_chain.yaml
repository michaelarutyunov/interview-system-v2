# Means-End Chain Methodology Configuration
#
# MEC explores chains from attributes → consequences → values
# by asking "why is that important?" to probe deeper.
#
# Strategy design principles:
# - chain_completion is the PRIMARY success metric — MEC's entire goal is
#   completing attribute → functional → psychosocial → instrumental → terminal chains
# - intellectual_engagement drives laddering — high IE means causal reasoning
#   is happening, which is the prime moment to keep climbing
# - specificity is INVERTED from JTBD: low specificity (abstract) → anchor_down,
#   high specificity (concrete) → ladder_up (ready to climb)
# - Adaptive termination: max_turns high, saturation + chain_completion gate closing
# - Repetition penalties symmetric at -0.7 (proven pattern from JTBD tuning)

method:
  name: means_end_chain
  version: "4.0"
  goal: "Explore causal chains from concrete attributes to abstract values"
  opening_bias: "Ask about specific product features, qualities, or characteristics they notice or care about."
  description: "Laddering: attributes → consequences → values"

ontology:
  nodes:
    - name: attribute
      level: 1
      terminal: false
      description: "Concrete product feature or characteristic"
      examples:
        - "creamy texture"
        - "plant-based"
        - "sustainable packaging"

    - name: functional_consequence
      level: 2
      terminal: false
      description: "Tangible outcome from using the product"
      examples:
        - "easier to digest"
        - "mixes well with coffee"

    - name: psychosocial_consequence
      level: 3
      terminal: false
      description: "Emotional or social outcome"
      examples:
        - "feel healthier"
        - "gain respect from peers"

    - name: instrumental_value
      level: 4
      terminal: false
      description: "Preferred mode of behavior"
      examples:
        - "being responsible"
        - "taking care of myself"

    - name: terminal_value
      level: 5
      terminal: true
      description: "End-state of existence, core life goal"
      examples:
        - "self-respect"
        - "sense of accomplishment"
        - "family security"

  edges:
    - name: leads_to
      description: "Causal or enabling relationship"
      permitted_connections:
        - [attribute, attribute]
        - [attribute, functional_consequence]
        - [functional_consequence, functional_consequence]
        - [functional_consequence, psychosocial_consequence]
        - [psychosocial_consequence, psychosocial_consequence]
        - [psychosocial_consequence, instrumental_value]
        - [instrumental_value, instrumental_value]
        - [instrumental_value, terminal_value]

    - name: revises
      description: "Contradiction - newer belief supersedes older one"
      permitted_connections:
        - ["*", "*"]

  extraction_guidelines:
    - "For Means-End Chain interviews, extract BOTH explicit AND implicit relationships"
    - "Explicit: Look for causal markers like 'because', 'so', 'that's why', 'leads to'"
    - "Implicit (MEC chains): When multiple concepts are mentioned in the same response, connect them if they form a logical means-end chain"
    - "Contextual: If the interviewer asks 'why does X matter' and the response describes Y, create X→Y relationship"
    - "For Means-End Chain interviews, create edges between concepts that represent the chain of reasoning, even when no explicit connector is used"
    - "attribute → functional_consequence (features → outcomes)"
    - "functional_consequence → psychosocial_consequence (outcomes → feelings)"
    - "psychosocial_consequence → instrumental_value (feelings → goals)"
    - "instrumental_value → terminal_value (goals → life values)"
    - "Target 2-3x more edges than nodes for a well-structured MEC graph"
    - "Prefer creating relationships over leaving concepts disconnected"

  relationship_examples:
    explicit_causal:
      description: "Discourse markers explicitly state the connection"
      example: "I like froth because it makes coffee less bitter"
      extraction: "froth → makes coffee taste less bitter"

    implicit_same_response:
      description: "Multiple concepts in one response form logical consequence chain"
      example: "Froth makes coffee taste less bitter and gives a nice sensation in the mouth"
      extraction: "makes coffee taste less bitter → nice sensation in the mouth"

    implicit_conversational:
      description: "Question establishes antecedent, response provides consequence"
      example: "Interviewer: 'Why does the nice sensation matter?' Respondent: 'It feels like a good start of the day'"
      extraction: "nice sensation in the mouth → good start of the day"

    value_progression:
      description: "Parallel goals forming a chain toward terminal value"
      example: "I can be my best and set a good example to my kids"
      extraction: "be my best → set a good example to my kids"

  extractability_criteria:
    extractable_contains:
      - "Product attributes, features, or characteristics"
      - "Benefits, outcomes, or consequences"
      - "Feelings, emotions, or social implications"
      - "Values or life goals"
      - "Causal relationships between any of the above"
    non_extractable_contains:
      - "Simple yes/no responses"
      - "Acknowledgments ('okay', 'I see')"
      - "Questions back to the interviewer"
      - "Off-topic tangents"
      - "Very short responses with no substance"

  concept_naming_convention: >
    Name each concept according to its level in the means-end chain:
    attributes as concrete product features or characteristics,
    consequences as functional or psychosocial outcomes,
    values as abstract personal values or life goals.
    Use the examples in each node type description as naming models.

signals:
  graph:
    - graph.node_count
    - graph.max_depth
    - graph.avg_depth
    - graph.orphan_count
    - graph.chain_completion
    # Canonical graph signals
    - graph.canonical_concept_count
    - graph.canonical_edge_density
    - graph.canonical_exhaustion_score

  llm:
    - llm.response_depth
    - llm.valence
    - llm.certainty
    - llm.specificity
    - llm.engagement
    - llm.intellectual_engagement
    # Note: llm.global_response_trend is session-scoped and managed separately

  temporal:
    - temporal.strategy_repetition_count
    - temporal.turns_since_strategy_change

  meta:
    - meta.interview.phase
    - meta.conversation.saturation
    - meta.canonical.saturation

strategies:
  # Strategy 1: Elicit concrete attributes (elaboration)
  - name: elicit_attribute
    description: "Surface concrete product features and qualities — ask 'What specifically about X matters to you?' or 'What do you notice about X?'"
    signal_weights:
      # Primary triggers — fire when graph is sparse
      graph.orphan_count: 0.4            # Orphans need connecting → but also need new attributes
      # Engagement — need willingness to describe
      llm.engagement.mid: 0.4
      llm.engagement.high: 0.3
      llm.engagement.low: -0.3           # Disengaged = don't push for new content
      # Specificity — already concrete responses suggest more attributes available
      llm.specificity.high: 0.3          # Concrete language = respondent in attribute-mode
      # Response quality — suppress during fatigue
      llm.global_response_trend.shallowing: -0.4
      llm.global_response_trend.fatigued: -0.6
      # Diversity
      temporal.strategy_repetition_count: -0.7
      temporal.turns_since_strategy_change: -0.4
      # Node-level signals
      graph.node.focus_streak.none: 0.6  # Fresh nodes
      graph.node.focus_streak.medium: -0.4
      graph.node.focus_streak.high: -0.7
      graph.node.yield_stagnation.false: 0.5
      graph.node.exhaustion_score.low: 0.4

  # Strategy 2: Ladder up — the core MEC move (laddering)
  - name: ladder_up
    description: "Ask 'Why is that important to you?' to climb from concrete attributes toward abstract values — the core laddering technique"
    signal_weights:
      # Primary trigger — incomplete chains are the strongest signal
      graph.chain_completion.has_complete.false: 1.0
      # Depth awareness — don't ladder when already at high depth
      graph.max_depth: -0.2              # Slight brake at high depth (already near values)
      # Intellectual engagement — high IE = respondent is reasoning about 'why'
      llm.intellectual_engagement.high: 0.6
      llm.intellectual_engagement.mid: 0.3
      # Response depth — surface/shallow answers are prime laddering targets
      llm.response_depth.surface: 0.5
      llm.response_depth.shallow: 0.4
      llm.response_depth.moderate: 0.15
      llm.response_depth.deep: 0.3      # Deep + incomplete chain = keep going
      # Specificity — concrete responses are ready to ladder from
      llm.specificity.high: 0.4         # Concrete = solid base to climb from
      # Engagement safety — need engagement to handle 'why' probing
      llm.engagement.high: 0.5
      llm.engagement.mid: 0.3
      llm.engagement.low: -0.5          # Don't push disengaged respondents
      # Valence safety
      llm.valence.high: 0.4             # Positive = safe to go deeper
      # Response quality trend
      llm.global_response_trend.shallowing: 0.4  # Shallow trend → try laddering
      # Diversity
      temporal.strategy_repetition_count: -0.7
      temporal.strategy_repetition_count.high: -1.0  # Severe penalty at 4-5 reps
      # Node-level signals
      graph.node.exhaustion_score.low: 1.0
      graph.node.focus_streak.low: 0.5
      graph.node.focus_streak.medium: -0.4
      graph.node.focus_streak.high: -0.8

  # Strategy 3: Anchor down to concrete (probing)
  - name: anchor_down
    description: "Pull the respondent back to concrete examples when they become too abstract — ask 'Can you give me a specific example?' or 'What does that look like in practice?'"
    signal_weights:
      # Primary trigger — low specificity = abstract/vague language
      llm.specificity.low: 0.9          # Vague/abstract = need concrete grounding
      llm.specificity.mid: 0.3          # Moderately vague = mild trigger
      # Depth — high depth + vague = floating in abstraction
      graph.max_depth: 0.4              # Higher depth = more likely to be too abstract
      # Certainty — uncertain abstract responses are prime anchoring targets
      llm.certainty.low: 0.4
      llm.certainty.mid: 0.2
      # Engagement — can anchor even at moderate engagement
      llm.engagement.mid: 0.3
      llm.engagement.high: 0.2
      # Diversity
      temporal.strategy_repetition_count: -0.7
      temporal.turns_since_strategy_change: -0.4
      # Node-level signals
      graph.node.exhaustion_score.low: 0.5
      graph.node.focus_streak.medium: -0.3
      graph.node.focus_streak.high: -0.6

  # Strategy 4: Bridge separate chains (probing)
  - name: bridge_chains
    description: "Connect separate ladders — ask 'How does what you said about X relate to Y?' to find convergence points between parallel chains"
    signal_weights:
      # Primary trigger — orphan nodes suggest disconnected chains
      graph.orphan_count: 0.6           # More orphans = more bridging needed
      # Chain completion — bridge when some chains exist but aren't connected
      graph.chain_completion.has_complete.true: 0.5  # At least one chain exists to bridge from
      # Intellectual engagement — high IE = respondent can reason about connections
      llm.intellectual_engagement.high: 0.5
      llm.intellectual_engagement.mid: 0.3
      # Engagement safety
      llm.engagement.high: 0.4
      llm.engagement.mid: 0.3
      llm.engagement.low: -0.4
      # Diversity
      temporal.strategy_repetition_count: -0.7
      temporal.turns_since_strategy_change: -0.4
      # Node-level signals — prefer nodes that have outgoing edges (part of a chain)
      graph.node.has_outgoing.true: 0.6
      graph.node.is_orphan.true: 0.5    # Orphans are prime bridging targets
      graph.node.exhaustion_score.low: 0.4
      graph.node.focus_streak.medium: -0.3
      graph.node.focus_streak.high: -0.6

  # Strategy 5: Clarify meaning (probing)
  - name: clarify
    description: "When the respondent's meaning is unclear, ask 'What do you mean by X?' or rephrase to check understanding"
    signal_weights:
      # Primary triggers — vague + uncertain
      llm.specificity.low: 0.8
      llm.certainty.low: 0.5
      # Engagement
      llm.engagement.mid: 0.3
      # Diversity
      temporal.strategy_repetition_count: -0.7
      temporal.turns_since_strategy_change: -0.3
      # Node-level signals
      graph.node.is_orphan.true: 0.7
      graph.node.exhaustion_score.low: 0.4
      graph.node.focus_streak.none: 0.3

  # Strategy 6: Validate chain (validation)
  - name: validate_chain
    description: "Reflect back a complete chain — 'So X matters because Y, which connects to Z — does that capture it?' Confirm the laddering is accurate"
    generates_closing_question: true
    signal_weights:
      # Primary trigger — complete chains exist to validate
      graph.chain_completion.has_complete.true: 0.8
      graph.chain_completion.ratio: 0.5  # Higher ratio = more to validate
      graph.max_depth: 0.5
      # Response quality — validate after deep responses
      llm.response_depth.deep: 0.3
      llm.response_depth.moderate: 0.2
      # Uncertainty triggers — validate uncertain chains
      llm.certainty.low: 0.7
      llm.certainty.mid: 0.5
      meta.node.opportunity.probe_deeper: 0.6
      # Engagement
      llm.engagement.low: 0.6           # Low engagement = validate & wrap
      # Diversity (lighter penalty — OK to repeat validation)
      temporal.strategy_repetition_count: -0.5
      # Node-level signals
      graph.node.has_outgoing.true: 0.8  # Well-connected nodes (part of chains)
      graph.node.focus_streak.high: 0.5  # Often-focused = worth validating
      technique.node.strategy_repetition.low: 0.3
      # Saturation triggers
      meta.conversation.saturation: 0.5
      meta.canonical.saturation: 0.3

  # Strategy 7: Revitalize engagement (elaboration)
  - name: revitalize
    description: "Shift to a fresh topic when the respondent shows fatigue — ask about a different product aspect or experience"
    signal_weights:
      # Fatigue detection triggers
      llm.global_response_trend.fatigued: 1.0
      llm.global_response_trend.shallowing: 0.5
      meta.node.opportunity.exhausted: 0.3
      # Engagement triggers
      llm.engagement.low: 0.8
      llm.engagement.mid: 0.4
      llm.engagement.high: -0.4          # Already engaged = not needed
      # Valence
      llm.valence.low: 0.5              # Negative valence = shift gears
      # Diversity
      temporal.strategy_repetition_count: -0.7
      temporal.turns_since_strategy_change: -0.6
      # Node-level signals
      graph.node.focus_streak.none: 0.8  # Fresh nodes
      graph.node.focus_streak.medium: -0.4
      graph.node.focus_streak.high: -0.7
      graph.node.exhaustion_score.low: 0.5

phases:
  early:
    description: "Surface concrete attributes — build the base of the ladder"
    phase_boundaries:
      early_max_turns: 4
      mid_max_turns: 14
    signal_weights:
      elicit_attribute: 1.5
      clarify: 1.2
      ladder_up: 0.5
      anchor_down: 0.3
      validate_chain: 0.2
    phase_bonuses:
      elicit_attribute: 0.2

  mid:
    description: "Ladder up — build chains from attributes to values"
    phase_boundaries:
      early_max_turns: 4
      mid_max_turns: 14
    signal_weights:
      ladder_up: 1.4
      anchor_down: 1.2
      bridge_chains: 1.1
      clarify: 0.8
      revitalize: 1.0
      elicit_attribute: 0.6
      validate_chain: 0.5
    phase_bonuses:
      ladder_up: 0.3
      anchor_down: 0.15
      bridge_chains: 0.1

  late:
    description: "Validate complete chains and bridge remaining gaps"
    phase_boundaries:
      early_max_turns: 4
      mid_max_turns: 14
    signal_weights:
      validate_chain: 1.5
      bridge_chains: 1.3
      ladder_up: 0.7
      anchor_down: 0.5
      elicit_attribute: 0.3
    phase_bonuses:
      validate_chain: 0.2
      bridge_chains: 0.15
