# Spec 2.8: Question Service

## Objective
Create the question service that generates follow-up questions using the LLM client and question prompts.

## Context
- Reference: PRD Section 4.1 (step 6: "Generates natural follow-up question")
- v1 Reference: `src/modules/question_generator.py`
- Phase 2 simplification: Hardcoded "deepen" strategy
- Uses LLM client (2.2), question prompts (2.7), and graph service (2.6) for context

## Input Files
- `src/llm/client.py` - LLM client from spec 2.2
- `src/llm/prompts/question.py` - Prompts from spec 2.7
- `src/services/graph_service.py` - For graph context from spec 2.6
- v1: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/question_generator.py`

## Output Files

### src/services/question_service.py
```python
"""
Question generation service.

Generates follow-up questions based on:
- Strategy (Phase 2: hardcoded "deepen")
- Recent conversation context
- Current graph state
- Focus concept

Uses LLM for natural language generation.
"""

from typing import Optional, List, Dict, Any

import structlog

from src.llm.client import LLMClient, get_llm_client
from src.llm.prompts.question import (
    get_question_system_prompt,
    get_question_user_prompt,
    get_opening_question_system_prompt,
    get_opening_question_user_prompt,
    get_graph_summary,
    format_question,
)
from src.domain.models.knowledge_graph import KGNode, GraphState

log = structlog.get_logger(__name__)


class QuestionService:
    """
    Service for generating interview questions.

    Generates natural, conversational questions using LLM
    based on strategy and conversation context.
    """

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        default_strategy: str = "deepen",
    ):
        """
        Initialize question service.

        Args:
            llm_client: LLM client instance (creates default if None)
            default_strategy: Default strategy for Phase 2 (hardcoded "deepen")
        """
        self.llm = llm_client or get_llm_client()
        self.default_strategy = default_strategy

        log.info("question_service_initialized", default_strategy=default_strategy)

    async def generate_question(
        self,
        focus_concept: str,
        recent_utterances: Optional[List[Dict[str, str]]] = None,
        graph_state: Optional[GraphState] = None,
        recent_nodes: Optional[List[KGNode]] = None,
        strategy: Optional[str] = None,
    ) -> str:
        """
        Generate a follow-up question.

        Args:
            focus_concept: Concept to focus the question on
            recent_utterances: Recent conversation turns
            graph_state: Current graph state for context
            recent_nodes: Recently added nodes
            strategy: Strategy to use (defaults to default_strategy)

        Returns:
            Generated question string

        Raises:
            RuntimeError: If LLM call fails
        """
        strategy = strategy or self.default_strategy

        log.info(
            "generating_question",
            focus=focus_concept,
            strategy=strategy,
        )

        # Build graph summary if we have state
        graph_summary = None
        if graph_state and recent_nodes:
            recent_concepts = [n.label for n in recent_nodes[:3]]
            graph_summary = get_graph_summary(
                nodes_by_type=graph_state.nodes_by_type,
                recent_concepts=recent_concepts,
                depth_achieved=graph_state.max_depth,
            )

        # Get prompts
        system_prompt = get_question_system_prompt(strategy)
        user_prompt = get_question_user_prompt(
            focus_concept=focus_concept,
            recent_utterances=recent_utterances,
            graph_summary=graph_summary,
            strategy=strategy,
        )

        try:
            response = await self.llm.complete(
                prompt=user_prompt,
                system=system_prompt,
                temperature=0.8,  # Higher for variety
                max_tokens=200,
            )

            question = format_question(response.content)

            log.info(
                "question_generated",
                strategy=strategy,
                question_length=len(question),
                latency_ms=response.latency_ms,
            )

            return question

        except Exception as e:
            log.error("question_generation_failed", error=str(e))
            raise RuntimeError(f"Question generation failed: {e}")

    async def generate_opening_question(
        self,
        concept_name: str,
        concept_description: str = "",
    ) -> str:
        """
        Generate an opening question for a new session.

        Args:
            concept_name: Name of the concept/product
            concept_description: Optional description

        Returns:
            Opening question string

        Raises:
            RuntimeError: If LLM call fails
        """
        log.info("generating_opening_question", concept=concept_name)

        system_prompt = get_opening_question_system_prompt()
        user_prompt = get_opening_question_user_prompt(
            concept_name=concept_name,
            description=concept_description,
        )

        try:
            response = await self.llm.complete(
                prompt=user_prompt,
                system=system_prompt,
                temperature=0.9,  # Even higher for variety
                max_tokens=150,
            )

            question = format_question(response.content)

            log.info(
                "opening_question_generated",
                concept=concept_name,
                question_length=len(question),
            )

            return question

        except Exception as e:
            log.error("opening_question_failed", error=str(e))
            raise RuntimeError(f"Opening question generation failed: {e}")

    def select_focus_concept(
        self,
        recent_nodes: List[KGNode],
        graph_state: GraphState,
        strategy: Optional[str] = None,
    ) -> str:
        """
        Select which concept to focus the next question on.

        Phase 2: Simple heuristics
        Phase 3: Full strategy-based selection

        Args:
            recent_nodes: Recently added nodes
            graph_state: Current graph state
            strategy: Strategy (affects selection)

        Returns:
            Concept label to focus on
        """
        strategy = strategy or self.default_strategy

        if not recent_nodes:
            return "the topic"  # Fallback

        if strategy == "deepen":
            # Focus on most recent concept to ladder up
            return recent_nodes[0].label

        elif strategy == "broaden":
            # Focus on a recent concept but ask for alternatives
            return recent_nodes[0].label

        elif strategy == "cover":
            # Would look at uncovered elements (Phase 3)
            return recent_nodes[0].label

        elif strategy == "close":
            # Summarize what we've learned
            return "what we've discussed"

        # Default: most recent
        return recent_nodes[0].label

    async def generate_fallback_question(self, focus_concept: str) -> str:
        """
        Generate a simple fallback question without LLM.

        Used when LLM is unavailable.

        Args:
            focus_concept: Concept to ask about

        Returns:
            Simple fallback question
        """
        log.warning("using_fallback_question", focus=focus_concept)

        # Simple laddering fallback
        return f"Why is {focus_concept} important to you?"
```

### tests/unit/test_question_service.py
```python
"""Tests for question service."""

import pytest
from unittest.mock import AsyncMock, MagicMock

from src.services.question_service import QuestionService
from src.llm.client import LLMResponse
from src.domain.models.knowledge_graph import KGNode, GraphState


@pytest.fixture
def mock_llm():
    """Create mock LLM client."""
    mock = AsyncMock()
    return mock


@pytest.fixture
def service(mock_llm):
    """Create question service with mock LLM."""
    return QuestionService(llm_client=mock_llm)


@pytest.fixture
def sample_graph_state():
    """Create sample graph state."""
    return GraphState(
        node_count=5,
        edge_count=3,
        nodes_by_type={"attribute": 2, "functional_consequence": 3},
        max_depth=2,
    )


@pytest.fixture
def sample_nodes():
    """Create sample recent nodes."""
    return [
        KGNode(
            id="n1", session_id="s1", label="creamy texture", node_type="attribute"
        ),
        KGNode(
            id="n2", session_id="s1", label="satisfying", node_type="functional_consequence"
        ),
    ]


class TestGenerateQuestion:
    """Tests for generate_question."""

    @pytest.mark.asyncio
    async def test_returns_question(self, service, mock_llm):
        """generate_question returns formatted question."""
        mock_llm.complete.return_value = LLMResponse(
            content="Why is that important to you?",
            model="test",
            latency_ms=100,
        )

        question = await service.generate_question(
            focus_concept="creamy texture",
        )

        assert question == "Why is that important to you?"

    @pytest.mark.asyncio
    async def test_uses_default_strategy(self, service, mock_llm):
        """Uses default strategy when not specified."""
        mock_llm.complete.return_value = LLMResponse(
            content="Test question?",
            model="test",
        )

        await service.generate_question(focus_concept="test")

        # Check that deepen strategy was used in system prompt
        call_args = mock_llm.complete.call_args
        assert "Deepen" in call_args.kwargs["system"]

    @pytest.mark.asyncio
    async def test_includes_context(self, service, mock_llm, sample_graph_state, sample_nodes):
        """Includes graph context in prompt."""
        mock_llm.complete.return_value = LLMResponse(
            content="Follow-up question?",
            model="test",
        )

        await service.generate_question(
            focus_concept="texture",
            graph_state=sample_graph_state,
            recent_nodes=sample_nodes,
        )

        call_args = mock_llm.complete.call_args
        prompt = call_args.kwargs["prompt"]

        # Should include graph summary
        assert "concepts" in prompt.lower() or "depth" in prompt.lower()

    @pytest.mark.asyncio
    async def test_includes_utterances(self, service, mock_llm):
        """Includes recent utterances in prompt."""
        mock_llm.complete.return_value = LLMResponse(
            content="Question?",
            model="test",
        )

        utterances = [
            {"speaker": "system", "text": "What do you think?"},
            {"speaker": "user", "text": "I love the taste"},
        ]

        await service.generate_question(
            focus_concept="taste",
            recent_utterances=utterances,
        )

        call_args = mock_llm.complete.call_args
        prompt = call_args.kwargs["prompt"]

        assert "I love the taste" in prompt

    @pytest.mark.asyncio
    async def test_formats_question(self, service, mock_llm):
        """Formats question output."""
        mock_llm.complete.return_value = LLMResponse(
            content='"Why is that important"',  # With quotes, no ?
            model="test",
        )

        question = await service.generate_question(focus_concept="test")

        assert not question.startswith('"')
        assert question.endswith("?")

    @pytest.mark.asyncio
    async def test_raises_on_llm_error(self, service, mock_llm):
        """Raises RuntimeError on LLM failure."""
        mock_llm.complete.side_effect = Exception("API error")

        with pytest.raises(RuntimeError, match="Question generation failed"):
            await service.generate_question(focus_concept="test")


class TestGenerateOpeningQuestion:
    """Tests for generate_opening_question."""

    @pytest.mark.asyncio
    async def test_returns_opening(self, service, mock_llm):
        """generate_opening_question returns formatted question."""
        mock_llm.complete.return_value = LLMResponse(
            content="What comes to mind when you think about Oat Milk?",
            model="test",
        )

        question = await service.generate_opening_question(
            concept_name="Oat Milk",
        )

        assert "Oat Milk" in question or len(question) > 10

    @pytest.mark.asyncio
    async def test_includes_description(self, service, mock_llm):
        """Includes description in prompt when provided."""
        mock_llm.complete.return_value = LLMResponse(
            content="Opening question?",
            model="test",
        )

        await service.generate_opening_question(
            concept_name="Oat Milk",
            concept_description="Plant-based milk alternative",
        )

        call_args = mock_llm.complete.call_args
        prompt = call_args.kwargs["prompt"]

        assert "Plant-based" in prompt


class TestSelectFocusConcept:
    """Tests for select_focus_concept."""

    def test_returns_most_recent_for_deepen(self, service, sample_nodes, sample_graph_state):
        """Returns most recent node for deepen strategy."""
        focus = service.select_focus_concept(
            recent_nodes=sample_nodes,
            graph_state=sample_graph_state,
            strategy="deepen",
        )

        assert focus == "creamy texture"

    def test_returns_fallback_when_empty(self, service, sample_graph_state):
        """Returns fallback when no recent nodes."""
        focus = service.select_focus_concept(
            recent_nodes=[],
            graph_state=sample_graph_state,
        )

        assert focus == "the topic"


class TestFallbackQuestion:
    """Tests for fallback question."""

    @pytest.mark.asyncio
    async def test_generates_simple_question(self, service):
        """Fallback generates simple laddering question."""
        question = await service.generate_fallback_question("texture")

        assert "texture" in question
        assert "important" in question.lower()
```

## Requirements
1. Must use LLM client from spec 2.2
2. Must use prompts from spec 2.7
3. Phase 2: Default to "deepen" strategy
4. Must format question output (remove quotes, add punctuation)
5. Must provide fallback question on LLM failure
6. Must log question generation with latency
7. Must handle empty/missing context gracefully

## Verification
```bash
# Run question service tests
pytest tests/unit/test_question_service.py -v

# Verify imports
python3 -c "from src.services.question_service import QuestionService; print('Question service imported')"

# Test fallback question
python3 -c "
import asyncio
from src.services.question_service import QuestionService

async def test():
    service = QuestionService.__new__(QuestionService)
    service.default_strategy = 'deepen'
    q = await service.generate_fallback_question('texture')
    print(f'Fallback: {q}')

asyncio.run(test())
"
```

## Success Criteria
- [ ] `QuestionService` class with LLM client injection
- [ ] `generate_question()` uses strategy and context
- [ ] `generate_opening_question()` for session start
- [ ] `select_focus_concept()` picks concept based on strategy
- [ ] `generate_fallback_question()` for LLM failures
- [ ] Questions formatted (quotes removed, punctuation added)
- [ ] Default strategy is "deepen" (Phase 2)
- [ ] All tests pass with mocked LLM
