# Spec 2.2: LLM Client

## Objective
Create the LLM client abstraction for Anthropic API calls following the Engineering Guide specification.

## Context
- Reference: ENGINEERING_GUIDE.md Section 6 (LLM Integration)
- Single provider (Anthropic) for v2 MVP - no fallback chains
- Uses httpx for async HTTP calls
- Structured logging of all LLM calls

## Input Files
- `ENGINEERING_GUIDE.md` - Section 6 has complete implementation
- `src/core/config.py` - Settings for API key, model, timeouts

## Output Files

### src/llm/client.py
```python
"""
LLM client abstraction for Anthropic API.

Provides async interface for LLM calls with:
- Structured logging of requests/responses
- Timeout handling
- Usage tracking (tokens)

Single provider for v2 MVP. Extensible for future providers.
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Optional, Dict, Any
import time

import httpx
import structlog

from src.core.config import settings

log = structlog.get_logger(__name__)


@dataclass
class LLMResponse:
    """Standardized LLM response."""
    content: str
    model: str
    usage: Dict[str, int] = field(default_factory=dict)
    latency_ms: float = 0.0
    raw_response: Optional[Dict[str, Any]] = None


class LLMClient(ABC):
    """Abstract base for LLM providers."""

    @abstractmethod
    async def complete(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> LLMResponse:
        """
        Generate a completion from the LLM.

        Args:
            prompt: User message/prompt
            system: Optional system prompt
            temperature: Sampling temperature (0.0-2.0)
            max_tokens: Maximum tokens in response

        Returns:
            LLMResponse with content and metadata
        """
        pass


class AnthropicClient(LLMClient):
    """Anthropic Claude API client.

    Uses httpx for async HTTP calls to the Messages API.
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: Optional[str] = None,
        timeout: Optional[float] = None,
    ):
        """
        Initialize Anthropic client.

        Args:
            api_key: API key (defaults to settings.anthropic_api_key)
            model: Model ID (defaults to settings.llm_model)
            timeout: Request timeout in seconds (defaults to settings.llm_timeout_seconds)

        Raises:
            ValueError: If API key is not configured
        """
        self.api_key = api_key or settings.anthropic_api_key
        self.model = model or settings.llm_model
        self.base_url = "https://api.anthropic.com/v1"
        self.timeout = timeout or settings.llm_timeout_seconds

        if not self.api_key:
            raise ValueError(
                "ANTHROPIC_API_KEY not configured. "
                "Set it in .env or pass api_key parameter."
            )

        log.info(
            "anthropic_client_initialized",
            model=self.model,
            timeout=self.timeout,
        )

    async def complete(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> LLMResponse:
        """
        Call Anthropic Messages API.

        Args:
            prompt: User message
            system: Optional system prompt
            temperature: Sampling temperature (defaults to settings.llm_temperature)
            max_tokens: Max tokens (defaults to settings.llm_max_tokens)

        Returns:
            LLMResponse with content and usage stats

        Raises:
            httpx.HTTPStatusError: On API errors (4xx, 5xx)
            httpx.TimeoutException: On timeout
        """
        start = time.perf_counter()

        headers = {
            "x-api-key": self.api_key,
            "content-type": "application/json",
            "anthropic-version": "2023-06-01",
        }

        payload: Dict[str, Any] = {
            "model": self.model,
            "max_tokens": max_tokens or settings.llm_max_tokens,
            "messages": [{"role": "user", "content": prompt}],
        }

        if system:
            payload["system"] = system

        if temperature is not None:
            payload["temperature"] = temperature
        else:
            payload["temperature"] = settings.llm_temperature

        log.debug(
            "llm_call_start",
            model=self.model,
            prompt_length=len(prompt),
            system_length=len(system) if system else 0,
        )

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            response = await client.post(
                f"{self.base_url}/messages",
                headers=headers,
                json=payload,
            )
            response.raise_for_status()
            data = response.json()

        latency_ms = (time.perf_counter() - start) * 1000

        # Extract content from response
        content = ""
        if data.get("content"):
            content = data["content"][0].get("text", "")

        usage = {
            "input_tokens": data.get("usage", {}).get("input_tokens", 0),
            "output_tokens": data.get("usage", {}).get("output_tokens", 0),
        }

        log.info(
            "llm_call_complete",
            model=self.model,
            latency_ms=round(latency_ms, 2),
            input_tokens=usage["input_tokens"],
            output_tokens=usage["output_tokens"],
        )

        return LLMResponse(
            content=content,
            model=data.get("model", self.model),
            usage=usage,
            latency_ms=latency_ms,
            raw_response=data,
        )


def get_llm_client() -> LLMClient:
    """
    Factory for LLM client based on configuration.

    Returns:
        LLMClient instance (AnthropicClient for v2)

    Raises:
        ValueError: If unknown provider configured
    """
    provider = settings.llm_provider

    if provider == "anthropic":
        return AnthropicClient()
    else:
        raise ValueError(f"Unknown LLM provider: {provider}")
```

### tests/unit/test_llm_client.py
```python
"""Tests for LLM client."""

import pytest
from unittest.mock import AsyncMock, patch, MagicMock

from src.llm.client import AnthropicClient, LLMResponse, get_llm_client


class TestAnthropicClient:
    """Tests for AnthropicClient."""

    def test_init_with_api_key(self):
        """Client initializes with explicit API key."""
        client = AnthropicClient(api_key="test-key")
        assert client.api_key == "test-key"

    def test_init_without_api_key_raises(self):
        """Client raises if no API key available."""
        with patch("src.llm.client.settings") as mock_settings:
            mock_settings.anthropic_api_key = None
            with pytest.raises(ValueError, match="ANTHROPIC_API_KEY"):
                AnthropicClient()

    def test_init_uses_settings_defaults(self):
        """Client uses settings for defaults."""
        with patch("src.llm.client.settings") as mock_settings:
            mock_settings.anthropic_api_key = "settings-key"
            mock_settings.llm_model = "claude-sonnet-4-20250514"
            mock_settings.llm_timeout_seconds = 30.0

            client = AnthropicClient()

            assert client.api_key == "settings-key"
            assert client.model == "claude-sonnet-4-20250514"
            assert client.timeout == 30.0

    @pytest.mark.asyncio
    async def test_complete_success(self):
        """complete() returns LLMResponse on success."""
        mock_response = {
            "content": [{"type": "text", "text": "Hello, world!"}],
            "model": "claude-sonnet-4-20250514",
            "usage": {"input_tokens": 10, "output_tokens": 5},
        }

        with patch("httpx.AsyncClient") as MockClient:
            mock_client = AsyncMock()
            mock_response_obj = MagicMock()
            mock_response_obj.json.return_value = mock_response
            mock_response_obj.raise_for_status = MagicMock()
            mock_client.post.return_value = mock_response_obj
            MockClient.return_value.__aenter__.return_value = mock_client

            client = AnthropicClient(api_key="test-key")
            response = await client.complete("Say hello")

            assert isinstance(response, LLMResponse)
            assert response.content == "Hello, world!"
            assert response.usage["input_tokens"] == 10
            assert response.usage["output_tokens"] == 5

    @pytest.mark.asyncio
    async def test_complete_with_system_prompt(self):
        """complete() includes system prompt in request."""
        mock_response = {
            "content": [{"type": "text", "text": "Response"}],
            "model": "claude-sonnet-4-20250514",
            "usage": {"input_tokens": 20, "output_tokens": 5},
        }

        with patch("httpx.AsyncClient") as MockClient:
            mock_client = AsyncMock()
            mock_response_obj = MagicMock()
            mock_response_obj.json.return_value = mock_response
            mock_response_obj.raise_for_status = MagicMock()
            mock_client.post.return_value = mock_response_obj
            MockClient.return_value.__aenter__.return_value = mock_client

            client = AnthropicClient(api_key="test-key")
            await client.complete("User message", system="You are helpful")

            # Verify system was included in payload
            call_args = mock_client.post.call_args
            payload = call_args.kwargs["json"]
            assert payload["system"] == "You are helpful"


class TestGetLLMClient:
    """Tests for get_llm_client factory."""

    def test_returns_anthropic_client(self):
        """Factory returns AnthropicClient for anthropic provider."""
        with patch("src.llm.client.settings") as mock_settings:
            mock_settings.llm_provider = "anthropic"
            mock_settings.anthropic_api_key = "test-key"
            mock_settings.llm_model = "claude-sonnet-4-20250514"
            mock_settings.llm_timeout_seconds = 30.0

            client = get_llm_client()

            assert isinstance(client, AnthropicClient)

    def test_raises_for_unknown_provider(self):
        """Factory raises for unknown provider."""
        with patch("src.llm.client.settings") as mock_settings:
            mock_settings.llm_provider = "openai"

            with pytest.raises(ValueError, match="Unknown LLM provider"):
                get_llm_client()


class TestLLMResponse:
    """Tests for LLMResponse dataclass."""

    def test_create_response(self):
        """LLMResponse can be created with all fields."""
        response = LLMResponse(
            content="Hello",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 10, "output_tokens": 5},
            latency_ms=150.5,
        )

        assert response.content == "Hello"
        assert response.model == "claude-sonnet-4-20250514"
        assert response.latency_ms == 150.5

    def test_response_defaults(self):
        """LLMResponse has sensible defaults."""
        response = LLMResponse(content="Hi", model="test")

        assert response.usage == {}
        assert response.latency_ms == 0.0
        assert response.raw_response is None
```

## Requirements
1. Must follow ENGINEERING_GUIDE.md Section 6 exactly
2. Use httpx for async HTTP (not requests or aiohttp)
3. Log all LLM calls with structlog (start, complete, or error)
4. Include usage tracking (input/output tokens)
5. Raise on HTTP errors (don't silently fail)
6. Support configurable timeout
7. Use settings for defaults (API key, model, temperature, max_tokens)

## Verification
```bash
# Run LLM client tests
pytest tests/unit/test_llm_client.py -v

# Verify imports work
python3 -c "from src.llm.client import AnthropicClient, LLMResponse, get_llm_client; print('LLM client imported')"

# Test initialization (without actual API call)
python3 -c "
from src.llm.client import AnthropicClient
try:
    client = AnthropicClient(api_key='test-key')
    print(f'Client initialized with model: {client.model}')
except Exception as e:
    print(f'Error: {e}')
"
```

## Success Criteria
- [ ] `LLMResponse` dataclass with content, model, usage, latency_ms fields
- [ ] `LLMClient` abstract base class with `complete()` method
- [ ] `AnthropicClient` implementation using httpx
- [ ] `get_llm_client()` factory function
- [ ] Raises `ValueError` if API key not configured
- [ ] Logs LLM calls with structlog (llm_call_start, llm_call_complete)
- [ ] Includes token usage in response
- [ ] All tests pass with mocked HTTP calls
