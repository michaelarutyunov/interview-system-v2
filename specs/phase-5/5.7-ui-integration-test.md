# Spec 5.7: UI Integration Test

## Objective
Create manual and automated tests for the Streamlit demo UI.

## Context
- Reference: IMPLEMENTATION_PLAN.md Phase 5 Gate Criteria
- Manual testing for UI components (difficult to automate Streamlit)
- Smoke tests for API connectivity
- Component-level validation
- End-to-end workflow testing

## Input Files
- Phase 5.6: `ui/streamlit_app.py` - Main app
- All Phase 5 components

## Output Files

### tests/ui/test_smoke.py
```python
"""
Smoke tests for Streamlit UI components.

Tests basic functionality without running the full Streamlit app.
Validates component initialization and basic operations.
"""

import pytest
from unittest.mock import MagicMock, AsyncMock

from ui.api_client import APIClient, SessionInfo
from ui.components.chat import ChatInterface, initialize_chat_state
from ui.components.graph import GraphVisualizer
from ui.components.metrics import MetricsPanel
from ui.components.controls import SessionControls, initialize_session_state


class TestAPIClient:
    """Tests for API client."""
    
    def test_init_with_default_url(self):
        """Client initializes with default URL."""
        client = APIClient()
        assert client.base_url == "http://localhost:8000"
    
    def test_init_with_custom_url(self):
        """Client accepts custom base URL."""
        client = APIClient(base_url="http://localhost:9000")
        assert client.base_url == "http://localhost:9000"
    
    @pytest.mark.asyncio
    async def test_create_session_makes_correct_request(self):
        """create_session makes correct API call."""
        import httpx
        
        mock_response = MagicMock()
        mock_response.json.return_value = {
            "id": "test-session-123",
            "concept_id": "oat_milk_v1",
            "status": "active",
            "opening_question": "What comes to mind?",
        }
        mock_response.raise_for_status = MagicMock()
        
        with pytest.MonkeyPatch.context() as m:
            async def mock_post(*args, **kwargs):
                return mock_response
            
            # Test would use actual httpx.AsyncClient mocking
            # This is simplified for demonstration
            client = APIClient()
            # result = await client.create_session("oat_milk_v1")
            # assert result.id == "test-session-123"


class TestChatInterface:
    """Tests for chat interface component."""
    
    def test_init_with_api_client(self):
        """ChatInterface initializes with API client."""
        mock_client = MagicMock(spec=APIClient)
        chat = ChatInterface(mock_client)
        assert chat.api_client == mock_client
        assert chat.max_history == 100
    
    def test_init_with_custom_max_history(self):
        """ChatInterface accepts custom max_history."""
        mock_client = MagicMock(spec=APIClient)
        chat = ChatInterface(mock_client)
        # Would test custom max_history if implemented
        assert chat.max_history == 100
    
    def test_clear_history_clears_session_state(self):
        """clear_history() clears chat history from session state."""
        mock_client = MagicMock(spec=APIClient)
        chat = ChatInterface(mock_client)
        
        # Simulate session state
        import streamlit as st
        if not hasattr(st, "session_state"):
            st.session_state = MagicMock()
        
        st.session_state.chat_history = [
            {"role": "user", "content": "test"},
            {"role": "assistant", "content": "response"},
        ]
        st.session_state.opening_displayed = True
        
        chat.clear_history()
        
        assert st.session_state.chat_history == []
        assert not hasattr(st.session_state, "opening_displayed")


class TestGraphVisualizer:
    """Tests for graph visualizer component."""
    
    def test_init_creates_layout_algorithms(self):
        """GraphVisualizer initializes with layout algorithms."""
        visualizer = GraphVisualizer()
        
        assert "Spring" in visualizer.layout_algorithms
        assert "Kamada-Kawai" in visualizer.layout_algorithms
        assert "Circular" in visualizer.layout_algorithms
    
    def test_node_colors_defined(self):
        """Node type colors are defined."""
        visualizer = GraphVisualizer()
        
        assert "attribute" in visualizer.NODE_COLORS
        assert "functional_consequence" in visualizer.NODE_COLORS
        assert "terminal_value" in visualizer.NODE_COLORS
    
    def test_render_controls_returns_dict(self):
        """render_controls returns control options dict."""
        visualizer = GraphVisualizer()
        
        # This would use Streamlit mocking in real test
        # Simplified here for demonstration
        expected_keys = {"layout", "dimensions", "show_labels", "node_filter"}
        # result = visualizer.render_controls()
        # assert set(result.keys()) == expected_keys


class TestMetricsPanel:
    """Tests for metrics panel component."""
    
    def test_init_creates_panel(self):
        """MetricsPanel initializes successfully."""
        panel = MetricsPanel()
        assert panel.coverage_emoji == ["â¬œ", "ðŸŸ©"]
    
    def test_coverage_emoji_length(self):
        """Coverage emoji has 2 states."""
        panel = MetricsPanel()
        assert len(panel.coverage_emoji) == 2
    
    def test_render_accepts_status_data(self):
        """render accepts status data dict."""
        panel = MetricsPanel()
        
        status_data = {
            "turn_number": 5,
            "max_turns": 20,
            "coverage": 0.6,
            "status": "active",
            "scoring": {
                "coverage": 0.6,
                "depth": 0.4,
                "saturation": 0.1,
            },
        }
        
        # Would use Streamlit mocking in real test
        # panel.render(status_data)


class TestSessionControls:
    """Tests for session controls component."""
    
    def test_init_with_api_client(self):
        """SessionControls initializes with API client."""
        mock_client = MagicMock(spec=APIClient)
        controls = SessionControls(mock_client)
        assert controls.api_client == mock_client


class TestSessionStateInitialization:
    """Tests for session state initialization functions."""
    
    def test_initialize_session_state_sets_defaults(self):
        """initialize_session_state() sets default values."""
        import streamlit as st
        if not hasattr(st, "session_state"):
            st.session_state = MagicMock()
        
        initialize_session_state()
        
        # Check defaults were set
        assert hasattr(st.session_state, "current_session")
        assert hasattr(st.session_state, "sessions")
        assert hasattr(st.session_state, "confirm_delete")
    
    def test_initialize_chat_state_sets_defaults(self):
        """initialize_chat_state() sets default values."""
        import streamlit as st
        if not hasattr(st, "session_state"):
            st.session_state = MagicMock()
        
        initialize_chat_state()
        
        assert hasattr(st.session_state, "chat_history")
        assert hasattr(st.session_state, "opening_displayed")


@pytest.fixture
def mock_streamlit_state():
    """Fixture to mock Streamlit session state."""
    import streamlit as st
    
    if not hasattr(st, "session_state"):
        st.session_state = MagicMock()
    
    # Set up default state
    st.session_state.current_session = None
    st.session_state.chat_history = []
    st.session_state.opening_displayed = False
    
    yield st.session_state
    
    # Cleanup
    delattr(st, "session_state")


@pytest.fixture
def sample_graph_data():
    """Fixture providing sample graph data."""
    return {
        "nodes": [
            {
                "id": "n1",
                "label": "creamy texture",
                "node_type": "attribute",
                "confidence": 0.9,
            },
            {
                "id": "n2",
                "label": "satisfying",
                "node_type": "functional_consequence",
                "confidence": 0.8,
            },
        ],
        "edges": [
            {
                "id": "e1",
                "source_node_id": "n1",
                "target_node_id": "n2",
                "edge_type": "leads_to",
                "confidence": 0.8,
            },
        ],
    }


@pytest.fixture
def sample_status_data():
    """Fixture providing sample status data."""
    return {
        "turn_number": 3,
        "max_turns": 20,
        "coverage": 0.4,
        "target_coverage": 0.8,
        "status": "active",
        "should_continue": True,
        "strategy_selected": "deepen",
        "strategy_reasoning": "Shallow chain, explore deeper",
        "scoring": {
            "coverage": 0.4,
            "depth": 0.2,
            "saturation": 0.0,
            "novelty": 1.0,
            "richness": 0.8,
        },
    }
```

### tests/ui/manual_test_checklist.md
```markdown
# UI Manual Test Checklist

## Prerequisites

- [ ] FastAPI backend running on http://localhost:8000
- [ ] Streamlit UI running on http://localhost:8501
- [ ] Test concept configuration loaded

## Session Management

### Create New Session
- [ ] Click "New Session" tab
- [ ] Select concept from dropdown
- [ ] Click "Start Interview" button
- [ ] Verify session created successfully
- [ ] Verify session ID displayed
- [ ] Verify opening question appears in chat

### Load Existing Session
- [ ] Click "Sessions" tab
- [ ] Verify session list loads
- [ ] Select a session from dropdown
- [ ] View session details in expander
- [ ] Click "Load" button
- [ ] Verify session loads correctly
- [ ] Verify chat history appears

### Delete Session
- [ ] Select a session
- [ ] Click "Delete" button
- [ ] Verify confirmation required (second click)
- [ ] Verify session removed from list
- [ ] Verify current session cleared if it was active

## Chat Interface

### Opening Question
- [ ] Opening question displayed when session loads
- [ ] Opening question in assistant message style
- [ ] Chat history shows opening question

### Send Response
- [ ] Type message in chat input
- [ ] Press Enter or click send
- [ ] Verify message appears in chat (user style)
- [ ] Verify assistant response appears
- [ ] Verify response is a question
- [ ] Verify turn counter increments

### Chat History
- [ ] Multiple messages display correctly
- [ ] User and assistant messages styled differently
- [ ] Chat scrolls for long conversations
- [ ] History persists across page refresh

## Knowledge Graph

### Graph Display
- [ ] Navigate to "Knowledge Graph" tab
- [ ] Verify graph displays after first turn
- [ ] Verify nodes colored by type
- [ ] Verify edges connect nodes
- [ ] Verify hover shows node details

### Graph Controls
- [ ] Select different layout algorithm
- [ ] Verify graph updates with new layout
- [ ] Toggle node labels on/off
- [ ] Verify labels appear/disappear
- [ ] Filter by node type
- [ ] Verify only selected types shown

### 3D View
- [ ] Select "3D" view option
- [ ] Verify 3D graph renders
- [ ] Verify graph is rotatable
- [ ] Verify hover still works in 3D

### Graph Stats
- [ ] Verify node count displayed
- [ ] Verify edge count displayed
- [ ] Verify nodes-by-type breakdown shown

## Metrics Panel

### Main Metrics
- [ ] Verify turn count displayed
- [ ] Verify turn progress bar shown
- [ ] Verify coverage percentage displayed
- [ ] Verify coverage visual bar shown
- [ ] Verify status displayed with emoji

### Scoring Breakdown
- [ ] Verify gauge charts for each score
- [ ] Verify Coverage gauge works
- [ ] Verify Depth gauge works
- [ ] Verify Saturation gauge works
- [ ] Verify Novelty gauge works
- [ ] Verify Richness gauge works

### Strategy Display
- [ ] Verify current strategy shown
- [ ] Verify strategy description displayed
- [ ] Click strategy reasoning expander
- [ ] Verify reasoning text appears

### Graph Stats in Metrics
- [ ] Verify node/edge counts shown
- [ ] Verify node type pie chart displayed

## Export

### Export JSON
- [ ] Select active session
- [ ] Go to "Export" tab
- [ ] Select "JSON" format
- [ ] Click "Export" button
- [ ] Verify download button appears
- [ ] Click download
- [ ] Verify file downloads with correct name
- [ ] Open file, verify valid JSON

### Export Markdown
- [ ] Select "Markdown" format
- [ ] Click "Export" button
- [ ] Verify download button appears
- [ ] Click download
- [ ] Verify file downloads with .md extension

## API Connection

### Change API URL
- [ ] Enter different API URL in sidebar
- [ ] Click "Reconnect" button
- [ ] Verify success message appears
- [ ] Verify API calls work with new URL

### API Failure Handling
- [ ] Stop the FastAPI backend
- [ ] Try to create a session
- [ ] Verify error message displayed
- [ ] Restart backend
- [ ] Click "Reconnect"
- [ ] Verify operations work again

## End-to-End Interview Flow

### Complete Short Interview
- [ ] Create new session
- [ ] Respond to opening question
- [ ] Respond to 3-5 follow-up questions
- [ ] Verify knowledge graph builds
- [ ] Verify coverage increases
- [ ] Verify strategy changes appropriately
- [ ] Export session data
- [ ] Verify export contains all turns

### Session Completion
- [ ] Run interview until should_continue=false
- [ ] Verify completion message shown
- [ ] Verify final status displayed
- [ ] Verify final coverage achieved
- [ ] Export completed session

## Browser Compatibility

### Chrome/Edge
- [ ] Test all features in Chrome
- [ ] Test all features in Edge

### Firefox
- [ ] Test all features in Firefox

### Safari
- [ ] Test all features in Safari (if on Mac)

## Performance

### Large Interview
- [ ] Create interview with 20+ turns
- [ ] Verify chat remains responsive
- [ ] Verify graph renders quickly
- [ ] Verify metrics update smoothly
- [ ] Verify no memory issues

### Large Graph
- [ ] Create graph with 50+ nodes
- [ ] Verify graph renders
- [ ] Verify interactivity maintained
- [ ] Verify filtering works

## Accessibility

### Keyboard Navigation
- [ ] Tab through interface elements
- [ ] Verify focus visible
- [ ] Verify Enter key submits chat

### Screen Reader
- [ ] Test with screen reader (if available)
- [ ] Verify alt text for graphs
- [ ] Verify logical reading order
```

## Requirements
1. Unit tests for component initialization
2. Unit tests for basic operations
3. Comprehensive manual test checklist
4. Fixtures for sample data
5. Tests for session state management
6. API connectivity validation

## Verification
```bash
# Run unit tests
pytest tests/ui/test_smoke.py -v

# Run specific test class
pytest tests/ui/test_smoke.py::TestChatInterface -v

# Run with coverage
pytest tests/ui/ --cov=ui --cov-report=term-missing
```

## Success Criteria
- [ ] `tests/ui/test_smoke.py` created with unit tests
- [ ] `tests/ui/manual_test_checklist.md` created
- [ ] All unit tests pass
- [ ] Manual checklist covers all features
- [ ] Sample data fixtures provided
- [ ] Tests validate component initialization
- [ ] Tests validate basic operations
- [ ] Manual checklist includes browser compatibility
