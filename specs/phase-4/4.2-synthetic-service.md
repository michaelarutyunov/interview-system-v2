# Spec 4.2: Synthetic Service

## Objective
Create the service layer for generating synthetic respondent answers.

## Context
- Reference: PRD Section 4.6 (Synthetic Respondent), Section 6.2 (Flow 2)
- Uses LLM client to generate contextually appropriate responses
- Integrates with knowledge graph for concept awareness
- Supports persona selection for varied test scenarios
- Async service following project patterns

## Input Files
- Phase 2: `src/llm/client.py` - LLM client
- Phase 2: `src/llm/prompts/synthetic.py` - Synthetic prompts
- Phase 2: `src/persistence/repositories/graph_repo.py` - Graph repository
- Phase 2: `src/domain/models/knowledge_graph.py` - Graph models

## Output Files

### src/services/synthetic_service.py
```python
"""
Synthetic respondent service for automated testing.

Generates plausible respondent answers based on:
- Current interview question
- Knowledge graph state (previous concepts)
- Configured persona
- Interview context

Enables rapid iteration without manual testing.
"""

from typing import Dict, Any, List, Optional
import uuid

import structlog

from src.llm.client import LLMClient, get_llm_client
from src.llm.prompts.synthetic import (
    get_synthetic_system_prompt,
    get_synthetic_system_prompt_with_deflection,
    get_synthetic_user_prompt,
    parse_synthetic_response,
    get_available_personas,
)
from src.domain.models.knowledge_graph import GraphState


log = structlog.get_logger(__name__)


class SyntheticService:
    """
    Service for generating synthetic respondent responses.
    
    Usage:
        service = SyntheticService(llm_client)
        response = await service.generate_response(
            question="Why is creamy texture important?",
            session_id="session-123",
            persona="health_conscious",
        )
    """
    
    DEFAULT_PERSONA = "health_conscious"
    DEFLECTION_CHANCE = 0.2  # 20% of responses use deflection prompt
    
    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        deflection_chance: float = DEFLECTION_CHANCE,
    ):
        """
        Initialize synthetic service.
        
        Args:
            llm_client: LLM client (uses get_llm_client() if None)
            deflection_chance: Probability (0-1) of using deflection prompt
        """
        self.llm_client = llm_client or get_llm_client()
        self.deflection_chance = deflection_chance
        
        log.info(
            "synthetic_service_initialized",
            deflection_chance=deflection_chance,
        )
    
    async def generate_response(
        self,
        question: str,
        session_id: str,
        persona: str = DEFAULT_PERSONA,
        graph_state: Optional[GraphState] = None,
        interview_context: Optional[Dict[str, Any]] = None,
        use_deflection: Optional[bool] = None,
    ) -> Dict[str, Any]:
        """
        Generate a synthetic response to an interview question.
        
        Args:
            question: The interviewer's question
            session_id: Session identifier for logging
            persona: Persona to use (must be in get_available_personas())
            graph_state: Optional current graph state (for concept awareness)
            interview_context: Optional context dict with:
                - product_name: str
                - turn_number: int
                - coverage_achieved: float
            use_deflection: Force deflection on/off (None=random based on chance)
        
        Returns:
            Dict with:
                - response: str (cleaned synthetic response)
                - persona: str (persona used)
                - question: str (original question)
                - latency_ms: float (LLM call latency)
                - tokens_used: dict (input/output tokens)
        
        Raises:
            ValueError: If persona is not available
        """
        log_ctx = log.bind(session_id=session_id, persona=persona)
        
        # Validate persona
        available = get_available_personas()
        if persona not in available:
            raise ValueError(
                f"Unknown persona: {persona}. "
                f"Available: {list(available.keys())}"
            )
        
        # Extract previous concepts from graph state
        previous_concepts = self._extract_previous_concepts(graph_state)
        
        # Build prompts
        if use_deflection is None:
            import random
            use_deflection = random.random() < self.deflection_chance
        
        if use_deflection:
            system_prompt = get_synthetic_system_prompt_with_deflection()
        else:
            system_prompt = get_synthetic_system_prompt()
        
        user_prompt = get_synthetic_user_prompt(
            question=question,
            persona=persona,
            previous_concepts=previous_concepts,
            interview_context=interview_context or {},
        )
        
        log_ctx.debug(
            "generating_synthetic_response",
            question_length=len(question),
            previous_concept_count=len(previous_concepts),
            use_deflection=use_deflection,
        )
        
        # Call LLM
        llm_response = await self.llm_client.complete(
            prompt=user_prompt,
            system=system_prompt,
            temperature=0.8,  # Higher temperature for variety
            max_tokens=256,   # Responses shouldn't be too long
        )
        
        # Parse response
        clean_response = parse_synthetic_response(llm_response.content)
        
        log_ctx.info(
            "synthetic_response_generated",
            response_length=len(clean_response),
            latency_ms=llm_response.latency_ms,
            input_tokens=llm_response.usage.get("input_tokens", 0),
            output_tokens=llm_response.usage.get("output_tokens", 0),
        )
        
        return {
            "response": clean_response,
            "persona": persona,
            "persona_name": available[persona],
            "question": question,
            "latency_ms": llm_response.latency_ms,
            "tokens_used": llm_response.usage,
            "used_deflection": use_deflection,
        }
    
    async def generate_multi_response(
        self,
        question: str,
        session_id: str,
        personas: Optional[List[str]] = None,
        graph_state: Optional[GraphState] = None,
        interview_context: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """
        Generate multiple responses (one per persona) for testing.
        
        Useful for comparing how different personas might respond.
        
        Args:
            question: The interviewer's question
            session_id: Session identifier
            personas: List of personas (None = use all available)
            graph_state: Optional graph state
            interview_context: Optional interview context
        
        Returns:
            List of response dicts (same format as generate_response)
        """
        if personas is None:
            personas = list(get_available_personas().keys())
        
        responses = []
        for persona in personas:
            response = await self.generate_response(
                question=question,
                session_id=session_id,
                persona=persona,
                graph_state=graph_state,
                interview_context=interview_context,
                use_deflection=False,  # No deflection for comparison
            )
            responses.append(response)
        
        log.info(
            "multi_synthetic_responses_generated",
            session_id=session_id,
            response_count=len(responses),
        )
        
        return responses
    
    def _extract_previous_concepts(
        self,
        graph_state: Optional[GraphState],
    ) -> List[str]:
        """
        Extract concept labels from graph state for context.
        
        Args:
            graph_state: Optional graph state
        
        Returns:
            List of concept labels (last 10)
        """
        if graph_state is None:
            return []
        
        # Extract from recent nodes if available
        # GraphState should have nodes or concept tracking
        concepts = []
        
        if hasattr(graph_state, "recent_nodes"):
            for node in graph_state.recent_nodes[-10:]:
                if hasattr(node, "label"):
                    concepts.append(node.label)
                elif isinstance(node, dict) and "label" in node:
                    concepts.append(node["label"])
        
        return concepts
    
    async def generate_interview_sequence(
        self,
        session_id: str,
        questions: List[str],
        persona: str = DEFAULT_PERSONA,
        product_name: str = "the product",
    ) -> List[Dict[str, Any]]:
        """
        Generate a sequence of responses for a full interview.
        
        Useful for running automated interviews end-to-end.
        
        Args:
            session_id: Session identifier
            questions: List of interview questions in order
            persona: Persona to use
            product_name: Name of product being discussed
        
        Returns:
            List of response dicts, one per question
        """
        responses = []
        
        for i, question in enumerate(questions):
            interview_context = {
                "product_name": product_name,
                "turn_number": i + 1,
                "coverage_achieved": i * 0.1,  # Rough estimate
            }
            
            response = await self.generate_response(
                question=question,
                session_id=session_id,
                persona=persona,
                interview_context=interview_context,
            )
            
            responses.append(response)
        
        log.info(
            "interview_sequence_generated",
            session_id=session_id,
            response_count=len(responses),
            persona=persona,
        )
        
        return responses


def get_synthetic_service(
    llm_client: Optional[LLMClient] = None,
) -> SyntheticService:
    """
    Factory for synthetic service.
    
    Args:
        llm_client: Optional LLM client (uses default if None)
    
    Returns:
        SyntheticService instance
    """
    return SyntheticService(llm_client=llm_client)
```

### tests/unit/test_synthetic_service.py
```python
"""Tests for synthetic service."""

import pytest
from unittest.mock import AsyncMock, patch, MagicMock

from src.services.synthetic_service import (
    SyntheticService,
    get_synthetic_service,
)
from src.llm.client import LLMResponse
from src.domain.models.knowledge_graph import GraphState


class TestSyntheticService:
    """Tests for SyntheticService."""
    
    def test_init_with_default_params(self):
        """Service initializes with default parameters."""
        with patch("src.services.synthetic_service.get_llm_client") as mock_client:
            mock_client.return_value = MagicMock()
            service = SyntheticService()
            
            assert service.llm_client is not None
            assert service.deflection_chance == 0.2
    
    def test_init_with_custom_params(self):
        """Service accepts custom parameters."""
        mock_llm = MagicMock()
        service = SyntheticService(
            llm_client=mock_llm,
            deflection_chance=0.5,
        )
        
        assert service.llm_client == mock_llm
        assert service.deflection_chance == 0.5
    
    @pytest.mark.asyncio
    async def test_generate_response_success(self):
        """generate_response returns synthetic response."""
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="I really like the creamy texture because it feels satisfying.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        result = await service.generate_response(
            question="Why is creamy texture important to you?",
            session_id="test-session",
            persona="health_conscious",
        )
        
        assert result["response"] == "I really like the creamy texture because it feels satisfying."
        assert result["persona"] == "health_conscious"
        assert "Health-Conscious" in result["persona_name"]
        assert result["question"] == "Why is creamy texture important to you?"
        assert result["latency_ms"] == 150.0
    
    @pytest.mark.asyncio
    async def test_generate_response_with_graph_state(self):
        """generate_response uses graph state for context."""
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="Yes, that's important to me.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 80, "output_tokens": 10},
            latency_ms=100.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        # Mock graph state with recent concepts
        graph_state = MagicMock()
        graph_state.recent_nodes = [
            MagicMock(label="creamy texture"),
            MagicMock(label="plant-based"),
        ]
        
        result = await service.generate_response(
            question="Does sustainability matter?",
            session_id="test-session",
            persona="sustainability_minded",
            graph_state=graph_state,
        )
        
        # Verify prompt included previous concepts
        call_args = mock_llm.complete.call_args
        prompt = call_args.kwargs["prompt"]
        assert "creamy texture" in prompt
        assert result["response"] == "Yes, that's important to me."
    
    @pytest.mark.asyncio
    async def test_generate_response_invalid_persona_raises(self):
        """generate_response raises for invalid persona."""
        mock_llm = AsyncMock()
        service = SyntheticService(llm_client=mock_llm)
        
        with pytest.raises(ValueError, match="Unknown persona"):
            await service.generate_response(
                question="Test question?",
                session_id="test-session",
                persona="invalid_persona",
            )
    
    @pytest.mark.asyncio
    async def test_generate_response_with_interview_context(self):
        """generate_response includes interview context."""
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="I think it's great.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 90, "output_tokens": 15},
            latency_ms=120.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        result = await service.generate_response(
            question="What do you think?",
            session_id="test-session",
            interview_context={
                "product_name": "Oat Milk",
                "turn_number": 5,
                "coverage_achieved": 0.6,
            },
        )
        
        # Verify context was included
        call_args = mock_llm.complete.call_args
        prompt = call_args.kwargs["prompt"]
        assert "Oat Milk" in prompt
        assert "turn 5" in prompt
    
    @pytest.mark.asyncio
    async def test_generate_multi_response(self):
        """generate_multi_response creates responses for multiple personas."""
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="It's important because...",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        responses = await service.generate_multi_response(
            question="Why does quality matter?",
            session_id="test-session",
            personas=["health_conscious", "price_sensitive"],
        )
        
        assert len(responses) == 2
        assert responses[0]["persona"] == "health_conscious"
        assert responses[1]["persona"] == "price_sensitive"
    
    @pytest.mark.asyncio
    async def test_generate_interview_sequence(self):
        """generate_interview_sequence creates full interview."""
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="That's a good question.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        questions = [
            "What comes to mind?",
            "Why is that important?",
            "What else matters?",
        ]
        
        responses = await service.generate_interview_sequence(
            session_id="test-session",
            questions=questions,
            persona="quality_focused",
            product_name="Oat Milk",
        )
        
        assert len(responses) == 3
        assert responses[0]["question"] == questions[0]
        assert responses[1]["question"] == questions[1]
        assert responses[2]["question"] == questions[2]
        assert all(r["persona"] == "quality_focused" for r in responses)
    
    def test_extract_previous_concepts_from_graph_state(self):
        """_extract_previous_concepts extracts labels from nodes."""
        service = SyntheticService()
        
        graph_state = MagicMock()
        graph_state.recent_nodes = [
            MagicMock(label="creamy texture"),
            MagicMock(label="plant-based"),
            MagicMock(label="satisfying"),
        ]
        
        concepts = service._extract_previous_concepts(graph_state)
        
        assert concepts == ["creamy texture", "plant-based", "satisfying"]
    
    def test_extract_previous_concepts_none(self):
        """_extract_previous_concepts returns empty list for None."""
        service = SyntheticService()
        
        concepts = service._extract_previous_concepts(None)
        
        assert concepts == []


class TestGetSyntheticService:
    """Tests for get_synthetic_service factory."""
    
    def test_returns_synthetic_service(self):
        """Factory returns SyntheticService instance."""
        with patch("src.services.synthetic_service.get_llm_client"):
            service = get_synthetic_service()
            
            assert isinstance(service, SyntheticService)
    
    def test_passes_llm_client(self):
        """Factory passes LLM client to service."""
        mock_llm = MagicMock()
        
        with patch("src.services.synthetic_service.get_llm_client"):
            service = get_synthetic_service(llm_client=mock_llm)
            
            assert service.llm_client == mock_llm
```

## Requirements
1. Service must use LLM client for generation
2. Support persona selection with validation
3. Include previous concepts from graph state
4. Support interview context (product_name, turn_number, coverage)
5. Include deflection chance for varied responses
6. Return response with metadata (latency, tokens, persona)
7. Async methods throughout

## Verification
```bash
# Run tests
pytest tests/unit/test_synthetic_service.py -v

# Verify imports
python3 -c "from src.services.synthetic_service import SyntheticService, get_synthetic_service; print('OK')"

# Test service initialization
python3 -c "
from unittest.mock import MagicMock
from src.services.synthetic_service import SyntheticService
mock_llm = MagicMock()
service = SyntheticService(llm_client=mock_llm)
print(f'Service initialized: {service.__class__.__name__}')
print(f'Deflection chance: {service.deflection_chance}')
"
```

## Success Criteria
- [ ] `src/services/synthetic_service.py` created
- [ ] `SyntheticService` class with async methods
- [ ] `generate_response()` method with all parameters
- [ ] `generate_multi_response()` for multiple personas
- [ ] `generate_interview_sequence()` for full interviews
- [ ] `_extract_previous_concepts()` helper
- [ ] `get_synthetic_service()` factory function
- [ ] All tests pass with mocked LLM calls
