# Spec 4.1: Synthetic Prompts

## Objective
Create LLM prompts for generating synthetic respondent answers during automated testing.

## Context
- Reference: PRD Section 6.2 (Flow 2: Automated Testing), Section 4.6 (Synthetic Respondent)
- Phase 4 Goal: Automated testing capability without manual input
- Prompts generate contextually appropriate responses based on question and interview state
- Supports configurable personas for varied response patterns

## Input Files
- `PRD.md` - Section 4.6, Section 6.2
- `ENGINEERING_GUIDE.md` - Section 6 (LLM Integration patterns)
- Phase 2: `src/llm/client.py` - LLM client for making calls
- Phase 2: `src/domain/models/knowledge_graph.py` - Graph state models

## Output Files

### src/llm/prompts/synthetic.py
```python
"""
Prompts for synthetic respondent generation.

Generates plausible respondent answers for automated testing:
- Contextually appropriate based on question content
- Persona-aware (health_conscious, price_sensitive, etc.)
- Graph-aware (references previously extracted concepts)
- Varied response patterns (short, detailed, deflecting)

All prompts produce natural text responses.
"""

from typing import Dict, Any, Optional, List


# Persona definitions for varied synthetic responses
PERSONAS = {
    "health_conscious": {
        "name": "Health-Conscious Millennial",
        "traits": [
            "prioritizes health and wellness",
            "reads nutrition labels carefully",
            "values natural/organic ingredients",
            "willing to pay more for quality",
            "concerned about environmental impact"
        ],
        "speech_pattern": "Uses health-related terminology, mentions wellness goals"
    },
    "price_sensitive": {
        "name": "Budget-Conscious Shopper",
        "traits": [
            "compares prices across brands",
            "looks for value and deals",
            "skeptical of premium pricing",
            "price is a primary decision factor",
            "practical about necessities vs luxuries"
        ],
        "speech_pattern": "Mentions cost, value, budget, affordability"
    },
    "convenience_seeker": {
        "name": "Busy Professional",
        "traits": [
            "values time and convenience",
            "prioritizes ease of use",
            "willing to pay for time-savers",
            "appreciates ready-to-use products",
            "minimal prep time preference"
        ],
        "speech_pattern": "Mentions convenience, time-saving, quick, easy"
    },
    "quality_focused": {
        "name": "Quality Enthusiast",
        "traits": [
            "appreciates craftsmanship and quality",
            "values premium experience",
            "brand-conscious but discerning",
            "seeks authentic experiences",
            "values taste/texture over convenience"
        ],
        "speech_pattern": "Mentions quality, premium, authentic, experience"
    },
    "sustainability minded": {
        "name": "Environmentally Conscious Consumer",
        "traits": [
            "prioritizes environmental impact",
            "seeks sustainable/eco-friendly options",
            "values ethical sourcing",
            "willing to change habits for environment",
            "researches brand sustainability practices"
        ],
        "speech_pattern": "Mentions environment, sustainable, eco-friendly, ethical"
    }
}


def get_synthetic_system_prompt() -> str:
    """
    Get system prompt for synthetic respondent generation.

    Returns:
        System prompt string for LLM
    """
    return """You are a synthetic respondent for testing an adaptive interview system.

Your task is to generate natural, realistic responses to interview questions about products and consumer preferences.

## Response Guidelines:
1. Be conversational and natural (not overly formal or robotic)
2. Show authentic opinions and reasoning
3. Vary response length (some short, some detailed)
4. Include relevant product experiences when appropriate
5. Demonstrate the persona traits in your responses
6. Make connections to previously mentioned concepts when applicable
7. Avoid being uniformly positive - show realistic mix of preferences

## Response Patterns:
- **Detailed responses** (2-4 sentences): For questions about values, deep preferences
- **Medium responses** (1-2 sentences): For follow-ups on specific attributes
- **Brief responses** (few words to 1 sentence): For confirmations, simple facts
- **Deflections**: Sometimes acknowledge but pivot to what matters more to you

## What to Avoid:
- Don't be overly helpful or eager to please
- Don't give textbook perfect laddering responses
- Don't always agree with the interviewer
- Don't be consistently verbose or brief - vary naturally

Generate a response that a real person might give in a qualitative research interview."""


def get_synthetic_user_prompt(
    question: str,
    persona: str = "health_conscious",
    previous_concepts: Optional[List[str]] = None,
    interview_context: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Get user prompt for synthetic response generation.

    Args:
        question: The interviewer's question
        persona: Persona identifier from PERSONAS
        previous_concepts: List of concepts mentioned earlier in interview
        interview_context: Optional context including:
            - product_name: str
            - turn_number: int
            - coverage_achieved: float

    Returns:
        User prompt string
    """
    # Get persona configuration
    persona_config = PERSONAS.get(persona, PERSONAS["health_conscious"])
    
    prompt_parts = [
        f"**Persona:** {persona_config['name']}",
        "",
        "**Your traits:**",
    ]
    
    for trait in persona_config["traits"]:
        prompt_parts.append(f"- {trait}")
    
    prompt_parts.append("")
    prompt_parts.append(f"**Speech pattern:** {persona_config['speech_pattern']}")
    prompt_parts.append("")
    
    # Add context about previous concepts
    if previous_concepts:
        prompt_parts.append("**Concepts already mentioned:**")
        for concept in previous_concepts[-5:]:  # Last 5 concepts
            prompt_parts.append(f"- {concept}")
        prompt_parts.append("")
        prompt_parts.append("Feel free to reference these naturally if relevant.")
        prompt_parts.append("")
    
    # Add interview context
    if interview_context:
        product = interview_context.get("product_name", "this product")
        turn = interview_context.get("turn_number", 1)
        coverage = interview_context.get("coverage_achieved", 0.0)
        
        prompt_parts.append(f"**Context:** Interview about {product} (turn {turn}, {coverage*100:.0f}% coverage)")
        prompt_parts.append("")
    
    prompt_parts.append("**Question:**")
    prompt_parts.append(question)
    prompt_parts.append("")
    prompt_parts.append("**Your response:**")
    
    return "\n".join(prompt_parts)


def get_synthetic_system_prompt_with_deflection() -> str:
    """
    Get system prompt that encourages occasional deflections.

    Returns:
        System prompt string with deflection guidance
    """
    base_prompt = get_synthetic_system_prompt()
    
    additional = """

## Deflection Patterns (use occasionally, ~20% of responses):
- "That's okay, but what really matters to me is..."
- "I guess, but I'm more interested in..."
- "Not really, I care more about..."
- "That's not my main concern..."

Use deflections to demonstrate that you have your own priorities,
not just answering whatever the interviewer asks about."""
    
    return base_prompt + additional


def parse_synthetic_response(response_text: str) -> str:
    """
    Parse LLM synthetic response into clean text.

    Args:
        response_text: Raw LLM response

    Returns:
        Cleaned response text
    """
    # Remove common prefixes
    text = response_text.strip()
    
    # Remove markdown quotes if present
    if text.startswith('"""'):
        text = text[3:]
    if text.startswith('"'):
        text = text[1:]
    if text.endswith('"""'):
        text = text[:-3]
    if text.endswith('"'):
        text = text[:-1]
    
    # Remove "Response:" prefix if present
    if text.lower().startswith("response:"):
        text = text[9:].strip()
    if text.lower().startswith("your response:"):
        text = text[15:].strip()
    
    return text.strip()


def get_available_personas() -> Dict[str, str]:
    """
    Get list of available personas.

    Returns:
        Dict mapping persona_id to persona_name
    """
    return {
        pid: config["name"] 
        for pid, config in PERSONAS.items()
    }
```

### tests/unit/test_synthetic_prompts.py
```python
"""Tests for synthetic respondent prompts."""

import pytest

from src.llm.prompts.synthetic import (
    get_synthetic_system_prompt,
    get_synthetic_user_prompt,
    get_synthetic_system_prompt_with_deflection,
    parse_synthetic_response,
    get_available_personas,
    PERSONAS,
)


class TestSystemPrompts:
    """Tests for system prompt generation."""
    
    def test_base_system_prompt_exists(self):
        """Base system prompt can be generated."""
        prompt = get_synthetic_system_prompt()
        
        assert isinstance(prompt, str)
        assert len(prompt) > 100
    
    def test_system_prompt_includes_guidelines(self):
        """System prompt includes response guidelines."""
        prompt = get_synthetic_system_prompt()
        
        assert "Response Guidelines" in prompt
        assert "conversational" in prompt.lower()
    
    def test_system_prompt_deflection_variant(self):
        """Deflection variant adds deflection guidance."""
        base_prompt = get_synthetic_system_prompt()
        deflection_prompt = get_synthetic_system_prompt_with_deflection()
        
        assert len(deflection_prompt) > len(base_prompt)
        assert "Deflection" in deflection_prompt
        assert "what really matters to me" in deflection_prompt.lower()


class TestUserPrompts:
    """Tests for user prompt generation."""
    
    def test_user_prompt_with_question_only(self):
        """User prompt works with just a question."""
        prompt = get_synthetic_user_prompt(
            question="Why is creamy texture important to you?",
            persona="health_conscious"
        )
        
        assert "creamy texture" in prompt
        assert "health_conscious" in prompt
        assert "Health-Conscious Millennial" in prompt
    
    def test_user_prompt_includes_persona_traits(self):
        """User prompt includes persona traits."""
        prompt = get_synthetic_user_prompt(
            question="What do you think?",
            persona="price_sensitive"
        )
        
        assert "Budget-Conscious Shopper" in prompt
        assert "compares prices" in prompt.lower()
    
    def test_user_prompt_with_previous_concepts(self):
        """User prompt includes previously mentioned concepts."""
        prompt = get_synthetic_user_prompt(
            question="Tell me more about that.",
            persona="health_conscious",
            previous_concepts=["creamy texture", "plant-based", "satisfying"]
        )
        
        assert "creamy texture" in prompt
        assert "plant-based" in prompt
        assert "Concepts already mentioned" in prompt
    
    def test_user_prompt_with_interview_context(self):
        """User prompt includes interview context."""
        prompt = get_synthetic_user_prompt(
            question="Why does that matter?",
            persona="quality_focused",
            interview_context={
                "product_name": "Oat Milk",
                "turn_number": 5,
                "coverage_achieved": 0.6
            }
        )
        
        assert "Oat Milk" in prompt
        assert "turn 5" in prompt
        assert "60% coverage" in prompt
    
    def test_user_prompt_all_parameters(self):
        """User prompt works with all parameters."""
        prompt = get_synthetic_user_prompt(
            question="What else matters?",
            persona="sustainability_minded",
            previous_concepts=["sustainable packaging"],
            interview_context={
                "product_name": "Oat Milk",
                "turn_number": 3,
                "coverage_achieved": 0.4
            }
        )
        
        assert "What else matters?" in prompt
        assert "sustainability" in prompt.lower()
        assert "sustainable packaging" in prompt.lower()
        assert "Oat Milk" in prompt


class TestPersonaConfig:
    """Tests for persona configuration."""
    
    def test_personas_have_required_keys(self):
        """All personas have required configuration keys."""
        for persona_id, config in PERSONAS.items():
            assert "name" in config
            assert "traits" in config
            assert isinstance(config["traits"], list)
            assert len(config["traits"]) > 0
            assert "speech_pattern" in config
    
    def test_get_available_personas(self):
        """Can get list of available personas."""
        personas = get_available_personas()
        
        assert isinstance(personas, dict)
        assert "health_conscious" in personas
        assert personas["health_conscious"] == "Health-Conscious Millennial"


class TestParseSyntheticResponse:
    """Tests for response parsing."""
    
    def test_parse_plain_response(self):
        """Parses plain response unchanged."""
        response = "I really like the creamy texture because it feels satisfying."
        parsed = parse_synthetic_response(response)
        
        assert parsed == response
    
    def test_parse_removes_markdown_quotes(self):
        """Removes markdown quote wrapping."""
        response = '"""I like the texture."""'
        parsed = parse_synthetic_response(response)
        
        assert parsed == "I like the texture."
        assert not parsed.startswith('"')
    
    def test_parse_removes_response_prefix(self):
        """Removes 'Response:' prefix."""
        response = "Response: I think it's great."
        parsed = parse_synthetic_response(response)
        
        assert parsed == "I think it's great."
        assert not parsed.lower().startswith("response:")
    
    def test_parse_removes_your_response_prefix(self):
        """Removes 'Your response:' prefix."""
        response = "Your response: It matters because..."
        parsed = parse_synthetic_response(response)
        
        assert parsed == "It matters because..."
    
    def test_parse_whitespace_cleanup(self):
        """Cleans up extra whitespace."""
        response = '  "  Response with spaces  "  '
        parsed = parse_synthetic_response(response)
        
        assert parsed == "Response with spaces"
```

## Requirements
1. System prompts must specify natural, realistic response patterns
2. Support at least 5 distinct personas (health_conscious, price_sensitive, convenience_seeker, quality_focused, sustainability_minded)
3. User prompts must accept question, persona, previous_concepts, and interview_context
4. Include deflection patterns for varied responses
5. Parser must clean common LLM artifacts (quotes, prefixes)
6. All personas must have name, traits list, and speech_pattern

## Verification
```bash
# Run tests
pytest tests/unit/test_synthetic_prompts.py -v

# Verify imports
python3 -c "from src.llm.prompts.synthetic import get_synthetic_user_prompt, PERSONAS; print('OK')"

# Test prompt generation
python3 -c "
from src.llm.prompts.synthetic import get_synthetic_user_prompt
prompt = get_synthetic_user_prompt(
    question='Why is creamy texture important?',
    persona='health_conscious'
)
print('Prompt length:', len(prompt))
assert 'creamy texture' in prompt
assert 'Health-Conscious Millennial' in prompt
print('Prompt generation works')
"
```

## Success Criteria
- [ ] `src/llm/prompts/synthetic.py` created with all prompt functions
- [ ] `PERSONAS` dict with at least 5 persona configurations
- [ ] `get_synthetic_system_prompt()` returns base system prompt
- [ ] `get_synthetic_user_prompt()` accepts all parameters
- [ ] `get_synthetic_system_prompt_with_deflection()` includes deflection guidance
- [ ] `parse_synthetic_response()` cleans response text
- [ ] `get_available_personas()` returns persona dict
- [ ] All tests pass
