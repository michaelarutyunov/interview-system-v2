# Spec 3.7: Arbitration Engine

## Objective
Create the arbitration engine for multi-dimensional strategy scoring.

## Context
- Reference: PRD Section 5.3 (Arbitration), IMPLEMENTATION_PLAN.md Section 3.7
- v1 Reference: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/arbitration_engine.py`
- Applies all 5 scorers multiplicatively: final_score = base × ∏(scorer^weight)
- Handles veto conditions and provides transparent scoring provenance

## Input Files
- `src/services/scoring/base.py` - ScorerBase and ScorerOutput
- All 5 scorers from specs 3.2-3.6
- v1: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/arbitration_engine.py`

## Output Files

### src/services/scoring/arbitration.py
```python
"""Arbitration engine for multi-dimensional strategy scoring.

Combines scores from 5 scorers using multiplicative formula to produce
emergent adaptive behavior.
"""

from typing import Any, Dict, List, Tuple

import structlog

from src.domain.models.knowledge_graph import GraphState
from src.services.scoring.base import ScorerBase, ScorerOutput


logger = structlog.get_logger(__name__)


class ArbitrationEngine:
    """
    Applies multi-dimensional scoring to strategy/focus pairs.

    Combines scores from 5 independent scorers using multiplicative formula:
    final_score = strategy.priority_base × ∏(scorer_output^weight)

    Features:
    - Early exit on per-scorer veto thresholds
    - Transparent scoring provenance (all scores logged)
    - Error handling for failed scorers
    """

    def __init__(self, scorers: List[ScorerBase], config: Dict[str, Any] = None):
        """
        Initialize arbitration engine.

        Args:
            scorers: All 5 scorers (only enabled ones will be used)
            config: Arbitration configuration
        """
        self.scorers = [s for s in scorers if s.enabled]
        self.config = config or {}

        # Configuration
        self._veto_threshold = self.config.get("veto_threshold", 0.1)
        self._score_precision = self.config.get("score_precision", 4)

        logger.info(
            "ArbitrationEngine initialized",
            num_scorers=len(self.scorers),
            enabled_scorers=[s.__class__.__name__ for s in self.scorers],
            veto_threshold=self._veto_threshold,
        )

    async def score(
        self,
        strategy: Dict[str, Any],
        focus: Dict[str, Any],
        graph_state: GraphState,
        recent_nodes: List[Dict[str, Any]],
    ) -> Tuple[float, List[ScorerOutput], List[str]]:
        """
        Score a strategy/focus combination using all scorers.

        Applies multiplicative scoring formula:
        final_score = strategy.priority_base
        for each scorer:
            weighted_score = raw_score ** scorer.weight
            final_score *= weighted_score
            if final_score < scorer.veto_threshold:
                break  # Per-scorer veto

        Args:
            strategy: Strategy dict with 'priority_base'
            focus: Focus dict
            graph_state: Current graph state
            recent_nodes: List of recent nodes

        Returns:
            Tuple of:
            - final_score: Cumulative multiplicative score
            - scorer_outputs: List of individual scorer results
            - reasoning_steps: Step-by-step score evolution for debugging

        Example reasoning_steps:
        [
            "Base: 1.000",
            "NoveltyScorer: 0.800^1.0=0.800 → cumulative=0.800",
            "CoverageScorer: 1.500^1.2=1.709 → cumulative=1.367",
            ...
        ]
        """
        base_score = strategy.get("priority_base", 1.0)
        score = base_score
        outputs = []
        reasoning = [f"Base: {base_score:.{self._score_precision}f}"]

        logger.debug(
            "Starting arbitration",
            strategy_id=strategy.get("id"),
            focus_type=focus.get("focus_type"),
            priority_base=base_score,
        )

        for scorer in self.scorers:
            try:
                # Apply scorer
                output = await scorer.score(strategy, focus, graph_state, recent_nodes)
                outputs.append(output)

                # Score is already weighted by scorer.make_output()
                score *= output.weighted_score

                # Format reasoning step
                step = (
                    f"{output.scorer_name}: "
                    f"{output.raw_score:.{self._score_precision}f}^{output.weight:.1f}="
                    f"{output.weighted_score:.{self._score_precision}f} → "
                    f"cumulative={score:.{self._score_precision}f}"
                )
                reasoning.append(step)

                # Check per-scorer veto threshold
                if score < scorer.veto_threshold:
                    veto_msg = (
                        f"VETOED by {output.scorer_name} "
                        f"(score={score:.{self._score_precision}f} < "
                        f"threshold={scorer.veto_threshold})"
                    )
                    reasoning.append(veto_msg)
                    logger.warning(
                        "Scorer veto triggered",
                        scorer=output.scorer_name,
                        final_score=score,
                        veto_threshold=scorer.veto_threshold,
                        strategy_id=strategy.get("id"),
                    )
                    break

            except Exception as e:
                # Log error but continue with other scorers
                logger.warning(
                    "Scorer failed",
                    scorer=scorer.__class__.__name__,
                    error=str(e),
                    strategy_id=strategy.get("id"),
                )
                reasoning.append(f"{scorer.__class__.__name__}: ERROR - {str(e)}")
                # Continue without this scorer's contribution

        logger.debug(
            "Arbitration complete",
            strategy_id=strategy.get("id"),
            final_score=score,
            num_scorers_applied=len(outputs),
        )

        return score, outputs, reasoning

    def __repr__(self) -> str:
        return (
            f"ArbitrationEngine("
            f"num_scorers={len(self.scorers)}, "
            f"veto_threshold={self._veto_threshold})"
        )
```

## Requirements
1. Accepts list of ScorerBase instances
2. Applies all enabled scorers in sequence
3. Uses multiplicative scoring: final = base × ∏(weighted_scores)
4. Early exits on per-scorer veto threshold
5. Returns final_score, all scorer outputs, and reasoning steps
6. Handles scorer exceptions gracefully

## Verification
```bash
# Run tests
pytest tests/unit/test_arbitration.py -v

# Verify import
python3 -c "from src.services.scoring.arbitration import ArbitrationEngine; print('OK')"
```

## Success Criteria
- [ ] `ArbitrationEngine` class created
- [ ] `score()` method accepts strategy, focus, graph_state, recent_nodes
- [ ] Applies all scorers and multiplies weighted outputs
- [ ] Early exits on veto threshold
- [ ] Returns (final_score, outputs, reasoning)
- [ ] Handles scorer exceptions without crashing
- [ ] All tests pass
