# Spec 3.1: Scorer Base

## Objective
Create the abstract base class for all strategy scorers.

## Context
- Reference: PRD Section 5.3 (Strategy Selection), IMPLEMENTATION_PLAN.md Section 3.7
- v1 Reference: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/scorers/base.py`
- Simplifies from v1: Removes History dependency (use direct graph queries)
- Core design: Scorer output = multiplier (1.0=neutral, >1.0=boost, <1.0=penalty)

## Input Files
- v1: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/scorers/base.py`
- Phase 2: `src/domain/models/knowledge_graph.py` - GraphState model

## Output Files

### src/services/scoring/__init__.py
```python
"""Strategy scoring package."""

from .base import ScorerBase, ScorerOutput

__all__ = ["ScorerBase", "ScorerOutput"]
```

### src/services/scoring/base.py
```python
"""Abstract base class for strategy scorers.

Scorers evaluate strategy options along orthogonal dimensions and return
multipliers that combine to produce adaptive behavior.

Scoring logic:
- 1.0 = neutral (no effect)
- >1.0 = boost (strategy more attractive)
- <1.0 = penalty (strategy less attractive)
- ~0.0 = veto (effectively blocks strategy)
"""

from abc import ABC, abstractmethod
from typing import Any, Dict, Optional

from pydantic import BaseModel, Field

import structlog

from src.domain.models.knowledge_graph import GraphState


logger = structlog.get_logger(__name__)


class ScorerOutput(BaseModel):
    """Output from a single scorer.

    Contains the score and provenance for debugging.
    """
    scorer_name: str = Field(description="Name of the scorer")
    raw_score: float = Field(default=1.0, ge=0.0, le=2.0, description="Raw score 0-2")
    weight: float = Field(default=1.0, ge=0.1, le=5.0, description="Scorer weight")
    weighted_score: float = Field(description="Score after weighting (raw^weight)")
    signals: Dict[str, Any] = Field(default_factory=dict, description="State signals used")
    reasoning: str = Field(default="", description="Human-readable explanation")

    class Config:
        from_attributes = True


class ScorerBase(ABC):
    """
    Abstract base class for all strategy scorers.

    Design constraints:
    - Pure functions of state (no side effects)
    - Single orthogonal dimension per scorer
    - All thresholds from config (no hardcoding)
    - Return multipliers, not absolute scores
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        Initialize scorer with configuration.

        Args:
            config: Scorer configuration with optional:
                - enabled: bool (default: True)
                - weight: float (default: 1.0)
                - veto_threshold: float (default: 0.1)
                - params: dict of scorer-specific params
        """
        self.config = config or {}
        self.enabled = self.config.get("enabled", True)
        self.weight = self.config.get("weight", 1.0)
        self.veto_threshold = self.config.get("veto_threshold", 0.1)
        self.params = self.config.get("params", {})

    @abstractmethod
    async def score(
        self,
        strategy: Dict[str, Any],
        focus: Dict[str, Any],
        graph_state: GraphState,
        recent_nodes: list,
    ) -> ScorerOutput:
        """
        Score a strategy/focus combination.

        Args:
            strategy: Strategy dict with keys:
                - id: str
                - type_category: str ("depth", "breadth", "coverage")
            focus: Focus dict with keys:
                - node_id: Optional[str]
                - focus_type: str
                - properties: dict
            graph_state: Current graph state (node_count, edge_count, etc.)
            recent_nodes: List of recent node dicts from last N turns

        Returns:
            ScorerOutput with score 0.0-2.0 and reasoning
        """
        pass

    def make_output(
        self,
        raw_score: float,
        signals: Dict[str, Any],
        reasoning: str,
    ) -> ScorerOutput:
        """
        Helper to construct ScorerOutput with proper validation.

        Args:
            raw_score: Raw score (will be clamped to 0.0-2.0)
            signals: State signals used in scoring
            reasoning: Human-readable explanation

        Returns:
            ScorerOutput with all fields populated
        """
        clamped = max(0.0, min(2.0, raw_score))
        weighted = clamped**self.weight

        return ScorerOutput(
            scorer_name=self.__class__.__name__,
            raw_score=clamped,
            weight=self.weight,
            weighted_score=weighted,
            signals=signals,
            reasoning=reasoning,
        )

    def __repr__(self) -> str:
        return (
            f"{self.__class__.__name__}("
            f"enabled={self.enabled}, "
            f"weight={self.weight}, "
            f"veto_threshold={self.veto_threshold})"
        )
```

## Requirements
1. Use Pydantic v2 for ScorerOutput
2. Score range must be 0.0-2.0 (clamped in make_output)
3. Weight exponentiation: weighted_score = raw_score**weight
4. All scorers must be async (for consistency)
5. Configuration-driven thresholds (no hardcoded values)

## Verification
```bash
# Run tests
pytest tests/unit/test_scorer_base.py -v

# Verify import
python3 -c "from src.services.scoring import ScorerBase, ScorerOutput; print('OK')"

# Verify output validation
python3 -c "
from src.services.scoring import ScorerOutput
# Should clamp to 2.0
out = ScorerOutput(scorer_name='test', raw_score=5.0, weight=1.0, weighted_score=5.0)
print(f'Clamped: {out.raw_score}')  # Should show 2.0
"
```

## Success Criteria
- [ ] `src/services/scoring/` directory created with `__init__.py`
- [ ] `ScorerOutput` Pydantic model with validation
- [ ] `ScorerBase` abstract base class with `score()` method
- [ ] `make_output()` helper with score clamping
- [ ] `enabled`, `weight`, `veto_threshold`, `params` config handling
- [ ] All tests pass
