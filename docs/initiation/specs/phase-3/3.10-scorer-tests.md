# Spec 3.10: Scorer Unit Tests

## Objective
Create comprehensive unit tests for all scoring components.

## Context
- Reference: ENGINEERING_GUIDE.md (testing patterns)
- Tests all 5 scorers, arbitration engine, and strategy service
- Uses pytest with fixtures for common test data

## Input Files
- All scoring modules from specs 3.1-3.8
- `src/domain/models/knowledge_graph.py` - GraphState

## Output Files

### tests/unit/test_scorer_base.py
```python
"""Tests for ScorerBase and ScorerOutput."""

import pytest

from src.services.scoring.base import ScorerBase, ScorerOutput
from src.domain.models.knowledge_graph import GraphState


class DummyScorer(ScorerBase):
    """Dummy scorer for testing base class."""

    async def score(self, strategy, focus, graph_state, recent_nodes):
        return self.make_output(
            raw_score=1.0,
            signals={"test": True},
            reasoning="Test scorer",
        )


class TestScorerOutput:
    """Tests for ScorerOutput model."""

    def test_create_valid_output(self):
        """ScorerOutput can be created with valid data."""
        output = ScorerOutput(
            scorer_name="TestScorer",
            raw_score=1.5,
            weight=1.0,
            weighted_score=1.5,
            signals={"key": "value"},
            reasoning="Test reasoning",
        )
        assert output.scorer_name == "TestScorer"
        assert output.raw_score == 1.5

    def test_raw_score_validation(self):
        """Raw score must be between 0 and 2."""
        with pytest.raises(ValueError):
            ScorerOutput(
                scorer_name="Test",
                raw_score=2.5,  # Too high
                weight=1.0,
                weighted_score=2.5,
            )

    def test_weight_validation(self):
        """Weight must be positive."""
        with pytest.raises(ValueError):
            ScorerOutput(
                scorer_name="Test",
                raw_score=1.0,
                weight=0.0,  # Too low
                weighted_score=1.0,
            )


class TestScorerBase:
    """Tests for ScorerBase."""

    def test_init_with_defaults(self):
        """ScorerBase initializes with default config."""
        scorer = DummyScorer()
        assert scorer.enabled is True
        assert scorer.weight == 1.0
        assert scorer.veto_threshold == 0.1
        assert scorer.params == {}

    def test_init_with_config(self):
        """ScorerBase uses provided config."""
        config = {
            "enabled": False,
            "weight": 2.0,
            "veto_threshold": 0.2,
            "params": {"threshold": 0.5},
        }
        scorer = DummyScorer(config)
        assert scorer.enabled is False
        assert scorer.weight == 2.0
        assert scorer.veto_threshold == 0.2
        assert scorer.params == {"threshold": 0.5}

    def test_make_output_clamps_high_score(self):
        """make_output clamps scores above 2.0."""
        scorer = DummyScorer()
        output = scorer.make_output(
            raw_score=5.0,
            signals={},
            reasoning="Too high",
        )
        assert output.raw_score == 2.0
        assert output.weighted_score == 5.0  # 2.0^1.0

    def test_make_output_clamps_low_score(self):
        """make_output clamps scores below 0.0."""
        scorer = DummyScorer()
        output = scorer.make_output(
            raw_score=-1.0,
            signals={},
            reasoning="Too low",
        )
        assert output.raw_score == 0.0

    def test_make_output_applies_weight(self):
        """make_output applies weight exponent."""
        scorer = DummyScorer(config={"weight": 2.0})
        output = scorer.make_output(
            raw_score=0.5,
            signals={},
            reasoning="Weighted",
        )
        assert output.raw_score == 0.5
        assert output.weighted_score == 0.25  # 0.5^2.0

    @pytest.mark.asyncio
    async def test_score_returns_output(self):
        """DummyScorer.score returns ScorerOutput."""
        scorer = DummyScorer()
        graph_state = GraphState()
        output = await scorer.score({}, {}, graph_state, [])
        assert isinstance(output, ScorerOutput)
        assert output.raw_score == 1.0
```

### tests/unit/test_coverage_scorer.py
```python
"""Tests for CoverageScorer."""

import pytest

from src.services.scoring.coverage import CoverageScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.mark.asyncio
async def test_coverage_complete_no_boost():
    """Complete coverage results in neutral score."""
    scorer = CoverageScorer()
    graph_state = GraphState(
        properties={
            "elements_total": 5,
            "elements_seen": {"a", "b", "c", "d", "e"},
        }
    )

    output = await scorer.score(
        strategy={"id": "cover", "type_category": "coverage"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.0
    assert "complete" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_coverage_gap_boosts_coverage_strategy():
    """Coverage gaps boost coverage strategies."""
    scorer = CoverageScorer()
    graph_state = GraphState(
        properties={
            "elements_total": 5,
            "elements_seen": {"a", "b", "c"},  # 2 gaps
        }
    )

    output = await scorer.score(
        strategy={"id": "cover", "type_category": "coverage"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 2.0  # gap_boost
    assert "gap" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_coverage_gap_no_effect_on_depth():
    """Coverage gaps don't affect depth strategies."""
    scorer = CoverageScorer()
    graph_state = GraphState(
        properties={
            "elements_total": 5,
            "elements_seen": {"a", "b", "c"},
        }
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.0
```

### tests/unit/test_depth_scorer.py
```python
"""Tests for DepthScorer."""

import pytest

from src.services.scoring.depth import DepthScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.mark.asyncio
async def test_early_phase_boosts_breadth():
    """Early phase boosts breadth strategies."""
    scorer = DepthScorer()
    graph_state = GraphState(
        properties={"turn_count": 3},
        max_depth=1,
    )

    output = await scorer.score(
        strategy={"id": "broaden", "type_category": "breadth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.5  # early_breadth_boost
    assert "early" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_late_phase_boosts_depth():
    """Late phase boosts depth strategies."""
    scorer = DepthScorer()
    graph_state = GraphState(
        properties={"turn_count": 16},
        max_depth=2,
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.4  # late_depth_boost
    assert "late" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_middle_phase_neutral():
    """Middle phase is neutral."""
    scorer = DepthScorer()
    graph_state = GraphState(
        properties={"turn_count": 10},
        max_depth=2,
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.0
    assert "middle" in output.reasoning.lower()
```

### tests/unit/test_saturation_scorer.py
```python
"""Tests for SaturationScorer."""

import pytest

from src.services.scoring.saturation import SaturationScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.mark.asyncio
async def test_not_saturated_neutral():
    """Not saturated results in neutral score."""
    scorer = SaturationScorer()
    graph_state = GraphState(
        properties={
            "new_info_rate": 0.2,  # Above threshold
            "consecutive_low_info_turns": 0,
        }
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.0
    assert "not saturated" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_saturated_penalizes_depth():
    """Saturated topic penalizes depth strategies."""
    scorer = SaturationScorer()
    graph_state = GraphState(
        properties={
            "new_info_rate": 0.02,  # Below threshold
            "consecutive_low_info_turns": 3,
        }
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 0.3  # saturated_penalty
    assert "saturated" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_saturated_boosts_breadth():
    """Saturated topic boosts breadth strategies."""
    scorer = SaturationScorer()
    graph_state = GraphState(
        properties={
            "new_info_rate": 0.02,
            "consecutive_low_info_turns": 2,
        }
    )

    output = await scorer.score(
        strategy={"id": "broaden", "type_category": "breadth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score > 1.0  # boost (1/0.3 ≈ 3.3)
    assert "breadth encouraged" in output.reasoning.lower()
```

### tests/unit/test_novelty_scorer.py
```python
"""Tests for NoveltyScorer."""

import pytest

from src.services.scoring.novelty import NoveltyScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.mark.asyncio
async def test_novel_focus_neutral():
    """Novel focus (not seen recently) results in neutral score."""
    scorer = NoveltyScorer()
    recent_nodes = [
        {"id": "node-1", "label": "concept A"},
        {"id": "node-2", "label": "concept B"},
    ]

    output = await scorer.score(
        strategy={"id": "deepen"},
        focus={"node_id": "node-3"},  # Different node
        graph_state=GraphState(),
        recent_nodes=recent_nodes,
    )

    assert output.raw_score == 1.0
    assert "novel" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_recent_focus_penalized():
    """Recently explored focus is penalized."""
    scorer = NoveltyScorer()
    recent_nodes = [
        {"id": "node-1", "label": "concept A"},
        {"id": "node-2", "label": "concept B"},
    ]

    output = await scorer.score(
        strategy={"id": "deepen"},
        focus={"node_id": "node-1"},  # Same as recent
        graph_state=GraphState(),
        recent_nodes=recent_nodes,
    )

    assert output.raw_score == 0.3  # recency_penalty
    assert "last" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_none_node_id_neutral():
    """None node_id results in neutral score."""
    scorer = NoveltyScorer()

    output = await scorer.score(
        strategy={"id": "broaden"},
        focus={"node_id": None},
        graph_state=GraphState(),
        recent_nodes=[],
    )

    assert output.raw_score == 1.0
```

### tests/unit/test_richness_scorer.py
```python
"""Tests for RichnessScorer."""

import pytest

from src.services.scoring.richness import RichnessScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.mark.asyncio
async def test_low_engagement_penalizes_depth():
    """Low engagement penalizes depth strategies."""
    scorer = RichnessScorer()
    graph_state = GraphState(
        properties={"avg_response_length": 30}  # Below low_threshold
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 0.6  # low_penalty
    assert "low" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_low_engagement_boosts_breadth():
    """Low engagement boosts breadth strategies."""
    scorer = RichnessScorer()
    graph_state = GraphState(
        properties={"avg_response_length": 30}
    )

    output = await scorer.score(
        strategy={"id": "broaden", "type_category": "breadth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score > 1.0  # boost (1/0.6 ≈ 1.67)


@pytest.mark.asyncio
async def test_high_engagement_boosts_depth():
    """High engagement boosts depth strategies."""
    scorer = RichnessScorer()
    graph_state = GraphState(
        properties={"avg_response_length": 250}  # Above high_threshold
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.4  # high_boost
    assert "high" in output.reasoning.lower()


@pytest.mark.asyncio
async def test_medium_engagement_neutral():
    """Medium engagement is neutral."""
    scorer = RichnessScorer()
    graph_state = GraphState(
        properties={"avg_response_length": 100}
    )

    output = await scorer.score(
        strategy={"id": "deepen", "type_category": "depth"},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert output.raw_score == 1.0
    assert "medium" in output.reasoning.lower()
```

### tests/unit/test_arbitration.py
```python
"""Tests for ArbitrationEngine."""

import pytest

from src.services.scoring.arbitration import ArbitrationEngine
from src.services.scoring.coverage import CoverageScorer
from src.services.scoring.depth import DepthScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.mark.asyncio
async def test_multiplies_scores():
    """ArbitrationEngine multiplies all scorer outputs."""
    scorers = [
        CoverageScorer(config={"weight": 1.0}),
        DepthScorer(config={"weight": 1.0}),
    ]
    engine = ArbitrationEngine(scorers)

    # Set up state where both scorers boost
    graph_state = GraphState(
        properties={
            "turn_count": 3,  # Early phase -> depth boosts breadth
            "elements_total": 5,
            "elements_seen": {"a", "b"},  # Gaps -> coverage boosted
        }
    )

    score, outputs, reasoning = await engine.score(
        strategy={
            "id": "broaden",
            "type_category": "breadth",
            "priority_base": 1.0,
        },
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    # Both scorers should boost breadth: 2.0 (coverage) × 1.5 (depth early phase) = 3.0
    assert score > 1.0
    assert len(outputs) == 2
    assert len(reasoning) == 3  # Base + 2 scorers


@pytest.mark.asyncio
async def test_veto_threshold():
    """ArbitrationEngine early exits on veto threshold."""
    scorer = CoverageScorer(config={
        "params": {"gap_boost": 0.05},  # Very low boost
        "veto_threshold": 0.1,
    })
    engine = ArbitrationEngine([scorer])

    graph_state = GraphState(
        properties={
            "elements_total": 5,
            "elements_seen": {"a"},  # Gaps but low boost
        }
    )

    score, outputs, reasoning = await engine.score(
        strategy={
            "id": "cover",
            "type_category": "coverage",
            "priority_base": 0.5,  # Base below veto
        },
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    # Score should be very low
    assert score < 0.1
    assert any("VETOED" in r or "ERROR" in r for r in reasoning)


@pytest.mark.asyncio
async def test_disabled_scorer_not_used():
    """Disabled scorers are not applied."""
    scorer = CoverageScorer(config={"enabled": False})
    engine = ArbitrationEngine([scorer])

    graph_state = GraphState()

    score, outputs, reasoning = await engine.score(
        strategy={"id": "test", "priority_base": 1.0},
        focus={},
        graph_state=graph_state,
        recent_nodes=[],
    )

    assert len(outputs) == 0
    assert score == 1.0  # Just base
```

### tests/unit/test_strategy_service.py
```python
"""Tests for StrategyService."""

import pytest

from src.services.strategy_service import StrategyService, SelectionResult
from src.services.scoring.arbitration import ArbitrationEngine
from src.services.scoring.coverage import CoverageScorer
from src.domain.models.knowledge_graph import GraphState


@pytest.fixture
def strategy_service():
    """Create a StrategyService with test config."""
    engine = ArbitrationEngine([
        CoverageScorer(),
    ])
    return StrategyService(engine, config={"veto_threshold": 0.1})


@pytest.mark.asyncio
async def test_selects_strategy(strategy_service):
    """StrategyService selects a strategy."""
    graph_state = GraphState(
        properties={
            "turn_count": 5,
            "elements_total": ["a", "b", "c"],
            "elements_seen": {"a"},
            "recent_nodes": [],
        }
    )

    result = await strategy_service.select(graph_state, [])

    assert isinstance(result, SelectionResult)
    assert "selected_strategy" in result.__dict__
    assert "selected_focus" in result.__dict__
    assert result.final_score > 0


@pytest.mark.asyncio
async def test_returns_alternatives(strategy_service):
    """StrategyService returns alternative strategies."""
    graph_state = GraphState(
        properties={
            "turn_count": 5,
            "elements_total": ["a", "b"],
            "elements_seen": {"a"},
            "recent_nodes": [{"id": "node-1", "label": "A"}],
        }
    )

    result = await strategy_service.select(graph_state, [{"id": "node-1", "label": "A"}])

    # Should have some alternatives
    assert hasattr(result, "alternative_strategies")


@pytest.mark.asyncio
async def test_fallback_when_no_candidates(monkeypatch, strategy_service):
    """StrategyService falls back when no candidates available."""
    # Patch _get_possible_focuses to return empty
    def mock_focuses(strategy, graph_state):
        return []

    import src.services.strategy_service
    monkeypatch.setattr(
        src.services.strategy_service.StrategyService,
        "_get_possible_focuses",
        mock_focuses,
    )

    graph_state = GraphState(properties={"turn_count": 1})

    result = await strategy_service.select(graph_state, [])

    # Should return fallback (broaden strategy)
    assert result.selected_strategy["id"] == "broaden"
```

## Requirements
1. Test each scorer independently with pytest.mark.asyncio
2. Test ArbitrationEngine with multiple scorers
3. Test StrategyService end-to-end selection
4. Use fixtures for common test data
5. Cover edge cases: veto, disabled scorers, empty states

## Verification
```bash
# Run all scorer tests
pytest tests/unit/test_scorer_base.py -v
pytest tests/unit/test_coverage_scorer.py -v
pytest tests/unit/test_depth_scorer.py -v
pytest tests/unit/test_saturation_scorer.py -v
pytest tests/unit/test_novelty_scorer.py -v
pytest tests/unit/test_richness_scorer.py -v
pytest tests/unit/test_arbitration.py -v
pytest tests/unit/test_strategy_service.py -v

# Run all at once
pytest tests/unit/ -k "scorer or strategy or arbitration" -v
```

## Success Criteria
- [ ] All 7 test files created
- [ ] Each scorer has tests for boost, penalty, and neutral cases
- [ ] ArbitrationEngine tests multiplication and veto
- [ ] StrategyService tests selection and fallback
- [ ] All tests pass
- [ ] Test coverage >80% for scoring module
