# Spec 2.2: LLM Client (3-Client Architecture)

## Objective
Create the LLM client abstraction supporting multiple providers and task-specific clients.

## Context
- Reference: ENGINEERING_GUIDE.md Section 6 (LLM Integration)
- **Three-client architecture**: extraction, scoring, generation
- **Multiple providers**: Anthropic, Kimi (Moonshot AI), DeepSeek
- Uses httpx for async HTTP calls
- Structured logging of all LLM calls
- Hardcoded defaults in code, minimal .env configuration

## Architecture

### Client Types

| Client Type | Purpose | Default Provider | Default Model |
|-------------|---------|------------------|---------------|
| `extraction` | Extract nodes/edges/stance from user responses | Anthropic | claude-sonnet-4-5-20250929 |
| `scoring` | Extract diagnostic signals for strategy scoring | Kimi | moonshot-v1-8k |
| `generation` | Generate interview questions | Anthropic | claude-sonnet-4-5-20250929 |

### Supported Providers

| Provider | API Base URL | Models |
|----------|--------------|--------|
| `anthropic` | https://api.anthropic.com/v1 | claude-sonnet-4-5-20250929, claude-haiku-4-20250514 |
| `kimi` | https://api.moonshot.ai/v1 | moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k |
| `deepseek` | https://api.deepseek.com | deepseek-chat, deepseek-coder |

### Configuration

**`.env`** - Only API keys required (everything else hardcoded in `client.py`):
```bash
ANTHROPIC_API_KEY=sk-xxx
KIMI_API_KEY=sk-xxx
DEEPSEEK_API_KEY=sk-xxx

# Optional: Override default provider for each client type
# LLM_EXTRACTION_PROVIDER=kimi
# LLM_SCORING_PROVIDER=deepseek
# LLM_GENERATION_PROVIDER=anthropic
```

**`src/llm/client.py`** - Hardcoded defaults:
```python
EXTRACTION_DEFAULTS = dict(
    provider="anthropic",
    model="claude-sonnet-4-5-20250929",
    temperature=0.3,  # Lower for structured extraction
    max_tokens=2048,
    timeout=30.0,
)

SCORING_DEFAULTS = dict(
    provider="kimi",
    model="moonshot-v1-8k",
    temperature=0.3,
    max_tokens=512,
    timeout=15.0,
)

GENERATION_DEFAULTS = dict(
    provider="anthropic",
    model="claude-sonnet-4-5-20250929",
    temperature=0.7,  # Higher for creative questions
    max_tokens=1024,
    timeout=30.0,
)
```

## Output Files

### src/llm/client.py
```python
"""
LLM client abstraction for multiple LLM providers.

Provides async interface for LLM calls with:
- Structured logging of requests/responses
- Timeout handling
- Usage tracking (tokens)
- Three-client architecture (extraction, scoring, generation)

Supported providers:
- anthropic: Claude models (Sonnet, Haiku)
- kimi: Moonshot AI models
- deepseek: DeepSeek models
"""

from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Optional, Dict, Any, Literal
import time
import httpx
import structlog

from src.core.config import settings

log = structlog.get_logger(__name__)

LLMClientType = Literal["extraction", "scoring", "generation"]


@dataclass
class LLMResponse:
    """Standardized LLM response."""
    content: str
    model: str
    usage: Dict[str, int] = field(default_factory=dict)
    latency_ms: float = 0.0
    raw_response: Optional[Dict[str, Any]] = None


class LLMClient(ABC):
    """Abstract base for LLM providers."""

    @abstractmethod
    async def complete(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
    ) -> LLMResponse:
        """Generate a completion from the LLM."""
        pass


class AnthropicClient(LLMClient):
    """Anthropic Claude API client."""

    def __init__(
        self,
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: float,
        client_type: LLMClientType,
        api_key: Optional[str] = None,
    ):
        self.api_key = api_key or settings.anthropic_api_key
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.client_type = client_type
        self.base_url = "https://api.anthropic.com/v1"

        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not configured. Set it in .env.")


class OpenAICompatibleClient(LLMClient):
    """Base class for OpenAI-compatible API clients (Kimi, DeepSeek)."""

    def __init__(
        self,
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: float,
        client_type: LLMClientType,
        base_url: str,
        provider_name: str,
        api_key: str,
    ):
        self.model = model
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.client_type = client_type
        self.base_url = base_url
        self.provider_name = provider_name
        self.api_key = api_key


class KimiClient(OpenAICompatibleClient):
    """Kimi (Moonshot AI) API client.

    Base URL: https://api.moonshot.ai/v1
    Models: moonshot-v1-8k, moonshot-v1-32k, moonshot-v1-128k
    """

    def __init__(
        self,
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: float,
        client_type: LLMClientType,
        api_key: Optional[str] = None,
    ):
        api_key = api_key or settings.kimi_api_key
        base_url = "https://api.moonshot.ai/v1"

        if not api_key:
            raise ValueError("KIMI_API_KEY not configured. Set it in .env.")

        super().__init__(model, temperature, max_tokens, timeout,
                       client_type, base_url, "kimi", api_key)


class DeepSeekClient(OpenAICompatibleClient):
    """DeepSeek API client.

    Base URL: https://api.deepseek.com
    Models: deepseek-chat, deepseek-coder
    """

    def __init__(
        self,
        model: str,
        temperature: float,
        max_tokens: int,
        timeout: float,
        client_type: LLMClientType,
        api_key: Optional[str] = None,
    ):
        api_key = api_key or settings.deepseek_api_key
        base_url = "https://api.deepseek.com"

        if not api_key:
            raise ValueError("DEEPSEEK_API_KEY not configured. Set it in .env.")

        super().__init__(model, temperature, max_tokens, timeout,
                       client_type, base_url, "deepseek", api_key)


def get_extraction_llm_client() -> LLMClient:
    """Factory for extraction LLM client (nodes/edges/stance)."""
    return get_llm_client("extraction")


def get_scoring_llm_client() -> LLMClient:
    """Factory for scoring LLM client (diagnostic signals)."""
    return get_llm_client("scoring")


def get_generation_llm_client() -> LLMClient:
    """Factory for generation LLM client (question generation)."""
    return get_llm_client("generation")


def get_llm_client(client_type: LLMClientType) -> LLMClient:
    """
    Factory for LLM client based on client type.

    Uses hardcoded defaults for each client type, with optional
    environment variable overrides (LLM_EXTRACTION_PROVIDER, etc.).
    """
    defaults = DEFAULTS_MAP[client_type]

    # Check for environment override
    env_override_key = f"llm_{client_type}_provider"
    provider = getattr(settings, env_override_key, None) or defaults["provider"]

    if provider == "anthropic":
        return AnthropicClient(
            model=defaults["model"],
            temperature=defaults["temperature"],
            max_tokens=defaults["max_tokens"],
            timeout=defaults["timeout"],
            client_type=client_type,
        )
    elif provider == "kimi":
        return KimiClient(
            model=defaults["model"],
            temperature=defaults["temperature"],
            max_tokens=defaults["max_tokens"],
            timeout=defaults["timeout"],
            client_type=client_type,
        )
    elif provider == "deepseek":
        return DeepSeekClient(
            model=defaults["model"],
            temperature=defaults["temperature"],
            max_tokens=defaults["max_tokens"],
            timeout=defaults["timeout"],
            client_type=client_type,
        )
    else:
        raise ValueError(f"Unknown LLM provider '{provider}' for {client_type}")
```

## Service Usage

| Service | Client Factory | Purpose |
|---------|---------------|---------|
| `ExtractionService` | `get_extraction_llm_client()` | Extract nodes/edges/stance |
| `QuestionService` | `get_generation_llm_client()` | Generate questions |
| `QualitativeSignalExtractor` | `get_scoring_llm_client()` | Extract scoring signals |

## Requirements
1. Support Anthropic, Kimi, and DeepSeek providers
2. Three task-specific clients with appropriate defaults
3. Hardcoded defaults in code, minimal .env configuration
4. OpenAI-compatible API format for Kimi/DeepSeek
5. Structured logging for all LLM calls

## Verification
```bash
# Test extraction client (Anthropic)
python3 -c "
from src.llm.client import get_extraction_llm_client
client = get_extraction_llm_client()
print(f'Extraction: {client.__class__.__name__} with {client.model}')
"

# Test scoring client (Kimi)
python3 -c "
from src.llm.client import get_scoring_llm_client
client = get_scoring_llm_client()
print(f'Scoring: {client.__class__.__name__} with {client.model}')
"

# Test generation client (Anthropic)
python3 -c "
from src.llm.client import get_generation_llm_client
client = get_generation_llm_client()
print(f'Generation: {client.__class__.__name__} with {client.model}')
"
```

## Success Criteria
- [x] Three client types: extraction, scoring, generation
- [x] Anthropic, Kimi, DeepSeek provider support
- [x] Hardcoded defaults in `client.py`
- [x] Minimal `.env` (only API keys)
- [x] Factory functions for each client type
- [x] OpenAI-compatible base class
- [x] Structured logging with provider and client_type
