# Spec 2.7: Question Prompts

## Objective
Create LLM prompts for generating follow-up questions based on strategy and context.

## Context
- Reference: PRD Section 4.4 (Adaptive Strategy Selection), ENGINEERING_GUIDE.md Section 4.3
- v1 Reference: `src/modules/question_generator.py`
- Phase 2 simplification: Hardcoded "deepen" strategy (Phase 3 adds full strategy selection)
- Questions should be natural, conversational, and methodology-aware

## Input Files
- `PRD.md` - Section 4.4 (strategies: DEEPEN, BROADEN, COVER, CLOSE)
- `ENGINEERING_GUIDE.md` - Section 4.2-4.3 (methodology config)
- v1: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/question_generator.py`

## Output Files

### src/llm/prompts/question.py
```python
"""
Prompts for question generation.

Generates follow-up questions based on:
- Selected strategy (deepen, broaden, cover, close)
- Current graph state (what we know so far)
- Recent conversation context
- Focus concept (what to ask about)

Phase 2: Hardcoded "deepen" strategy.
Phase 3: Full strategy selection.
"""

from typing import Optional, List, Dict, Any


# Strategy descriptions for prompts
STRATEGY_DESCRIPTIONS = {
    "deepen": {
        "name": "Deepen",
        "intent": "Explore why something matters to understand deeper motivations",
        "probe_style": "Ask 'why is that important?' type questions",
        "example": "You mentioned {concept} - why is that important to you?",
    },
    "broaden": {
        "name": "Broaden",
        "intent": "Find new branches and related concepts",
        "probe_style": "Ask 'what else?' type questions",
        "example": "Besides {concept}, what else matters to you about this?",
    },
    "cover": {
        "name": "Cover",
        "intent": "Explore untouched stimulus elements",
        "probe_style": "Introduce new topic naturally",
        "example": "I'd like to hear your thoughts about {element}...",
    },
    "close": {
        "name": "Close",
        "intent": "Wrap up the interview naturally",
        "probe_style": "Summarize and invite final thoughts",
        "example": "We've covered a lot - is there anything else you'd like to add?",
    },
}


def get_question_system_prompt(strategy: str = "deepen") -> str:
    """
    Get system prompt for question generation.

    Args:
        strategy: Strategy name (deepen, broaden, cover, close)

    Returns:
        System prompt string
    """
    strat = STRATEGY_DESCRIPTIONS.get(strategy, STRATEGY_DESCRIPTIONS["deepen"])

    return f"""You are a skilled qualitative researcher conducting an interview using the Means-End Chain methodology.

Your current strategy is: **{strat['name']}**
Strategy intent: {strat['intent']}
Probe style: {strat['probe_style']}

## Interview Guidelines:
1. Ask ONE question at a time
2. Use the respondent's own language and concepts
3. Be warm, curious, and non-judgmental
4. Questions should feel natural and conversational
5. Avoid leading questions - stay open-ended
6. Reference what the respondent said to show you're listening

## Means-End Chain Methodology:
- Start with concrete Attributes (product features)
- Probe toward Functional Consequences (what it does)
- Move to Psychosocial Consequences (how it makes them feel)
- Ultimately reach Values (why it matters deeply)

The typical "laddering" question is: "Why is that important to you?"

## Output:
Generate ONLY the question - no explanations, no quotation marks, just the question itself."""


def get_question_user_prompt(
    focus_concept: str,
    recent_utterances: Optional[List[Dict[str, str]]] = None,
    graph_summary: Optional[str] = None,
    strategy: str = "deepen",
) -> str:
    """
    Get user prompt for question generation.

    Args:
        focus_concept: Concept to focus the question on
        recent_utterances: Recent conversation turns [{"speaker": "user/system", "text": "..."}]
        graph_summary: Summary of what we know so far
        strategy: Strategy name

    Returns:
        User prompt string
    """
    prompt_parts = []

    # Add recent conversation context
    if recent_utterances:
        context_lines = []
        for utt in recent_utterances[-5:]:  # Last 5 turns
            speaker = "Respondent" if utt.get("speaker") == "user" else "Interviewer"
            context_lines.append(f"{speaker}: {utt['text']}")

        prompt_parts.append("Recent conversation:")
        prompt_parts.append("\n".join(context_lines))
        prompt_parts.append("")

    # Add graph summary if available
    if graph_summary:
        prompt_parts.append(f"What we know so far: {graph_summary}")
        prompt_parts.append("")

    # Add focus and strategy
    strat = STRATEGY_DESCRIPTIONS.get(strategy, STRATEGY_DESCRIPTIONS["deepen"])
    prompt_parts.append(f"Focus concept: {focus_concept}")
    prompt_parts.append(f"Strategy: {strat['name']} - {strat['intent']}")
    prompt_parts.append("")
    prompt_parts.append("Generate a natural follow-up question:")

    return "\n".join(prompt_parts)


def get_opening_question_system_prompt() -> str:
    """
    Get system prompt for generating opening questions.

    Returns:
        System prompt string
    """
    return """You are starting a qualitative research interview about a product or concept.

Your goal is to warmly invite the participant to share their initial thoughts.

## Guidelines:
1. Be friendly and put the respondent at ease
2. Ask about their general thoughts, experiences, or associations
3. Keep it open-ended - don't assume anything
4. Use simple, conversational language
5. One question only

## Output:
Generate ONLY the opening question - no explanations, no quotation marks."""


def get_opening_question_user_prompt(concept_name: str, description: str = "") -> str:
    """
    Get user prompt for generating opening question.

    Args:
        concept_name: Name of the concept/product being discussed
        description: Optional description of the concept

    Returns:
        User prompt string
    """
    prompt = f"Generate an opening question about: {concept_name}"

    if description:
        prompt += f"\n\nProduct description: {description}"

    prompt += "\n\nGenerate a warm, open-ended opening question:"

    return prompt


def get_graph_summary(
    nodes_by_type: Dict[str, int],
    recent_concepts: List[str],
    depth_achieved: int,
) -> str:
    """
    Generate a brief graph summary for context.

    Args:
        nodes_by_type: Count of nodes by type
        recent_concepts: Recently discussed concepts
        depth_achieved: How deep we've gone in the chain

    Returns:
        Brief summary string
    """
    parts = []

    # Depth progress
    depth_labels = ["starting", "surface", "developing", "deep", "very deep"]
    depth_label = depth_labels[min(depth_achieved, len(depth_labels) - 1)]
    parts.append(f"Depth: {depth_label}")

    # What we've covered
    total_nodes = sum(nodes_by_type.values())
    if total_nodes > 0:
        parts.append(f"Explored {total_nodes} concepts")

    # Recent focus
    if recent_concepts:
        recent = ", ".join(recent_concepts[:3])
        parts.append(f"Recent topics: {recent}")

    return " | ".join(parts)


def format_question(raw_question: str) -> str:
    """
    Clean up generated question.

    Args:
        raw_question: Raw LLM output

    Returns:
        Cleaned question string
    """
    question = raw_question.strip()

    # Remove surrounding quotes if present
    if question.startswith('"') and question.endswith('"'):
        question = question[1:-1]
    if question.startswith("'") and question.endswith("'"):
        question = question[1:-1]

    # Ensure ends with question mark or appropriate punctuation
    if question and question[-1] not in ".?!":
        question += "?"

    return question
```

### tests/unit/test_question_prompts.py
```python
"""Tests for question prompts."""

import pytest

from src.llm.prompts.question import (
    get_question_system_prompt,
    get_question_user_prompt,
    get_opening_question_system_prompt,
    get_opening_question_user_prompt,
    get_graph_summary,
    format_question,
    STRATEGY_DESCRIPTIONS,
)


class TestQuestionSystemPrompt:
    """Tests for question system prompt generation."""

    def test_includes_strategy_name(self):
        """System prompt includes strategy name."""
        prompt = get_question_system_prompt("deepen")
        assert "Deepen" in prompt

    def test_includes_strategy_intent(self):
        """System prompt includes strategy intent."""
        prompt = get_question_system_prompt("deepen")
        assert "deeper motivations" in prompt.lower()

    def test_includes_methodology(self):
        """System prompt includes Means-End Chain methodology."""
        prompt = get_question_system_prompt()
        assert "Means-End Chain" in prompt

    def test_different_strategies(self):
        """Different strategies produce different prompts."""
        deepen = get_question_system_prompt("deepen")
        broaden = get_question_system_prompt("broaden")

        assert "why" in deepen.lower()
        assert "what else" in broaden.lower()

    def test_defaults_to_deepen(self):
        """Unknown strategy defaults to deepen."""
        prompt = get_question_system_prompt("unknown")
        assert "Deepen" in prompt


class TestQuestionUserPrompt:
    """Tests for question user prompt generation."""

    def test_includes_focus_concept(self):
        """User prompt includes focus concept."""
        prompt = get_question_user_prompt("creamy texture")
        assert "creamy texture" in prompt

    def test_includes_recent_utterances(self):
        """User prompt includes conversation context."""
        utterances = [
            {"speaker": "system", "text": "What do you think?"},
            {"speaker": "user", "text": "I like the taste"},
        ]
        prompt = get_question_user_prompt("taste", recent_utterances=utterances)

        assert "I like the taste" in prompt
        assert "Respondent:" in prompt

    def test_includes_graph_summary(self):
        """User prompt includes graph summary."""
        prompt = get_question_user_prompt(
            "test",
            graph_summary="Explored 5 concepts",
        )
        assert "5 concepts" in prompt

    def test_limits_recent_utterances(self):
        """Only includes last 5 utterances."""
        utterances = [{"speaker": "user", "text": f"Turn {i}"} for i in range(10)]
        prompt = get_question_user_prompt("test", recent_utterances=utterances)

        assert "Turn 9" in prompt
        assert "Turn 0" not in prompt  # Older turns excluded


class TestOpeningQuestionPrompts:
    """Tests for opening question prompts."""

    def test_system_prompt_is_warm(self):
        """Opening system prompt encourages warmth."""
        prompt = get_opening_question_system_prompt()
        assert "warm" in prompt.lower() or "friendly" in prompt.lower()

    def test_user_prompt_includes_concept(self):
        """User prompt includes concept name."""
        prompt = get_opening_question_user_prompt("Oat Milk")
        assert "Oat Milk" in prompt

    def test_user_prompt_includes_description(self):
        """User prompt includes description when provided."""
        prompt = get_opening_question_user_prompt(
            "Oat Milk",
            description="Plant-based milk alternative"
        )
        assert "Plant-based" in prompt


class TestGraphSummary:
    """Tests for graph summary generation."""

    def test_includes_depth(self):
        """Summary includes depth label."""
        summary = get_graph_summary(
            nodes_by_type={"attribute": 2},
            recent_concepts=["texture"],
            depth_achieved=2,
        )
        assert "developing" in summary.lower() or "depth" in summary.lower()

    def test_includes_node_count(self):
        """Summary includes total node count."""
        summary = get_graph_summary(
            nodes_by_type={"attribute": 3, "functional_consequence": 2},
            recent_concepts=[],
            depth_achieved=1,
        )
        assert "5" in summary

    def test_includes_recent_concepts(self):
        """Summary includes recent concepts."""
        summary = get_graph_summary(
            nodes_by_type={},
            recent_concepts=["texture", "taste"],
            depth_achieved=0,
        )
        assert "texture" in summary


class TestFormatQuestion:
    """Tests for question formatting."""

    def test_strips_whitespace(self):
        """Strips leading/trailing whitespace."""
        result = format_question("  Hello?  ")
        assert result == "Hello?"

    def test_removes_quotes(self):
        """Removes surrounding quotes."""
        assert format_question('"What do you think?"') == "What do you think?"
        assert format_question("'What do you think?'") == "What do you think?"

    def test_adds_question_mark(self):
        """Adds question mark if missing."""
        result = format_question("Why is that important")
        assert result.endswith("?")

    def test_preserves_existing_punctuation(self):
        """Preserves existing punctuation."""
        assert format_question("Tell me more.") == "Tell me more."
        assert format_question("Why?") == "Why?"
```

## Requirements
1. Prompts must be strategy-aware (deepen, broaden, cover, close)
2. System prompts must include Means-End Chain methodology guidance
3. User prompts must include conversation context
4. Opening question prompts must be warm and inviting
5. Questions must be cleaned (remove quotes, ensure punctuation)
6. Graph summary provides context without overwhelming detail

## Verification
```bash
# Run prompt tests
pytest tests/unit/test_question_prompts.py -v

# Verify imports
python3 -c "from src.llm.prompts.question import get_question_system_prompt, format_question; print('Question prompts imported')"

# Test prompt content
python3 -c "
from src.llm.prompts.question import get_question_system_prompt
prompt = get_question_system_prompt('deepen')
assert 'Deepen' in prompt
assert 'Means-End Chain' in prompt
print('Question prompts working')
"
```

## Success Criteria
- [ ] `STRATEGY_DESCRIPTIONS` dict with all 4 strategies
- [ ] `get_question_system_prompt()` includes strategy and methodology
- [ ] `get_question_user_prompt()` includes focus, context, and summary
- [ ] `get_opening_question_system_prompt()` is warm and inviting
- [ ] `get_opening_question_user_prompt()` includes concept name
- [ ] `get_graph_summary()` creates brief context string
- [ ] `format_question()` cleans LLM output
- [ ] All tests pass
