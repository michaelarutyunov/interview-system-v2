# Spec 2.3: Extraction Prompts

## Objective
Create LLM prompts for extracting concepts and relationships from user responses.

## Context
- Reference: PRD Section 4.1 (Interview Conduction), Section 7.3 (Knowledge Graph Schema)
- v1 Reference: `src/modules/language_understanding.py` (extraction prompts)
- Prompts must produce structured JSON output for parsing
- Methodology-aware: Uses Means-End Chain node types

## Input Files
- `PRD.md` - Section 7.3 (node types, edge types)
- `ENGINEERING_GUIDE.md` - Section 4.3 (methodology schema)
- v1: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/language_understanding.py`

## Output Files

### src/llm/prompts/extraction.py
```python
"""
Prompts for concept and relationship extraction.

Extracts:
- Concepts (potential knowledge graph nodes)
- Relationships (edges between concepts)
- Discourse markers (linguistic signals)

All prompts produce JSON for structured parsing.
"""

from typing import Dict, Any

# Node type descriptions for Means-End Chain methodology
NODE_TYPE_DESCRIPTIONS = {
    "attribute": "Concrete product feature or characteristic (e.g., 'creamy texture', 'plant-based')",
    "functional_consequence": "Tangible outcome from using the product (e.g., 'easier to digest', 'mixes well')",
    "psychosocial_consequence": "Emotional or social outcome (e.g., 'feel healthier', 'impress friends')",
    "instrumental_value": "Preferred mode of behavior (e.g., 'being responsible', 'taking care of myself')",
    "terminal_value": "End-state of existence (e.g., 'happiness', 'security', 'self-fulfillment')",
}

# Edge type descriptions
EDGE_TYPE_DESCRIPTIONS = {
    "leads_to": "Causal or enabling relationship (source enables/causes target)",
    "revises": "Contradiction - newer belief supersedes older one",
}


def get_extraction_system_prompt() -> str:
    """
    Get system prompt for concept/relationship extraction.

    Returns:
        System prompt string for LLM
    """
    node_types_str = "\n".join(
        f"  - {name}: {desc}" for name, desc in NODE_TYPE_DESCRIPTIONS.items()
    )
    edge_types_str = "\n".join(
        f"  - {name}: {desc}" for name, desc in EDGE_TYPE_DESCRIPTIONS.items()
    )

    return f"""You are an expert qualitative researcher extracting knowledge from interview responses.

Your task is to identify concepts and relationships from the respondent's text that reveal their mental model about the product being discussed.

## Valid Node Types (Means-End Chain):
{node_types_str}

## Valid Edge Types:
{edge_types_str}

## Extraction Guidelines:
1. Only extract concepts EXPLICITLY mentioned or clearly implied
2. Use the respondent's own language for concept labels
3. Classify each concept into the most appropriate node type
4. Identify causal relationships indicated by language like "because", "so", "that's why"
5. Look for discourse markers that signal relationships
6. Assign confidence based on how explicit the concept/relationship is
7. Include the verbatim quote that supports each extraction

## Output Format:
Return valid JSON with this structure:
{{
  "concepts": [
    {{
      "text": "concept label in respondent's words",
      "node_type": "one of the valid node types",
      "confidence": 0.0-1.0,
      "source_quote": "verbatim text that supports this"
    }}
  ],
  "relationships": [
    {{
      "source_text": "source concept label",
      "target_text": "target concept label",
      "relationship_type": "leads_to or revises",
      "confidence": 0.0-1.0,
      "source_quote": "verbatim text showing relationship"
    }}
  ],
  "discourse_markers": ["because", "so", ...]
}}

If the text contains no extractable concepts, return:
{{"concepts": [], "relationships": [], "discourse_markers": []}}"""


def get_extraction_user_prompt(text: str, context: str = "") -> str:
    """
    Get user prompt for extraction with the respondent's text.

    Args:
        text: Respondent's utterance to extract from
        context: Optional context from previous turns

    Returns:
        User prompt string
    """
    prompt = f'Extract concepts and relationships from this response:\n\n"{text}"'

    if context:
        prompt = f"Previous context:\n{context}\n\n{prompt}"

    return prompt


def get_extractability_system_prompt() -> str:
    """
    Get system prompt for assessing extractability.

    Used as a fast pre-filter before full extraction.

    Returns:
        System prompt string
    """
    return """You are assessing whether text contains extractable knowledge for a qualitative research interview.

Extractable text contains:
- Product attributes, features, or characteristics
- Benefits, outcomes, or consequences
- Feelings, emotions, or social implications
- Values or life goals
- Causal relationships between any of the above

Non-extractable text includes:
- Simple yes/no responses
- Acknowledgments ("okay", "I see")
- Questions back to the interviewer
- Off-topic tangents
- Very short responses with no substance

Return JSON:
{
  "extractable": true or false,
  "reason": "brief explanation"
}"""


def get_extractability_user_prompt(text: str) -> str:
    """
    Get user prompt for extractability assessment.

    Args:
        text: Text to assess

    Returns:
        User prompt string
    """
    return f'Is this utterance extractable?\n\n"{text}"'


def parse_extraction_response(response_text: str) -> Dict[str, Any]:
    """
    Parse LLM extraction response into structured data.

    Args:
        response_text: Raw LLM response (should be JSON)

    Returns:
        Parsed dict with concepts, relationships, discourse_markers

    Raises:
        ValueError: If response is not valid JSON
    """
    import json

    # Try to extract JSON from response (handle markdown code blocks)
    text = response_text.strip()

    # Remove markdown code block if present
    if text.startswith("```json"):
        text = text[7:]
    if text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]

    text = text.strip()

    try:
        data = json.loads(text)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in extraction response: {e}")

    # Validate structure
    if not isinstance(data, dict):
        raise ValueError("Extraction response must be a JSON object")

    # Ensure required keys exist with defaults
    return {
        "concepts": data.get("concepts", []),
        "relationships": data.get("relationships", []),
        "discourse_markers": data.get("discourse_markers", []),
    }


def parse_extractability_response(response_text: str) -> tuple[bool, str]:
    """
    Parse LLM extractability response.

    Args:
        response_text: Raw LLM response (should be JSON)

    Returns:
        (is_extractable, reason) tuple

    Raises:
        ValueError: If response is not valid JSON
    """
    import json

    text = response_text.strip()

    # Remove markdown code block if present
    if text.startswith("```json"):
        text = text[7:]
    if text.startswith("```"):
        text = text[3:]
    if text.endswith("```"):
        text = text[:-3]

    text = text.strip()

    try:
        data = json.loads(text)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in extractability response: {e}")

    return (
        bool(data.get("extractable", True)),
        str(data.get("reason", ""))
    )
```

### tests/unit/test_extraction_prompts.py
```python
"""Tests for extraction prompts."""

import pytest
import json

from src.llm.prompts.extraction import (
    get_extraction_system_prompt,
    get_extraction_user_prompt,
    get_extractability_system_prompt,
    get_extractability_user_prompt,
    parse_extraction_response,
    parse_extractability_response,
    NODE_TYPE_DESCRIPTIONS,
    EDGE_TYPE_DESCRIPTIONS,
)


class TestExtractionPrompts:
    """Tests for extraction prompt generation."""

    def test_system_prompt_includes_node_types(self):
        """System prompt includes all node type descriptions."""
        prompt = get_extraction_system_prompt()

        for node_type in NODE_TYPE_DESCRIPTIONS:
            assert node_type in prompt

    def test_system_prompt_includes_edge_types(self):
        """System prompt includes all edge type descriptions."""
        prompt = get_extraction_system_prompt()

        for edge_type in EDGE_TYPE_DESCRIPTIONS:
            assert edge_type in prompt

    def test_system_prompt_specifies_json_output(self):
        """System prompt specifies JSON output format."""
        prompt = get_extraction_system_prompt()

        assert "JSON" in prompt
        assert '"concepts"' in prompt
        assert '"relationships"' in prompt

    def test_user_prompt_includes_text(self):
        """User prompt includes the text to extract."""
        text = "I love the creamy texture because it's satisfying"
        prompt = get_extraction_user_prompt(text)

        assert text in prompt

    def test_user_prompt_with_context(self):
        """User prompt includes context when provided."""
        text = "That's right"
        context = "Interviewer asked about texture preferences"
        prompt = get_extraction_user_prompt(text, context)

        assert text in prompt
        assert context in prompt
        assert "Previous context" in prompt


class TestExtractabilityPrompts:
    """Tests for extractability prompt generation."""

    def test_extractability_system_prompt(self):
        """Extractability system prompt explains criteria."""
        prompt = get_extractability_system_prompt()

        assert "extractable" in prompt.lower()
        assert "yes/no" in prompt.lower()
        assert "JSON" in prompt

    def test_extractability_user_prompt(self):
        """Extractability user prompt includes text."""
        text = "Yes"
        prompt = get_extractability_user_prompt(text)

        assert text in prompt


class TestParseExtractionResponse:
    """Tests for parsing extraction responses."""

    def test_parse_valid_json(self):
        """Parses valid JSON response."""
        response = json.dumps({
            "concepts": [
                {"text": "creamy", "node_type": "attribute", "confidence": 0.9}
            ],
            "relationships": [],
            "discourse_markers": ["because"]
        })

        result = parse_extraction_response(response)

        assert len(result["concepts"]) == 1
        assert result["concepts"][0]["text"] == "creamy"
        assert result["discourse_markers"] == ["because"]

    def test_parse_json_with_code_block(self):
        """Parses JSON wrapped in markdown code block."""
        response = '''```json
{
  "concepts": [],
  "relationships": [],
  "discourse_markers": []
}
```'''

        result = parse_extraction_response(response)

        assert result["concepts"] == []

    def test_parse_empty_response(self):
        """Handles empty extraction result."""
        response = '{"concepts": [], "relationships": [], "discourse_markers": []}'

        result = parse_extraction_response(response)

        assert result["concepts"] == []
        assert result["relationships"] == []

    def test_parse_invalid_json_raises(self):
        """Raises ValueError for invalid JSON."""
        with pytest.raises(ValueError, match="Invalid JSON"):
            parse_extraction_response("not json")

    def test_parse_missing_keys_uses_defaults(self):
        """Missing keys default to empty lists."""
        response = '{}'

        result = parse_extraction_response(response)

        assert result["concepts"] == []
        assert result["relationships"] == []
        assert result["discourse_markers"] == []


class TestParseExtractabilityResponse:
    """Tests for parsing extractability responses."""

    def test_parse_extractable_true(self):
        """Parses extractable=true response."""
        response = '{"extractable": true, "reason": "Contains product attributes"}'

        is_extractable, reason = parse_extractability_response(response)

        assert is_extractable is True
        assert "attributes" in reason

    def test_parse_extractable_false(self):
        """Parses extractable=false response."""
        response = '{"extractable": false, "reason": "Yes/no response"}'

        is_extractable, reason = parse_extractability_response(response)

        assert is_extractable is False
        assert "Yes/no" in reason

    def test_parse_with_code_block(self):
        """Parses JSON wrapped in code block."""
        response = '```json\n{"extractable": true, "reason": "ok"}\n```'

        is_extractable, reason = parse_extractability_response(response)

        assert is_extractable is True

    def test_parse_invalid_json_raises(self):
        """Raises ValueError for invalid JSON."""
        with pytest.raises(ValueError, match="Invalid JSON"):
            parse_extractability_response("yes")
```

## Requirements
1. System prompts must include all valid node types (Means-End Chain)
2. System prompts must include all valid edge types
3. Output format must be JSON for reliable parsing
4. Parser must handle markdown code blocks (`\`\`\`json`)
5. Parser must provide sensible defaults for missing keys
6. Prompts must emphasize using respondent's own language
7. Confidence scoring must be explained in prompts

## Verification
```bash
# Run prompt tests
pytest tests/unit/test_extraction_prompts.py -v

# Verify imports
python3 -c "from src.llm.prompts.extraction import get_extraction_system_prompt, parse_extraction_response; print('Prompts imported')"

# Test prompt content
python3 -c "
from src.llm.prompts.extraction import get_extraction_system_prompt
prompt = get_extraction_system_prompt()
assert 'attribute' in prompt
assert 'leads_to' in prompt
assert 'JSON' in prompt
print('Prompt contains required elements')
"
```

## Success Criteria
- [ ] `get_extraction_system_prompt()` includes all node types
- [ ] `get_extraction_system_prompt()` includes all edge types
- [ ] `get_extraction_system_prompt()` specifies JSON output format
- [ ] `get_extraction_user_prompt()` accepts text and optional context
- [ ] `get_extractability_system_prompt()` explains extractability criteria
- [ ] `parse_extraction_response()` handles valid JSON
- [ ] `parse_extraction_response()` handles markdown code blocks
- [ ] `parse_extraction_response()` raises on invalid JSON
- [ ] `parse_extractability_response()` returns (bool, str) tuple
- [ ] All tests pass
