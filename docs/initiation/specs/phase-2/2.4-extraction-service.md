# Spec 2.4: Extraction Service

## Objective
Create the extraction service that processes user responses and returns structured extraction results.

## Context
- Reference: PRD Section 4.1 (Interview Conduction step 2: "Extracts concepts and relationships")
- v1 Reference: `src/modules/language_understanding.py`
- Orchestrates: extractability check → extraction → result parsing
- Uses LLM client (2.2) and extraction prompts (2.3)

## Input Files
- `src/llm/client.py` - LLM client from spec 2.2
- `src/llm/prompts/extraction.py` - Prompts from spec 2.3
- `src/domain/models/extraction.py` - Models from spec 2.1
- v1: `/home/mikhailarutyunov/projects/graph-enabled-ai-interviewer/src/modules/language_understanding.py`

## Output Files

### src/services/extraction_service.py
```python
"""
Extraction service for processing user responses.

Pipeline:
1. Assess extractability (fast pre-filter)
2. Extract concepts and relationships via LLM
3. Parse and validate results
4. Return ExtractionResult

Graceful degradation: Returns empty result on LLM errors.
"""

import time
from typing import Optional, List

import structlog

from src.llm.client import LLMClient, get_llm_client
from src.llm.prompts.extraction import (
    get_extraction_system_prompt,
    get_extraction_user_prompt,
    get_extractability_system_prompt,
    get_extractability_user_prompt,
    parse_extraction_response,
    parse_extractability_response,
)
from src.domain.models.extraction import (
    ExtractedConcept,
    ExtractedRelationship,
    ExtractionResult,
)

log = structlog.get_logger(__name__)


class ExtractionService:
    """
    Service for extracting concepts and relationships from text.

    Uses LLM to identify knowledge graph elements from user responses.
    """

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        skip_extractability_check: bool = False,
        min_word_count: int = 3,
    ):
        """
        Initialize extraction service.

        Args:
            llm_client: LLM client instance (creates default if None)
            skip_extractability_check: Skip fast pre-filter (for testing)
            min_word_count: Minimum words for extractability
        """
        self.llm = llm_client or get_llm_client()
        self.skip_extractability_check = skip_extractability_check
        self.min_word_count = min_word_count

        log.info("extraction_service_initialized")

    async def extract(
        self,
        text: str,
        context: str = "",
    ) -> ExtractionResult:
        """
        Extract concepts and relationships from text.

        Full pipeline:
        1. Fast heuristic check (word count, yes/no detection)
        2. LLM extractability assessment (if heuristics pass)
        3. Full LLM extraction
        4. Parse and validate results

        Args:
            text: User's response text
            context: Optional context from previous turns

        Returns:
            ExtractionResult with concepts, relationships, and metadata
        """
        start_time = time.perf_counter()
        log.info("extraction_started", text_length=len(text))

        # Step 1: Fast heuristic check
        if not self.skip_extractability_check:
            is_extractable, reason = self._fast_extractability_check(text)
            if not is_extractable:
                log.info("extraction_skipped_heuristic", reason=reason)
                return ExtractionResult(
                    is_extractable=False,
                    extractability_reason=reason,
                    latency_ms=int((time.perf_counter() - start_time) * 1000),
                )

        # Step 2: LLM extractability check (optional, can skip for speed)
        # Skipping for v2 MVP - heuristics are sufficient

        # Step 3: Full extraction via LLM
        try:
            extraction_data = await self._extract_via_llm(text, context)
        except Exception as e:
            log.error("extraction_llm_error", error=str(e))
            # Graceful degradation: return empty result
            return ExtractionResult(
                is_extractable=True,
                extractability_reason=f"LLM error: {e}",
                latency_ms=int((time.perf_counter() - start_time) * 1000),
            )

        # Step 4: Convert to domain models
        concepts = self._parse_concepts(extraction_data.get("concepts", []))
        relationships = self._parse_relationships(extraction_data.get("relationships", []))
        discourse_markers = extraction_data.get("discourse_markers", [])

        latency_ms = int((time.perf_counter() - start_time) * 1000)

        log.info(
            "extraction_complete",
            concept_count=len(concepts),
            relationship_count=len(relationships),
            latency_ms=latency_ms,
        )

        return ExtractionResult(
            concepts=concepts,
            relationships=relationships,
            discourse_markers=discourse_markers,
            is_extractable=True,
            latency_ms=latency_ms,
        )

    def _fast_extractability_check(self, text: str) -> tuple[bool, Optional[str]]:
        """
        Fast heuristic check for extractability.

        Args:
            text: Input text

        Returns:
            (is_extractable, reason) tuple
        """
        # Word count check
        word_count = len(text.split())
        if word_count < self.min_word_count:
            return False, f"Too short ({word_count} words)"

        # Yes/no response check
        normalized = text.lower().strip()
        yes_no_responses = {
            "yes", "no", "yeah", "nope", "yep", "nah",
            "sure", "okay", "ok", "fine", "right",
            "uh huh", "mm hmm", "mhm",
        }
        if normalized in yes_no_responses:
            return False, "Yes/no or minimal response"

        # Single word check
        if word_count == 1:
            return False, "Single word response"

        return True, None

    async def _extract_via_llm(self, text: str, context: str) -> dict:
        """
        Call LLM for extraction.

        Args:
            text: Text to extract from
            context: Optional context

        Returns:
            Parsed extraction data dict

        Raises:
            ValueError: If LLM response is invalid
        """
        system_prompt = get_extraction_system_prompt()
        user_prompt = get_extraction_user_prompt(text, context)

        response = await self.llm.complete(
            prompt=user_prompt,
            system=system_prompt,
            temperature=0.3,  # Lower temperature for more consistent extraction
            max_tokens=2000,
        )

        return parse_extraction_response(response.content)

    def _parse_concepts(self, raw_concepts: List[dict]) -> List[ExtractedConcept]:
        """
        Convert raw extraction data to ExtractedConcept models.

        Args:
            raw_concepts: List of concept dicts from LLM

        Returns:
            List of ExtractedConcept models
        """
        concepts = []
        for raw in raw_concepts:
            try:
                concept = ExtractedConcept(
                    text=raw.get("text", ""),
                    node_type=raw.get("node_type", "attribute"),
                    confidence=float(raw.get("confidence", 0.8)),
                    source_quote=raw.get("source_quote", ""),
                    properties=raw.get("properties", {}),
                )
                if concept.text:  # Skip empty concepts
                    concepts.append(concept)
            except Exception as e:
                log.warning("concept_parse_error", raw=raw, error=str(e))

        return concepts

    def _parse_relationships(
        self, raw_relationships: List[dict]
    ) -> List[ExtractedRelationship]:
        """
        Convert raw extraction data to ExtractedRelationship models.

        Args:
            raw_relationships: List of relationship dicts from LLM

        Returns:
            List of ExtractedRelationship models
        """
        relationships = []
        for raw in raw_relationships:
            try:
                rel = ExtractedRelationship(
                    source_text=raw.get("source_text", ""),
                    target_text=raw.get("target_text", ""),
                    relationship_type=raw.get("relationship_type", "leads_to"),
                    confidence=float(raw.get("confidence", 0.7)),
                    source_quote=raw.get("source_quote", ""),
                )
                if rel.source_text and rel.target_text:  # Skip incomplete
                    relationships.append(rel)
            except Exception as e:
                log.warning("relationship_parse_error", raw=raw, error=str(e))

        return relationships

    async def assess_extractability(self, text: str) -> tuple[bool, str]:
        """
        Assess whether text is extractable using LLM.

        More expensive than heuristics but more accurate.

        Args:
            text: Text to assess

        Returns:
            (is_extractable, reason) tuple
        """
        # First do fast check
        is_extractable, reason = self._fast_extractability_check(text)
        if not is_extractable:
            return is_extractable, reason or ""

        # Then LLM check
        try:
            system_prompt = get_extractability_system_prompt()
            user_prompt = get_extractability_user_prompt(text)

            response = await self.llm.complete(
                prompt=user_prompt,
                system=system_prompt,
                temperature=0.1,
                max_tokens=100,
            )

            return parse_extractability_response(response.content)
        except Exception as e:
            log.warning("extractability_llm_error", error=str(e))
            # Default to extractable on error
            return True, "LLM check failed, assuming extractable"
```

### tests/unit/test_extraction_service.py
```python
"""Tests for extraction service."""

import pytest
from unittest.mock import AsyncMock, MagicMock

from src.services.extraction_service import ExtractionService
from src.domain.models.extraction import ExtractionResult
from src.llm.client import LLMResponse


class TestExtractionService:
    """Tests for ExtractionService."""

    @pytest.fixture
    def mock_llm(self):
        """Create mock LLM client."""
        mock = AsyncMock()
        return mock

    @pytest.fixture
    def service(self, mock_llm):
        """Create extraction service with mock LLM."""
        return ExtractionService(llm_client=mock_llm)

    def test_fast_extractability_too_short(self, service):
        """Short text is not extractable."""
        is_extractable, reason = service._fast_extractability_check("Hi")

        assert not is_extractable
        assert "short" in reason.lower()

    def test_fast_extractability_yes_no(self, service):
        """Yes/no responses are not extractable."""
        for response in ["yes", "no", "yeah", "okay"]:
            is_extractable, reason = service._fast_extractability_check(response)
            assert not is_extractable

    def test_fast_extractability_valid(self, service):
        """Substantive text is extractable."""
        text = "I really like the creamy texture because it's satisfying"
        is_extractable, reason = service._fast_extractability_check(text)

        assert is_extractable
        assert reason is None

    @pytest.mark.asyncio
    async def test_extract_returns_result(self, service, mock_llm):
        """extract() returns ExtractionResult."""
        mock_llm.complete.return_value = LLMResponse(
            content='{"concepts": [], "relationships": [], "discourse_markers": []}',
            model="test",
        )

        result = await service.extract("I like the taste")

        assert isinstance(result, ExtractionResult)
        assert result.is_extractable

    @pytest.mark.asyncio
    async def test_extract_parses_concepts(self, service, mock_llm):
        """extract() parses concepts from LLM response."""
        mock_llm.complete.return_value = LLMResponse(
            content="""{
                "concepts": [
                    {"text": "creamy texture", "node_type": "attribute", "confidence": 0.9}
                ],
                "relationships": [],
                "discourse_markers": []
            }""",
            model="test",
        )

        result = await service.extract("I love the creamy texture")

        assert len(result.concepts) == 1
        assert result.concepts[0].text == "creamy texture"
        assert result.concepts[0].node_type == "attribute"

    @pytest.mark.asyncio
    async def test_extract_parses_relationships(self, service, mock_llm):
        """extract() parses relationships from LLM response."""
        mock_llm.complete.return_value = LLMResponse(
            content="""{
                "concepts": [
                    {"text": "creamy", "node_type": "attribute"},
                    {"text": "satisfying", "node_type": "functional_consequence"}
                ],
                "relationships": [
                    {"source_text": "creamy", "target_text": "satisfying", "relationship_type": "leads_to"}
                ],
                "discourse_markers": ["because"]
            }""",
            model="test",
        )

        result = await service.extract("The creamy texture is satisfying")

        assert len(result.relationships) == 1
        assert result.relationships[0].source_text == "creamy"
        assert result.relationships[0].relationship_type == "leads_to"
        assert "because" in result.discourse_markers

    @pytest.mark.asyncio
    async def test_extract_skips_non_extractable(self, service, mock_llm):
        """extract() skips non-extractable text."""
        result = await service.extract("yes")

        assert not result.is_extractable
        assert result.concepts == []
        mock_llm.complete.assert_not_called()  # LLM not called

    @pytest.mark.asyncio
    async def test_extract_handles_llm_error(self, service, mock_llm):
        """extract() returns empty result on LLM error."""
        mock_llm.complete.side_effect = Exception("API error")

        result = await service.extract("I like the product a lot")

        assert result.is_extractable  # Still marked extractable
        assert result.concepts == []
        assert "LLM error" in result.extractability_reason

    @pytest.mark.asyncio
    async def test_extract_records_latency(self, service, mock_llm):
        """extract() records latency in milliseconds."""
        mock_llm.complete.return_value = LLMResponse(
            content='{"concepts": [], "relationships": [], "discourse_markers": []}',
            model="test",
        )

        result = await service.extract("I like the taste")

        assert result.latency_ms >= 0

    def test_parse_concepts_handles_invalid(self, service):
        """_parse_concepts handles invalid data gracefully."""
        raw = [
            {"text": "valid", "node_type": "attribute"},
            {"invalid": "data"},  # Missing required fields
            {"text": "", "node_type": "attribute"},  # Empty text
        ]

        concepts = service._parse_concepts(raw)

        assert len(concepts) == 1
        assert concepts[0].text == "valid"

    def test_parse_relationships_handles_invalid(self, service):
        """_parse_relationships handles invalid data gracefully."""
        raw = [
            {"source_text": "a", "target_text": "b", "relationship_type": "leads_to"},
            {"source_text": "", "target_text": "b"},  # Empty source
            {"source_text": "a", "target_text": ""},  # Empty target
        ]

        relationships = service._parse_relationships(raw)

        assert len(relationships) == 1
        assert relationships[0].source_text == "a"
```

## Requirements
1. Must use LLM client from spec 2.2
2. Must use prompts from spec 2.3
3. Must return domain models from spec 2.1
4. Fast heuristic check must skip obvious non-extractable text
5. Must handle LLM errors gracefully (return empty result, don't crash)
6. Must log all extractions with structlog
7. Must record latency in milliseconds
8. Must validate and filter invalid extraction data

## Verification
```bash
# Run extraction service tests
pytest tests/unit/test_extraction_service.py -v

# Verify imports
python3 -c "from src.services.extraction_service import ExtractionService; print('Service imported')"

# Test fast extractability check
python3 -c "
from src.services.extraction_service import ExtractionService

# Create with skip LLM for testing
service = ExtractionService(skip_extractability_check=True)

# Test heuristics
assert not service._fast_extractability_check('yes')[0]
assert not service._fast_extractability_check('ok')[0]
assert service._fast_extractability_check('I really like the creamy texture')[0]
print('Heuristics working correctly')
"
```

## Success Criteria
- [ ] `ExtractionService` class created with `extract()` method
- [ ] Fast heuristic check filters yes/no, short responses
- [ ] LLM called with correct prompts from spec 2.3
- [ ] Response parsed into `ExtractedConcept` and `ExtractedRelationship` models
- [ ] `ExtractionResult` returned with all fields populated
- [ ] Graceful degradation on LLM errors (empty result, logged)
- [ ] Latency recorded in milliseconds
- [ ] Invalid extraction data filtered gracefully
- [ ] All tests pass with mocked LLM
