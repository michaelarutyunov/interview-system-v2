# Spec 4.3: Synthetic Endpoint

## Objective
Create the FastAPI endpoint for synthetic respondent generation.

## Context
- Reference: PRD Section 8.4 (Synthetic Respondent API)
- API endpoint: `POST /synthetic/respond`
- Enables automated test scripts to generate synthetic responses
- Follows existing API patterns from Phase 1-2

## Input Files
- Phase 1: `src/api/routes/sessions.py` - Reference for route patterns
- Phase 2: `src/api/schemas.py` - Request/response models
- Phase 4.2: `src/services/synthetic_service.py` - Synthetic service

## Output Files

### src/api/routes/synthetic.py
```python
"""
API routes for synthetic respondent generation.

POST /synthetic/respond - Generate synthetic response to a question
"""

from typing import Optional, List, Dict, Any
from fastapi import APIRouter, HTTPException, Depends, status
from pydantic import BaseModel, Field

import structlog

from src.services.synthetic_service import SyntheticService, get_synthetic_service
from src.domain.models.knowledge_graph import GraphState


log = structlog.get_logger(__name__)
router = APIRouter(prefix="/synthetic", tags=["synthetic"])


# Request/Response Models
class SyntheticRespondRequest(BaseModel):
    """Request for synthetic response generation."""
    
    question: str = Field(
        ...,
        description="The interviewer's question to respond to",
        min_length=1,
        max_length=1000,
    )
    session_id: str = Field(
        ...,
        description="Session identifier for logging and context",
        min_length=1,
    )
    persona: str = Field(
        default="health_conscious",
        description="Persona to use for response generation",
    )
    interview_context: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Optional interview context (product_name, turn_number, coverage_achieved)",
    )
    use_deflection: Optional[bool] = Field(
        default=None,
        description="Force deflection on/off (None=random based on chance)",
    )


class SyntheticRespondResponse(BaseModel):
    """Response from synthetic generation."""
    
    response: str = Field(description="Generated synthetic response")
    persona: str = Field(description="Persona ID used")
    persona_name: str = Field(description="Human-readable persona name")
    question: str = Field(description="Original question")
    latency_ms: float = Field(description="LLM generation latency")
    tokens_used: Dict[str, int] = Field(description="Token usage")
    used_deflection: bool = Field(description="Whether deflection prompt was used")


class MultiSyntheticRequest(BaseModel):
    """Request for multiple synthetic responses."""
    
    question: str = Field(
        ...,
        description="The interviewer's question",
        min_length=1,
        max_length=1000,
    )
    session_id: str = Field(
        ...,
        description="Session identifier",
    )
    personas: Optional[List[str]] = Field(
        default=None,
        description="List of personas (None = all available)",
    )
    interview_context: Optional[Dict[str, Any]] = Field(
        default=None,
        description="Optional interview context",
    )


class InterviewSequenceRequest(BaseModel):
    """Request for full interview sequence."""
    
    questions: List[str] = Field(
        ...,
        description="List of interview questions in order",
        min_length=1,
    )
    session_id: str = Field(
        ...,
        description="Session identifier",
    )
    persona: str = Field(
        default="health_conscious",
        description="Persona to use for all responses",
    )
    product_name: str = Field(
        default="the product",
        description="Name of product being discussed",
    )


# Dependencies
async def get_synthetic_service_dep() -> SyntheticService:
    """Dependency to get synthetic service instance."""
    return get_synthetic_service()


# Routes
@router.post(
    "/respond",
    response_model=SyntheticRespondResponse,
    status_code=status.HTTP_200_OK,
    summary="Generate synthetic response",
    description="Generate a plausible respondent answer for automated testing",
)
async def generate_synthetic_response(
    request: SyntheticRespondRequest,
    service: SyntheticService = Depends(get_synthetic_service_dep),
) -> SyntheticRespondResponse:
    """
    Generate a synthetic response to an interview question.
    
    Args:
        request: Synthetic response request with question, persona, context
        service: Injected synthetic service
    
    Returns:
        SyntheticRespondResponse with generated response and metadata
    
    Raises:
        HTTPException 400: Invalid persona
        HTTPException 500: LLM generation error
    """
    log_ctx = log.bind(session_id=request.session_id, persona=request.persona)
    
    log_ctx.info(
        "synthetic_response_requested",
        question_length=len(request.question),
    )
    
    try:
        result = await service.generate_response(
            question=request.question,
            session_id=request.session_id,
            persona=request.persona,
            interview_context=request.interview_context,
            use_deflection=request.use_deflection,
        )
        
        log_ctx.info(
            "synthetic_response_generated",
            response_length=len(result["response"]),
            latency_ms=result["latency_ms"],
        )
        
        return SyntheticRespondResponse(**result)
        
    except ValueError as e:
        # Invalid persona
        log_ctx.warning("synthetic_response_failed", error=str(e))
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=str(e),
        )
    except Exception as e:
        log_ctx.error(
            "synthetic_response_error",
            error_type=type(e).__name__,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate synthetic response: {str(e)}",
        )


@router.post(
    "/respond/multi",
    response_model=List[SyntheticRespondResponse],
    status_code=status.HTTP_200_OK,
    summary="Generate multiple synthetic responses",
    description="Generate one response per persona for comparison testing",
)
async def generate_multi_synthetic(
    request: MultiSyntheticRequest,
    service: SyntheticService = Depends(get_synthetic_service_dep),
) -> List[SyntheticRespondResponse]:
    """
    Generate multiple synthetic responses (one per persona).
    
    Useful for comparing how different personas might respond.
    
    Args:
        request: Multi-synthetic request
        service: Injected synthetic service
    
    Returns:
        List of SyntheticRespondResponse, one per persona
    """
    log_ctx = log.bind(session_id=request.session_id)
    
    log_ctx.info(
        "multi_synthetic_requested",
        question_length=len(request.question),
        persona_count=len(request.personas) if request.personas else 0,
    )
    
    try:
        results = await service.generate_multi_response(
            question=request.question,
            session_id=request.session_id,
            personas=request.personas,
            interview_context=request.interview_context,
        )
        
        log_ctx.info(
            "multi_synthetic_generated",
            response_count=len(results),
        )
        
        return [SyntheticRespondResponse(**r) for r in results]
        
    except Exception as e:
        log_ctx.error(
            "multi_synthetic_error",
            error_type=type(e).__name__,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate multi-synthetic responses: {str(e)}",
        )


@router.post(
    "/respond/sequence",
    response_model=List[SyntheticRespondResponse],
    status_code=status.HTTP_200_OK,
    summary="Generate interview sequence",
    description="Generate responses for a full interview sequence",
)
async def generate_interview_sequence(
    request: InterviewSequenceRequest,
    service: SyntheticService = Depends(get_synthetic_service_dep),
) -> List[SyntheticRespondResponse]:
    """
    Generate synthetic responses for a complete interview.
    
    Useful for automated end-to-end testing.
    
    Args:
        request: Interview sequence request
        service: Injected synthetic service
    
    Returns:
        List of SyntheticRespondResponse, one per question
    """
    log_ctx = log.bind(session_id=request.session_id)
    
    log_ctx.info(
        "interview_sequence_requested",
        question_count=len(request.questions),
        persona=request.persona,
    )
    
    try:
        results = await service.generate_interview_sequence(
            session_id=request.session_id,
            questions=request.questions,
            persona=request.persona,
            product_name=request.product_name,
        )
        
        log_ctx.info(
            "interview_sequence_generated",
            response_count=len(results),
        )
        
        return [SyntheticRespondResponse(**r) for r in results]
        
    except Exception as e:
        log_ctx.error(
            "interview_sequence_error",
            error_type=type(e).__name__,
            error_message=str(e),
        )
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to generate interview sequence: {str(e)}",
        )


@router.get(
    "/personas",
    response_model=Dict[str, str],
    status_code=status.HTTP_200_OK,
    summary="List available personas",
    description="Get list of available personas for synthetic responses",
)
async def list_personas(
    service: SyntheticService = Depends(get_synthetic_service_dep),
) -> Dict[str, str]:
    """
    List all available personas.
    
    Returns:
        Dict mapping persona_id to persona_name
    """
    from src.llm.prompts.synthetic import get_available_personas
    
    personas = get_available_personas()
    
    log.info("personas_listed", count=len(personas))
    
    return personas
```

### src/api/schemas.py (additions)
```python
# Add to existing src/api/schemas.py from Phase 2

from typing import Optional, List, Dict, Any
from pydantic import BaseModel, Field


# Synthetic Respondent Schemas
class SyntheticRespondRequest(BaseModel):
    """Request for synthetic response generation."""
    
    question: str = Field(..., min_length=1, max_length=1000)
    session_id: str = Field(..., min_length=1)
    persona: str = Field(default="health_conscious")
    interview_context: Optional[Dict[str, Any]] = None
    use_deflection: Optional[bool] = None


class SyntheticRespondResponse(BaseModel):
    """Response from synthetic generation."""
    
    response: str
    persona: str
    persona_name: str
    question: str
    latency_ms: float
    tokens_used: Dict[str, int]
    used_deflection: bool


class MultiSyntheticRequest(BaseModel):
    """Request for multiple synthetic responses."""
    
    question: str = Field(..., min_length=1, max_length=1000)
    session_id: str = Field(..., min_length=1)
    personas: Optional[List[str]] = None
    interview_context: Optional[Dict[str, Any]] = None


class InterviewSequenceRequest(BaseModel):
    """Request for full interview sequence."""
    
    questions: List[str] = Field(..., min_length=1)
    session_id: str = Field(..., min_length=1)
    persona: str = Field(default="health_conscious")
    product_name: str = Field(default="the product")
```

### src/main.py (additions)
```python
# Add to existing src/main.py from Phase 1

from src.api.routes import synthetic

# Include synthetic router
app.include_router(synthetic.router)
```

### tests/integration/test_synthetic_api.py
```python
"""Integration tests for synthetic API endpoints."""

import pytest
from httpx import AsyncClient
from unittest.mock import AsyncMock, patch, MagicMock

from src.main import app
from src.llm.client import LLMResponse


class TestSyntheticRespondEndpoint:
    """Tests for POST /synthetic/respond."""
    
    @pytest.mark.asyncio
    async def test_generate_synthetic_response_success(self):
        """Endpoint returns synthetic response."""
        mock_llm_response = LLMResponse(
            content="I really like the creamy texture because it feels satisfying.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                response = await client.post(
                    "/synthetic/respond",
                    json={
                        "question": "Why is creamy texture important to you?",
                        "session_id": "test-session-123",
                        "persona": "health_conscious",
                    },
                )
            
            assert response.status_code == 200
            data = response.json()
            assert data["response"] == "I really like the creamy texture because it feels satisfying."
            assert data["persona"] == "health_conscious"
            assert "Health-Conscious" in data["persona_name"]
            assert data["latency_ms"] == 150.0
    
    @pytest.mark.asyncio
    async def test_invalid_persona_returns_400(self):
        """Endpoint returns 400 for invalid persona."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.post(
                "/synthetic/respond",
                json={
                    "question": "Test question?",
                    "session_id": "test-session",
                    "persona": "invalid_persona",
                },
            )
        
        assert response.status_code == 400
        assert "Unknown persona" in response.json()["detail"]
    
    @pytest.mark.asyncio
    async def test_with_interview_context(self):
        """Endpoint includes interview context in generation."""
        mock_llm_response = LLMResponse(
            content="I think it's great.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 90, "output_tokens": 15},
            latency_ms=120.0,
        )
        
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                response = await client.post(
                    "/synthetic/respond",
                    json={
                        "question": "What do you think?",
                        "session_id": "test-session",
                        "interview_context": {
                            "product_name": "Oat Milk",
                            "turn_number": 5,
                            "coverage_achieved": 0.6,
                        },
                    },
                )
            
            assert response.status_code == 200


class TestMultiSyntheticEndpoint:
    """Tests for POST /synthetic/respond/multi."""
    
    @pytest.mark.asyncio
    async def test_generate_multi_response(self):
        """Endpoint returns multiple responses."""
        mock_llm_response = LLMResponse(
            content="It's important because...",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                response = await client.post(
                    "/synthetic/respond/multi",
                    json={
                        "question": "Why does quality matter?",
                        "session_id": "test-session",
                        "personas": ["health_conscious", "price_sensitive"],
                    },
                )
            
            assert response.status_code == 200
            data = response.json()
            assert isinstance(data, list)
            assert len(data) == 2
            assert data[0]["persona"] == "health_conscious"
            assert data[1]["persona"] == "price_sensitive"


class TestInterviewSequenceEndpoint:
    """Tests for POST /synthetic/respond/sequence."""
    
    @pytest.mark.asyncio
    async def test_generate_interview_sequence(self):
        """Endpoint returns full interview sequence."""
        mock_llm_response = LLMResponse(
            content="That's a good question.",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                response = await client.post(
                    "/synthetic/respond/sequence",
                    json={
                        "questions": [
                            "What comes to mind?",
                            "Why is that important?",
                            "What else matters?",
                        ],
                        "session_id": "test-session",
                        "persona": "quality_focused",
                        "product_name": "Oat Milk",
                    },
                )
            
            assert response.status_code == 200
            data = response.json()
            assert isinstance(data, list)
            assert len(data) == 3
            assert all(r["persona"] == "quality_focused" for r in data)


class TestListPersonasEndpoint:
    """Tests for GET /synthetic/personas."""
    
    @pytest.mark.asyncio
    async def test_list_personas(self):
        """Endpoint returns available personas."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.get("/synthetic/personas")
        
        assert response.status_code == 200
        data = response.json()
        assert isinstance(data, dict)
        assert "health_conscious" in data
        assert data["health_conscious"] == "Health-Conscious Millennial"
```

## Requirements
1. POST /synthetic/respond - Generate single response
2. POST /synthetic/respond/multi - Generate responses for multiple personas
3. POST /synthetic/respond/sequence - Generate full interview sequence
4. GET /synthetic/personas - List available personas
5. Use FastAPI dependency injection for service
6. Return appropriate HTTP status codes (200, 400, 500)
7. Structured logging for all requests

## Verification
```bash
# Run tests
pytest tests/integration/test_synthetic_api.py -v

# Start server and test endpoint
uvicorn src.main:app --reload
curl -X POST http://localhost:8000/synthetic/respond \
  -H "Content-Type: application/json" \
  -d '{"question": "Why is creamy texture important?", "session_id": "test-123", "persona": "health_conscious"}'

# List personas
curl http://localhost:8000/synthetic/personas
```

## Success Criteria
- [ ] `src/api/routes/synthetic.py` created with all endpoints
- [ ] POST /synthetic/respond - generates synthetic response
- [ ] POST /synthetic/respond/multi - generates multiple responses
- [ ] POST /synthetic/respond/sequence - generates interview sequence
- [ ] GET /synthetic/personas - lists available personas
- [ ] Proper error handling (400 for invalid persona, 500 for errors)
- [ ] Structured logging for all requests
- [ ] Router registered in main.py
- [ ] All integration tests pass
