# Spec 4.5: Regression Tests

## Objective
Create integration tests that validate the complete synthetic interview flow.

## Context
- Reference: PRD Section 2.2 (Success Metrics), IMPLEMENTATION_PLAN.md Phase 4 Gate Criteria
- Tests should validate end-to-end synthetic interview functionality
- Ensures system works correctly for automated testing scenarios
- Complements unit tests from previous specs

## Input Files
- Phase 4.2: `src/services/synthetic_service.py` - Synthetic service
- Phase 4.3: `src/api/routes/synthetic.py` - Synthetic API
- Phase 4.4: `scripts/run_synthetic_interview.py` - Test script reference

## Output Files

### tests/integration/test_synthetic.py
```python
"""Integration tests for synthetic respondent functionality.

These tests validate the complete synthetic interview flow:
- Session creation with synthetic responses
- Coverage achievement through synthetic turns
- Graph state updates
- Session completion detection
"""

import pytest
import asyncio
from typing import Dict, Any, List
from unittest.mock import AsyncMock, patch, MagicMock

from httpx import AsyncClient

from src.main import app
from src.llm.client import LLMResponse
from src.services.synthetic_service import SyntheticService


# Fixtures
@pytest.fixture
def mock_llm_response():
    """Mock LLM response for testing."""
    return LLMResponse(
        content="I really like the creamy texture because it makes my coffee feel satisfying.",
        model="claude-sonnet-4-20250514",
        usage={"input_tokens": 100, "output_tokens": 25},
        latency_ms=150.0,
    )


@pytest.fixture
def sample_concept_config():
    """Sample concept configuration for testing."""
    return {
        "id": "test_concept_v1",
        "name": "Test Product",
        "methodology": "means_end_chain",
        "elements": [
            {"id": "element1", "label": "Test Attribute", "type": "attribute", "priority": "high"},
            {"id": "element2", "label": "Test Benefit", "type": "functional_consequence", "priority": "high"},
        ],
        "completion": {
            "target_coverage": 0.8,
            "max_turns": 10,
        },
    }


# Test Classes
class TestSyntheticInterviewFlow:
    """Tests for complete synthetic interview flow."""
    
    @pytest.mark.asyncio
    async def test_synthetic_interview_completes(self, mock_llm_response, sample_concept_config):
        """A complete synthetic interview runs without errors."""
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            # Setup mock LLM
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            # Mock concept repository
            with patch("src.persistence.repositories.concept_repo.ConceptRepository.get") as mock_get_concept:
                mock_get_concept.return_value = sample_concept_config
                
                async with AsyncClient(app=app, base_url="http://test") as client:
                    # Create session
                    session_response = await client.post(
                        "/sessions",
                        json={
                            "concept_id": "test_concept_v1",
                            "max_turns": 5,
                            "target_coverage": 0.8,
                        },
                    )
                    assert session_response.status_code == 200
                    session = session_response.json()
                    session_id = session["id"]
                    
                    # Run synthetic turns
                    for i in range(5):
                        # Get question
                        status_response = await client.get(f"/sessions/{session_id}/status")
                        status = status_response.json()
                        question = status.get("next_question")
                        
                        if not question or not status.get("should_continue", True):
                            break
                        
                        # Get synthetic response
                        synthetic_response = await client.post(
                            "/synthetic/respond",
                            json={
                                "question": question,
                                "session_id": session_id,
                                "persona": "health_conscious",
                            },
                        )
                        assert synthetic_response.status_code == 200
                        response_text = synthetic_response.json()["response"]
                        
                        # Submit turn
                        turn_response = await client.post(
                            f"/sessions/{session_id}/turns",
                            json={"user_input": response_text},
                        )
                        assert turn_response.status_code == 200
                    
                    # Verify session completed or is active
                    final_status = await client.get(f"/sessions/{session_id}/status")
                    final_data = final_status.json()
                    assert final_data["status"] in ["active", "completed", "coverage_met"]
    
    @pytest.mark.asyncio
    async def test_synthetic_interview_achieves_coverage(self, mock_llm_response, sample_concept_config):
        """Synthetic interview achieves meaningful coverage."""
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            with patch("src.persistence.repositories.concept_repo.ConceptRepository.get") as mock_get_concept:
                mock_get_concept.return_value = sample_concept_config
                
                async with AsyncClient(app=app, base_url="http://test") as client:
                    # Create and run session
                    session_response = await client.post(
                        "/sessions",
                        json={"concept_id": "test_concept_v1", "max_turns": 10},
                    )
                    session_id = session_response.json()["id"]
                    
                    # Run turns
                    for _ in range(10):
                        status = (await client.get(f"/sessions/{session_id}/status")).json()
                        question = status.get("next_question")
                        if not question:
                            break
                        
                        synthetic = await client.post(
                            "/synthetic/respond",
                            json={"question": question, "session_id": session_id, "persona": "health_conscious"},
                        )
                        response_text = synthetic.json()["response"]
                        
                        await client.post(
                            f"/sessions/{session_id}/turns",
                            json={"user_input": response_text},
                        )
                    
                    # Check coverage
                    final_status = (await client.get(f"/sessions/{session_id}/status")).json()
                    coverage = final_status.get("coverage", 0.0)
                    
                    # Some coverage should be achieved (>0)
                    assert coverage > 0.0
    
    @pytest.mark.asyncio
    async def test_synthetic_multi_persona_comparison(self, mock_llm_response):
        """Can generate responses from multiple personas for comparison."""
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                response = await client.post(
                    "/synthetic/respond/multi",
                    json={
                        "question": "Why does quality matter to you?",
                        "session_id": "test-comparison",
                        "personas": ["health_conscious", "price_sensitive", "quality_focused"],
                    },
                )
                
                assert response.status_code == 200
                data = response.json()
                assert isinstance(data, list)
                assert len(data) == 3
                
                personas = [r["persona"] for r in data]
                assert "health_conscious" in personas
                assert "price_sensitive" in personas
                assert "quality_focused" in personas
    
    @pytest.mark.asyncio
    async def test_synthetic_interview_sequence(self, mock_llm_response):
        """Can generate a complete interview sequence at once."""
        questions = [
            "What comes to mind when you think about oat milk?",
            "Why is that important to you?",
            "What else matters?",
        ]
        
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                response = await client.post(
                    "/synthetic/respond/sequence",
                    json={
                        "questions": questions,
                        "session_id": "test-sequence",
                        "persona": "convenience_seeker",
                        "product_name": "Oat Milk",
                    },
                )
                
                assert response.status_code == 200
                data = response.json()
                assert isinstance(data, list)
                assert len(data) == 3
                
                # Verify all questions are in responses
                for i, resp in enumerate(data):
                    assert resp["question"] == questions[i]
                    assert resp["persona"] == "convenience_seeker"


class TestSyntheticServiceIntegration:
    """Integration tests for synthetic service."""
    
    @pytest.mark.asyncio
    async def test_service_generates_varied_responses(self):
        """Service generates different responses with deflection."""
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="Sample response",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 100, "output_tokens": 20},
            latency_ms=150.0,
        )
        
        service = SyntheticService(llm_client=mock_llm, deflection_chance=1.0)
        
        # Generate with deflection forced on
        result1 = await service.generate_response(
            question="Test question?",
            session_id="test-1",
            use_deflection=True,
        )
        
        # Generate without deflection
        result2 = await service.generate_response(
            question="Test question?",
            session_id="test-2",
            use_deflection=False,
        )
        
        assert result1["used_deflection"] is True
        assert result2["used_deflection"] is False
    
    @pytest.mark.asyncio
    async def test_service_handles_all_personas(self):
        """Service successfully handles all available personas."""
        from src.llm.prompts.synthetic import get_available_personas
        
        personas = get_available_personas()
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="Response",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 50, "output_tokens": 10},
            latency_ms=100.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        for persona_id in personas.keys():
            result = await service.generate_response(
                question="Test?",
                session_id="test",
                persona=persona_id,
            )
            assert result["persona"] == persona_id
            assert result["persona_name"] == personas[persona_id]


class TestSyntheticAPIErrorHandling:
    """Tests for error handling in synthetic API."""
    
    @pytest.mark.asyncio
    async def test_invalid_persona_returns_400(self):
        """API returns 400 for invalid persona."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.post(
                "/synthetic/respond",
                json={
                    "question": "Test?",
                    "session_id": "test",
                    "persona": "invalid_persona_name",
                },
            )
            
            assert response.status_code == 400
            assert "Unknown persona" in response.json()["detail"]
    
    @pytest.mark.asyncio
    async def test_missing_question_returns_422(self):
        """API returns 422 for missing required fields."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.post(
                "/synthetic/respond",
                json={"session_id": "test"},
            )
            
            assert response.status_code == 422  # Validation error


class TestSyntheticWithGraphState:
    """Tests for synthetic service with graph state context."""
    
    @pytest.mark.asyncio
    async def test_uses_previous_concepts(self):
        """Service includes previous concepts from graph state."""
        from src.domain.models.knowledge_graph import GraphState
        
        # Create mock graph state with previous concepts
        graph_state = GraphState(
            node_count=5,
            edge_count=3,
            recent_nodes=[
                MagicMock(label="creamy texture"),
                MagicMock(label="plant-based"),
                MagicMock(label="satisfying"),
            ],
        )
        
        mock_llm = AsyncMock()
        mock_llm.complete.return_value = LLMResponse(
            content="Yes, that's important",
            model="claude-sonnet-4-20250514",
            usage={"input_tokens": 80, "output_tokens": 15},
            latency_ms=120.0,
        )
        
        service = SyntheticService(llm_client=mock_llm)
        
        await service.generate_response(
            question="Does sustainability matter?",
            session_id="test",
            graph_state=graph_state,
        )
        
        # Verify prompt included previous concepts
        call_args = mock_llm.complete.call_args
        prompt = call_args.kwargs["prompt"]
        assert "creamy texture" in prompt
        assert "plant-based" in prompt


class TestSyntheticInterviewValidation:
    """Tests that validate synthetic interview quality."""
    
    @pytest.mark.asyncio
    async def test_interview_produces_graph_state(self, mock_llm_response, sample_concept_config):
        """Synthetic interview produces valid graph state."""
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            with patch("src.persistence.repositories.concept_repo.ConceptRepository.get") as mock_get_concept:
                mock_get_concept.return_value = sample_concept_config
                
                async with AsyncClient(app=app, base_url="http://test") as client:
                    # Create and run session
                    session_response = await client.post(
                        "/sessions",
                        json={"concept_id": "test_concept_v1", "max_turns": 5},
                    )
                    session_id = session_response.json()["id"]
                    
                    # Run a few turns
                    for _ in range(3):
                        status = (await client.get(f"/sessions/{session_id}/status")).json()
                        question = status.get("next_question")
                        if not question:
                            break
                        
                        synthetic = await client.post(
                            "/synthetic/respond",
                            json={"question": question, "session_id": session_id, "persona": "health_conscious"},
                        )
                        response_text = synthetic.json()["response"]
                        
                        await client.post(
                            f"/sessions/{session_id}/turns",
                            json={"user_input": response_text},
                        )
                    
                    # Get graph state
                    graph_response = await client.get(f"/sessions/{session_id}/graph")
                    assert graph_response.status_code == 200
                    
                    graph = graph_response.json()
                    assert "nodes" in graph
                    assert "edges" in graph
                    # Should have some nodes after extraction
                    assert len(graph["nodes"]) >= 0
    
    @pytest.mark.asyncio
    async def test_interview_logs_metrics(self, mock_llm_response, sample_concept_config):
        """Synthetic interview produces metrics for analysis."""
        with patch("src.services.synthetic_service.get_llm_client") as mock_get_client:
            mock_client = AsyncMock()
            mock_client.complete.return_value = mock_llm_response
            mock_get_client.return_value = mock_client
            
            with patch("src.persistence.repositories.concept_repo.ConceptRepository.get") as mock_get_concept:
                mock_get_concept.return_value = sample_concept_config
                
                async with AsyncClient(app=app, base_url="http://test") as client:
                    session_response = await client.post(
                        "/sessions",
                        json={"concept_id": "test_concept_v1", "max_turns": 3},
                    )
                    session_id = session_response.json()["id"]
                    
                    # Run turns
                    for _ in range(3):
                        status = (await client.get(f"/sessions/{session_id}/status")).json()
                        question = status.get("next_question")
                        if not question:
                            break
                        
                        synthetic = await client.post(
                            "/synthetic/respond",
                            json={"question": question, "session_id": session_id, "persona": "health_conscious"},
                        )
                        response_text = synthetic.json()["response"]
                        
                        turn_response = await client.post(
                            f"/sessions/{session_id}/turns",
                            json={"user_input": response_text},
                        )
                        
                        # Verify turn response includes metrics
                        turn_data = turn_response.json()
                        assert "scoring" in turn_data
                        assert "latency_ms" in turn_data or turn_data.get("scoring", {}).get("latency_ms")
```

## Requirements
1. Tests must validate complete synthetic interview flow
2. Tests should check coverage achievement
3. Tests should validate graph state updates
4. Tests should verify error handling
5. Tests should check all personas work correctly
6. Use pytest-asyncio for async tests
7. Mock LLM calls to avoid actual API usage

## Verification
```bash
# Run all synthetic integration tests
pytest tests/integration/test_synthetic.py -v

# Run specific test class
pytest tests/integration/test_synthetic.py::TestSyntheticInterviewFlow -v

# Run with coverage
pytest tests/integration/test_synthetic.py --cov=src/services/synthetic_service --cov=src/api/routes/synthetic -v

# Run specific test
pytest tests/integration/test_synthetic.py::TestSyntheticInterviewFlow::test_synthetic_interview_completes -v
```

## Success Criteria
- [ ] `tests/integration/test_synthetic.py` created
- [ ] `TestSyntheticInterviewFlow` - validates complete interview flow
- [ ] `TestSyntheticServiceIntegration` - validates service behavior
- [ ] `TestSyntheticAPIErrorHandling` - validates error handling
- [ ] `TestSyntheticWithGraphState` - validates graph context usage
- [ ] `TestSyntheticInterviewValidation` - validates interview quality
- [ ] All tests pass with mocked LLM calls
- [ ] Tests use proper async fixtures and mocking
