# Spec 6.8: Performance Check

## Objective
Validate that the system meets all performance requirements from the PRD.

## Context
- Reference: PRD Section 2.2 (Success Metrics), Section 10 (Technical Constraints)
- Key requirement: Turn latency p95 < 5s
- Validate under realistic load

## Input Files
- PRD.md Section 2.2, Section 10
- All previous implementations

## Output Files

### tests/performance/test_latency.py
```python
"""
Performance tests for interview system latency.

Validates that the system meets PRD performance requirements.
"""

import pytest
import time
import asyncio
import statistics
from typing import List
from httpx import AsyncClient

from src.main import app


class TestTurnLatency:
    """Tests for turn processing latency."""
    
    @pytest.mark.asyncio
    async def test_single_turn_latency_under_5s(self):
        """Single turn completes in under 5 seconds."""
        # This test measures system latency (excluding LLM)
        # In production, LLM latency varies by provider
        
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create session
            response = await client.post(
                "/sessions",
                json={"concept_id": "oat_milk_v1"},
            )
            session_id = response.json()["id"]
            
            # Measure turn latency
            start = time.perf_counter()
            
            response = await client.post(
                f"/sessions/{session_id}/turns",
                json={"user_input": "Test response for latency measurement"},
            )
            
            elapsed = time.perf_counter() - start
            
            assert response.status_code == 200
            assert elapsed < 5.0, f"Turn took {elapsed:.2f}s (should be <5s)"
    
    @pytest.mark.asyncio
    async def test_multiple_turns_average_latency(self):
        """Average turn latency across multiple turns is acceptable."""
        num_turns = 5
        latencies: List[float] = []
        
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create session
            response = await client.post(
                "/sessions",
                json={"concept_id": "oat_milk_v1"},
            )
            session_id = response.json()["id"]
            
            # Run multiple turns
            for i in range(num_turns):
                start = time.perf_counter()
                
                response = await client.post(
                    f"/sessions/{session_id}/turns",
                    json={"user_input": f"Test response {i+1}"},
                )
                
                assert response.status_code == 200
                
                elapsed = time.perf_counter() - start
                latencies.append(elapsed)
            
            # Calculate statistics
            avg_latency = statistics.mean(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
            
            print(f"Average latency: {avg_latency:.2f}s")
            print(f"P95 latency: {p95_latency:.2f}s")
            
            # P95 should be under 5s
            assert p95_latency < 5.0, f"P95 latency: {p95_latency:.2f}s"
    
    @pytest.mark.asyncio
    async def test_graph_query_latency(self):
        """Graph queries complete quickly."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create session with some data
            response = await client.post(
                "/sessions",
                json={"concept_id": "oat_milk_v1"},
            )
            session_id = response.json()["id"]
            
            # Query graph
            start = time.perf_counter()
            response = await client.get(f"/sessions/{session_id}/graph")
            elapsed = time.perf_counter() - start
            
            assert response.status_code == 200
            assert elapsed < 1.0, f"Graph query took {elapsed:.2f}s"
    
    @pytest.mark.asyncio
    async def test_export_latency(self):
        """Export operations complete quickly."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create session
            response = await client.post(
                "/sessions",
                json={"concept_id": "oat_milk_v1"},
            )
            session_id = response.json()["id"]
            
            # Add a turn
            await client.post(
                f"/sessions/{session_id}/turns",
                json={"user_input": "Test"},
            )
            
            # Test JSON export
            start = time.perf_counter()
            response = await client.get(
                f"/sessions/{session_id}/export",
                params={"format": "json"},
            )
            elapsed = time.perf_counter() - start
            
            assert response.status_code == 200
            assert elapsed < 2.0, f"JSON export took {elapsed:.2f}s"


class TestThroughput:
    """Tests for system throughput capacity."""
    
    @pytest.mark.asyncio
    async def test_concurrent_session_capacity(self):
        """System can handle multiple concurrent sessions."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create multiple sessions concurrently
            tasks = [
                client.post("/sessions", json={"concept_id": "oat_milk_v1"})
                for _ in range(10)
            ]
            
            start = time.perf_counter()
            responses = await asyncio.gather(*tasks)
            elapsed = time.perf_counter() - start
            
            # All should succeed
            for response in responses:
                assert response.status_code == 200
            
            # Should complete in reasonable time
            assert elapsed < 10.0, f"Creating 10 sessions took {elapsed:.2f}s"


class TestResourceUsage:
    """Tests for resource usage patterns."""
    
    @pytest.mark.asyncio
    async def test_memory_leak_check(self):
        """No significant memory growth across operations."""
        import gc
        import tracemalloc
        
        tracemalloc.start()
        
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create session
            response = await client.post(
                "/sessions",
                json={"concept_id": "oat_milk_v1"},
            )
            session_id = response.json()["id"]
            
            # Snapshot 1
            snapshot1 = tracemalloc.take_snapshot()
            
            # Run multiple operations
            for i in range(10):
                await client.post(
                    f"/sessions/{session_id}/turns",
                    json={"user_input": f"Test {i}"},
                )
            
            # Force garbage collection
            gc.collect()
            
            # Snapshot 2
            snapshot2 = tracemalloc.take_snapshot()
            
            # Compare memory usage
            top_stats = snapshot2.compare_to(snapshot1)
            
            # Sum up the top 10 allocations
            total_diff = sum(stat.size_diff for stat in top_stats[:10])
            
            # Memory growth should be reasonable (< 1MB for 10 turns)
            assert total_diff < 1_000_000, f"Memory growth: {total_diff} bytes"
        
        tracemalloc.stop()
```

### scripts/benchmark.py
```python
#!/usr/bin/env python3
"""
Benchmark script for performance testing.

Measures key performance metrics and reports results.
"""

import asyncio
import time
import statistics
from typing import List
import httpx


async def benchmark_turn_latency(base_url: str, num_turns: int = 10):
    """Benchmark turn processing latency."""
    print(f"Benchmarking turn latency ({num_turns} turns)...")
    
    async with httpx.AsyncClient() as client:
        # Create session
        response = await client.post(
            f"{base_url}/sessions",
            json={"concept_id": "oat_milk_v1"},
        )
        session_id = response.json()["id"]
        
        latencies: List[float] = []
        
        for i in range(num_turns):
            start = time.perf_counter()
            
            response = await client.post(
                f"{base_url}/sessions/{session_id}/turns",
                json={"user_input": f"Benchmark response {i+1}"},
            )
            
            elapsed = time.perf_counter() - start
            latencies.append(elapsed)
            
            if response.status_code != 200:
                print(f"Turn {i+1} failed: {response.status_code}")
                return
        
        # Statistics
        avg = statistics.mean(latencies)
        median = statistics.median(latencies)
        p95 = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
        p99 = statistics.quantiles(latencies, n=100)[98]  # 99th percentile
        min_val = min(latencies)
        max_val = max(latencies)
        
        print("\n=== Turn Latency Results ===")
        print(f"Turns:      {num_turns}")
        print(f"Average:    {avg*1000:.0f}ms")
        print(f"Median:     {median*1000:.0f}ms")
        print(f"P95:        {p95*1000:.0f}ms")
        print(f"P99:        {p99*1000:.0f}ms")
        print(f"Min:        {min_val*1000:.0f}ms")
        print(f"Max:        {max_val*1000:.0f}ms")
        
        # Check against PRD requirement
        if p95 < 5.0:
            print(f"\n✅ P95 latency ({p95*1000:.0f}ms) meets PRD requirement (<5s)")
        else:
            print(f"\n❌ P95 latency ({p95*1000:.0f}ms) exceeds PRD requirement (<5s)")
        
        return {
            "avg_ms": avg * 1000,
            "p95_ms": p95 * 1000,
            "passes_prd": p95 < 5.0,
        }


async def main():
    """Run all benchmarks."""
    import sys
    
    base_url = sys.argv[1] if len(sys.argv) > 1 else "http://localhost:8000"
    
    print(f"Benchmarking against: {base_url}\n")
    
    results = await benchmark_turn_latency(base_url, num_turns=10)
    
    # Exit with appropriate code
    sys.exit(0 if results["passes_prd"] else 1)


if __name__ == "__main__":
    asyncio.run(main())
```

### docs/PERFORMANCE.md
```markdown
# Performance Requirements

## PRD Requirements

From PRD Section 2.2 and Section 10:

| Metric | Target | Measurement |
|--------|--------|-------------|
| Turn latency p95 | <5s | Time from user input to system response |
| Interview completion rate | ≥90% | Sessions reaching natural close |
| Element coverage | ≥80% | Stimulus elements explored |
| Extraction accuracy | ≥80% | Concepts correctly identified (sampled review) |
| Graph coherence | <10% orphans | Nodes without relationships |

## Testing

Run benchmarks:

```bash
# Start backend
uvicorn src.main:app

# Run benchmarks
python scripts/benchmark.py http://localhost:8000

# Run performance tests
pytest tests/performance/ -v
```

## Target Architecture

The single-event-loop architecture (FastAPI + uvicorn) ensures:
- No async context switching overhead
- Direct SQLite access (no connection pool contention)
- Minimal middleware layers
- Efficient structured logging
```

## Requirements
1. Turn latency test (p95 < 5s)
2. Multiple turn average latency test
3. Graph query latency test
4. Export operation latency test
5. Concurrent session capacity test
6. Memory leak check
7. Benchmark script for manual testing

## Verification
```bash
# Run performance tests
pytest tests/performance/ -v

# Run benchmark script
python scripts/benchmark.py

# Check system can handle load
ab -n 100 -c 10 http://localhost:8000/health
```

## Success Criteria
- [ ] Turn latency p95 < 5 seconds
- [ ] Graph queries complete in <1s
- [ ] Export operations complete in <2s
- [ ] System handles 10+ concurrent sessions
- [ ] No memory leaks detected
- [ ] Benchmark script reports results
- [ ] All performance tests pass
