# Spec 6.7: End-to-End Testing

## Objective
Create comprehensive end-to-end tests validating the complete system.

## Context
- Reference: PRD Section 2.2 (Success Metrics), IMPLEMENTATION_PLAN.md Phase 6 Gate Criteria
- Tests validate complete workflows from API to database
- Integration tests use real dependencies (not mocks)
- Validates system is production-ready

## Input Files
- All previous phase implementations
- Phase 4: `scripts/run_synthetic_interview.py` - Reference for test patterns

## Output Files

### tests/integration/test_e2e_system.py
```python
"""
End-to-end integration tests for the complete interview system.

These tests validate:
- Complete interview workflow
- Coverage achievement
- Graph construction
- Session lifecycle
- Export functionality
"""

import pytest
import asyncio
import tempfile
from pathlib import Path

from httpx import AsyncClient

from src.main import app
from src.persistence.database import init_database
from src.core.config import Settings


class TestCompleteInterviewWorkflow:
    """Tests for complete interview from start to finish."""
    
    @pytest.mark.asyncio
    async def test_full_interview_workflow(self):
        """A complete interview runs through all phases."""
        # Setup
        with tempfile.TemporaryDirectory() as tmpdir:
            settings = Settings(
                database_path=Path(tmpdir) / "test.db",
                anthropic_api_key="test-key",  # Would use mocked LLM
            )
            await init_database(settings.database_path)
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                # 1. Create session
                response = await client.post(
                    "/sessions",
                    json={
                        "concept_id": "oat_milk_v1",
                        "max_turns": 10,
                        "target_coverage": 0.8,
                    },
                )
                assert response.status_code == 200
                session = response.json()
                session_id = session["id"]
                
                # 2. Get opening question
                response = await client.get(f"/sessions/{session_id}/status")
                assert response.status_code == 200
                status = response.json()
                question = status.get("next_question")
                assert question is not None
                
                # 3. Run a few turns (with mocked LLM for testing)
                for i in range(3):
                    # Submit turn
                    response = await client.post(
                        f"/sessions/{session_id}/turns",
                        json={"user_input": f"Test response {i+1}"},
                    )
                    assert response.status_code == 200
                    turn_result = response.json()
                    
                    # Verify response has expected fields
                    assert "next_question" in turn_result
                    assert "extracted" in turn_result
                    assert "scoring" in turn_result
                
                # 4. Get final status
                response = await client.get(f"/sessions/{session_id}/status")
                assert response.status_code == 200
                final_status = response.json()
                
                # 5. Get graph data
                response = await client.get(f"/sessions/{session_id}/graph")
                assert response.status_code == 200
                graph = response.json()
                assert "nodes" in graph
                assert "edges" in graph
                
                # 6. Export session
                response = await client.get(
                    f"/sessions/{session_id}/export",
                    params={"format": "json"},
                )
                assert response.status_code == 200
                export_data = response.text
                assert len(export_data) > 0
    
    @pytest.mark.asyncio
    async def test_session_lifecycle(self):
        """Test complete session lifecycle: create, use, close, export."""
        with tempfile.TemporaryDirectory() as tmpdir:
            settings = Settings(
                database_path=Path(tmpdir) / "test.db",
                anthropic_api_key="test-key",
            )
            await init_database(settings.database_path)
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                # Create
                response = await client.post(
                    "/sessions",
                    json={"concept_id": "oat_milk_v1"},
                )
                assert response.status_code == 200
                session_id = response.json()["id"]
                
                # Use (submit turn)
                response = await client.post(
                    f"/sessions/{session_id}/turns",
                    json={"user_input": "Test response"},
                )
                assert response.status_code == 200
                
                # Export
                response = await client.get(
                    f"/sessions/{session_id}/export",
                    params={"format": "markdown"},
                )
                assert response.status_code == 200
                
                # Delete
                response = await client.delete(f"/sessions/{session_id}")
                assert response.status_code in [200, 204]
                
                # Verify deleted
                response = await client.get(f"/sessions/{session_id}")
                assert response.status_code == 404


class TestSyntheticInterviewIntegration:
    """Tests using synthetic respondent for full interviews."""
    
    @pytest.mark.asyncio
    async def test_synthetic_interview_achieves_coverage(self):
        """Synthetic interview achieves meaningful coverage."""
        with tempfile.TemporaryDirectory() as tmpdir:
            settings = Settings(
                database_path=Path(tmpdir) / "test.db",
                anthropic_api_key="test-key",
            )
            await init_database(settings.database_path)
            
            # Mock LLM for synthetic responses
            # ... mock setup ...
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                # Create session
                response = await client.post(
                    "/sessions",
                    json={
                        "concept_id": "oat_milk_v1",
                        "max_turns": 15,
                        "target_coverage": 0.7,
                    },
                )
                session_id = response.json()["id"]
                
                # Run interview with synthetic responses
                for i in range(10):
                    status = (await client.get(f"/sessions/{session_id}/status")).json()
                    question = status.get("next_question")
                    if not question:
                        break
                    
                    # Get synthetic response
                    synth = await client.post(
                        "/synthetic/respond",
                        json={
                            "question": question,
                            "session_id": session_id,
                            "persona": "health_conscious",
                        },
                    )
                    synthetic_response = synth.json()["response"]
                    
                    # Submit turn
                    await client.post(
                        f"/sessions/{session_id}/turns",
                        json={"user_input": synthetic_response},
                    )
                
                # Check final coverage
                final_status = (await client.get(f"/sessions/{session_id}/status")).json()
                coverage = final_status.get("coverage", 0.0)
                
                # Some coverage should be achieved
                assert coverage > 0.0


class TestExportFormats:
    """Tests for export functionality."""
    
    @pytest.mark.asyncio
    async def test_all_export_formats_work(self):
        """All export formats produce valid output."""
        with tempfile.TemporaryDirectory() as tmpdir:
            settings = Settings(
                database_path=Path(tmpdir) / "test.db",
                anthropic_api_key="test-key",
            )
            await init_database(settings.database_path)
            
            async with AsyncClient(app=app, base_url="http://test") as client:
                # Create and use session
                response = await client.post(
                    "/sessions",
                    json={"concept_id": "oat_milk_v1"},
                )
                session_id = response.json()["id"]
                
                await client.post(
                    f"/sessions/{session_id}/turns",
                    json={"user_input": "Test response"},
                )
                
                # Test JSON export
                response = await client.get(
                    f"/sessions/{session_id}/export",
                    params={"format": "json"},
                )
                assert response.status_code == 200
                assert response.headers["content-type"] == "application/json"
                
                # Test Markdown export
                response = await client.get(
                    f"/sessions/{session_id}/export",
                    params={"format": "markdown"},
                )
                assert response.status_code == 200
                assert "text/markdown" in response.headers["content-type"]
                
                # Test CSV export
                response = await client.get(
                    f"/sessions/{session_id}/export",
                    params={"format": "csv"},
                )
                assert response.status_code == 200
                assert "text/csv" in response.headers["content-type"]


class TestConceptEndpoints:
    """Tests for concept management endpoints."""
    
    @pytest.mark.asyncio
    async def test_list_and_get_concepts(self):
        """Concept endpoints work correctly."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            # List concepts
            response = await client.get("/concepts")
            assert response.status_code == 200
            concepts = response.json()
            assert isinstance(concepts, list)
            
            # Get specific concept (if oat_milk_v1 exists)
            if len(concepts) > 0:
                concept_id = concepts[0]["id"]
                response = await client.get(f"/concepts/{concept_id}")
                # May be 404 if config file doesn't exist
                assert response.status_code in [200, 404]


class TestHealthEndpoint:
    """Tests for health check endpoint."""
    
    @pytest.mark.asyncio
    async def test_health_check_returns_ok(self):
        """Health endpoint returns 200."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            response = await client.get("/health")
            assert response.status_code == 200
            data = response.json()
            assert "status" in data
```

### tests/integration/test_e2e_performance.py
```python
"""
Performance tests for the interview system.

Validates that the system meets performance requirements from the PRD.
"""

import pytest
import time
import asyncio
from httpx import AsyncClient

from src.main import app


class TestPerformanceRequirements:
    """Tests for PRD performance requirements."""
    
    @pytest.mark.asyncio
    async def test_turn_latency_under_5_seconds(self):
        """Turn processing completes in under 5 seconds (PRD requirement)."""
        # This would use mocked LLM to measure system latency only
        # Real LLM latency is external and variable
        
        start = time.perf_counter()
        
        # ... perform turn operation ...
        
        elapsed = time.perf_counter() - start
        
        # Allow some margin for test environment
        assert elapsed < 4.5, f"Turn took {elapsed:.2f}s (should be <5s)"
    
    @pytest.mark.asyncio
    async def test_concurrent_sessions_performance(self):
        """System handles multiple sessions without degradation."""
        async with AsyncClient(app=app, base_url="http://test") as client:
            # Create multiple sessions
            session_ids = []
            
            for i in range(5):
                response = await client.post(
                    "/sessions",
                    json={"concept_id": "oat_milk_v1"},
                )
                assert response.status_code == 200
                session_ids.append(response.json()["id"])
            
            # All sessions should be accessible
            for session_id in session_ids:
                response = await client.get(f"/sessions/{session_id}/status")
                assert response.status_code == 200
```

### scripts/run_e2e_tests.sh
```bash
#!/bin/bash
# Run end-to-end tests

set -e

echo "Running end-to-end tests..."

# Start backend in background
uvicorn src.main:app --host localhost --port 8000 &
BACKEND_PID=$!

# Wait for backend to start
sleep 5

# Run tests
pytest tests/integration/test_e2e_system.py -v

# Cleanup
kill $BACKEND_PID

echo "E2E tests complete!"
```

## Requirements
1. Complete interview workflow test
2. Session lifecycle test (create, use, close, export)
3. Synthetic interview integration test
4. All export formats test
5. Concept endpoints test
6. Health check test
7. Performance validation tests

## Verification
```bash
# Run all E2E tests
pytest tests/integration/test_e2e_system.py -v

# Run performance tests
pytest tests/integration/test_e2e_performance.py -v

# Run with E2E test script
bash scripts/run_e2e_tests.sh
```

## Success Criteria
- [ ] Complete interview workflow test passes
- [ ] Session lifecycle test passes
- [ ] Synthetic interview achieves coverage
- [ ] All export formats work
- [ ] Health endpoint works
- [ ] Performance tests meet PRD requirements
- [ ] E2E test script works
